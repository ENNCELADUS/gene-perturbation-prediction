# Weekly Report: 12.12

## Finetune scGPT Progress
This week, we focused on optimizing the scGPT finetuning process. A key strategy was to freeze a significant portion of the model to prevent catastrophic forgetting.

**Model Parameter Stats:**
- **Frozen Parameters:** 50,278,400 (Encoder, Value Encoder, Transformer Encoder)
- **Trainable Parameters:** 1,581,059 (3.0% of model)
- **Trainable Components:** `{'pert_encoder', 'decoder', 'cls_decoder'}`

## New Feature Inspection

### 1. Encoder Freezing
To preserve the gene-gene relationships learned during pretraining on ~33M cells, we can now freeze the encoder layers during finetuning.
- **Function:** `freeze_encoder_layers()` in `src/utils/training.py`
- **Behavior:** Sets `requires_grad=False` for parameters matching the `freeze_prefixes`.
- **Config:** Enabled via `training.freeze_encoder` in `finetune.yaml`.
- **Benefit:** Prevents the encoder from deteriorating when training on only ~100 perturbations.

### 2. Weighted Loss Function
A new weighted loss mechanism was added to `train_epoch` to address the issue where the model ignores high-change genes (perturbations).
- **Mechanism:**
    - Calculates the absolute difference (delta) between target and input expression: `|target - input|`.
    - Normalizes this delta within the batch.
    - Applies a weight factor: `Weight = 1.0 + weight_factor * normalized_delta`.
- **Default Config:** `weight_factor` is set to `10.0` in the new config (default `5.0` in code).
- **Goal:** Encourages the model to focus on genes that actually change, aligning with the **Top2000-MAE** metric.

## Evaluation Results

| Model | PDS | MAE | DES | Overall Score |
| :--- | :--- | :--- | :--- | :--- |
| Baseline | 0.4833 | 0.1258 | 0.2534 | - |