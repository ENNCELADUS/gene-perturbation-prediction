==========================================
Step 0: Splitting data (30 test genes excluded from train)...
==========================================
Split data already exists, skipping data splitting...
==========================================
Step 1: Converting data to GEARS format...
==========================================
GEARS data already exists, skipping conversion...
==========================================
Step 2: Finetuning scGPT with DDP...
==========================================
Detected 3 GPUs
Using device: cuda:0, Saving to model/scGPT_finetuned
scGPT - INFO - Running on 2025-12-01 23:00:00
scGPT - INFO - Config: {'paths': {'gears_data_dir': 'data/processed/gears', 'dataset_name': 'vcc', 'pretrained_model_dir': 'model/scGPT', 'output_dir': 'model/scGPT_finetuned'}, 'data': {'pad_token': '<pad>', 'special_tokens': ['<pad>', '<cls>', '<eoc>'], 'pad_value': 0, 'pert_pad_id': 0, 'include_zero_gene': 'batch-wise', 'max_seq_len': 1536}, 'training': {'MLM': True, 'CLS': False, 'CCE': False, 'MVC': False, 'ECS': False, 'amp': True}, 'optimizer': {'lr': 0.0001, 'batch_size': 64, 'eval_batch_size': 64, 'epochs': 15, 'schedule_interval': 1, 'schedule_gamma': 0.9, 'early_stop': 10, 'grad_clip': 1.0}, 'model': {'embsize': 512, 'd_hid': 512, 'nlayers': 12, 'nhead': 8, 'n_layers_cls': 3, 'dropout': 0.0, 'use_fast_transformer': False}, 'load_param_prefixes': ['encoder', 'value_encoder', 'transformer_encoder'], 'split': {'test_genes_file': 'data/raw/test_set.csv', 'train_ratio': 0.833, 'val_ratio': 0.167, 'seed': 42}, 'metrics': {'mae_top_k': 2000}, 'early_stopping': {'metric': 'combined'}, 'logging': {'log_interval': 100, 'save_interval': 1}, 'hardware': {'device': 'cuda'}}
scGPT - INFO - Loading data...
here1
scGPT - INFO - Dataset: 18080 genes, 17840 in vocab
scGPT - INFO - Test: 30, Train: 99, Val: 21 perts
here1
scGPT - INFO - Loading pretrained model from model/scGPT/best_model.pt
scGPT - INFO - Model config: embsize=512, nlayers=12, nheads=8
here1
scGPT - INFO - Train cells: 149364, Val cells: 28977
scGPT - INFO - Loading pretrained model from model/scGPT/best_model.pt
scGPT - INFO - Model config: embsize=512, nlayers=12, nheads=8
scGPT - INFO - Loading pretrained model from model/scGPT/best_model.pt
scGPT - INFO - Model config: embsize=512, nlayers=12, nheads=8
scGPT - INFO - Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter value_encoder.norm.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter value_encoder.norm.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
scGPT - INFO - 
============================================================
Starting training...
============================================================
scGPT - INFO - | epoch   1 | 100/2371 batches | ms/batch 498.83 | loss 30.2907
scGPT - INFO - | epoch   1 | 200/2371 batches | ms/batch 502.64 | loss 21.2855
scGPT - INFO - | epoch   1 | 300/2371 batches | ms/batch 516.55 | loss 20.8741
scGPT - INFO - | epoch   1 | 400/2371 batches | ms/batch 524.24 | loss 18.4933
scGPT - INFO - | epoch   1 | 500/2371 batches | ms/batch 523.34 | loss 24.2969
scGPT - INFO - | epoch   1 | 600/2371 batches | ms/batch 519.29 | loss 24.8555
scGPT - INFO - | epoch   1 | 700/2371 batches | ms/batch 521.35 | loss 21.7178
scGPT - INFO - | epoch   1 | 800/2371 batches | ms/batch 523.86 | loss 23.5433
scGPT - INFO - | epoch   1 | 900/2371 batches | ms/batch 519.06 | loss 22.0623
scGPT - INFO - | epoch   1 | 1000/2371 batches | ms/batch 524.34 | loss 23.4740
scGPT - INFO - | epoch   1 | 1100/2371 batches | ms/batch 522.09 | loss 20.7960
scGPT - INFO - | epoch   1 | 1200/2371 batches | ms/batch 521.44 | loss 20.4452
scGPT - INFO - | epoch   1 | 1300/2371 batches | ms/batch 518.30 | loss 21.1300
scGPT - INFO - | epoch   1 | 1400/2371 batches | ms/batch 520.94 | loss 20.6998
scGPT - INFO - | epoch   1 | 1500/2371 batches | ms/batch 522.52 | loss 23.6627
scGPT - INFO - | epoch   1 | 1600/2371 batches | ms/batch 520.18 | loss 22.6619
scGPT - INFO - | epoch   1 | 1700/2371 batches | ms/batch 522.06 | loss 21.1839
scGPT - INFO - | epoch   1 | 1800/2371 batches | ms/batch 520.05 | loss 21.1797
scGPT - INFO - | epoch   1 | 1900/2371 batches | ms/batch 517.79 | loss 18.7229
scGPT - INFO - | epoch   1 | 2000/2371 batches | ms/batch 518.44 | loss 20.4190
scGPT - INFO - | epoch   1 | 2100/2371 batches | ms/batch 519.00 | loss 20.1773
scGPT - INFO - | epoch   1 | 2200/2371 batches | ms/batch 519.58 | loss 21.6783
scGPT - INFO - | epoch   1 | 2300/2371 batches | ms/batch 519.28 | loss 19.3153
scGPT - INFO - | epoch   1 | time: 2887.29s | loss: 496.1612 | nPDS: 0.5278 | DES: 0.1931 | MAE: 9.1606
scGPT - INFO -   -> New best (combined=3.4984)
scGPT - INFO - | epoch   2 | 100/2371 batches | ms/batch 511.24 | loss 18.9554
scGPT - INFO - | epoch   2 | 200/2371 batches | ms/batch 519.58 | loss 21.9401
scGPT - INFO - | epoch   2 | 300/2371 batches | ms/batch 518.62 | loss 22.3275
scGPT - INFO - | epoch   2 | 400/2371 batches | ms/batch 520.51 | loss 19.0152
scGPT - INFO - | epoch   2 | 500/2371 batches | ms/batch 525.53 | loss 19.8470
scGPT - INFO - | epoch   2 | 600/2371 batches | ms/batch 518.71 | loss 20.0354
scGPT - INFO - | epoch   2 | 700/2371 batches | ms/batch 520.09 | loss 21.3540
scGPT - INFO - | epoch   2 | 800/2371 batches | ms/batch 519.00 | loss 19.2547
scGPT - INFO - | epoch   2 | 900/2371 batches | ms/batch 516.92 | loss 22.0072
scGPT - INFO - | epoch   2 | 1000/2371 batches | ms/batch 518.29 | loss 21.3445
scGPT - INFO - | epoch   2 | 1100/2371 batches | ms/batch 517.26 | loss 20.2333
scGPT - INFO - | epoch   2 | 1200/2371 batches | ms/batch 519.13 | loss 20.1850
scGPT - INFO - | epoch   2 | 1300/2371 batches | ms/batch 517.78 | loss 20.4085
scGPT - INFO - | epoch   2 | 1400/2371 batches | ms/batch 518.54 | loss 18.8436
scGPT - INFO - | epoch   2 | 1500/2371 batches | ms/batch 517.39 | loss 20.6867
scGPT - INFO - | epoch   2 | 1600/2371 batches | ms/batch 519.58 | loss 21.8091
scGPT - INFO - | epoch   2 | 1700/2371 batches | ms/batch 518.60 | loss 20.7000
scGPT - INFO - | epoch   2 | 1800/2371 batches | ms/batch 517.38 | loss 19.8186
scGPT - INFO - | epoch   2 | 1900/2371 batches | ms/batch 519.60 | loss 19.8885
scGPT - INFO - | epoch   2 | 2000/2371 batches | ms/batch 518.25 | loss 21.5448
scGPT - INFO - | epoch   2 | 2100/2371 batches | ms/batch 522.01 | loss 21.5978
scGPT - INFO - | epoch   2 | 2200/2371 batches | ms/batch 518.84 | loss 20.4727
scGPT - INFO - | epoch   2 | 2300/2371 batches | ms/batch 517.96 | loss 21.1960
scGPT - INFO - | epoch   2 | time: 2904.67s | loss: 462.0270 | nPDS: 0.5278 | DES: 0.1912 | MAE: 7.4118
scGPT - INFO -   -> New best (combined=2.9161)
scGPT - INFO - | epoch   3 | 100/2371 batches | ms/batch 513.28 | loss 21.0835
scGPT - INFO - | epoch   3 | 200/2371 batches | ms/batch 516.99 | loss 21.5332
scGPT - INFO - | epoch   3 | 300/2371 batches | ms/batch 523.71 | loss 20.3047
scGPT - INFO - | epoch   3 | 400/2371 batches | ms/batch 520.35 | loss 19.8965
scGPT - INFO - | epoch   3 | 500/2371 batches | ms/batch 524.10 | loss 22.3360
scGPT - INFO - | epoch   3 | 600/2371 batches | ms/batch 519.54 | loss 22.1042
scGPT - INFO - | epoch   3 | 700/2371 batches | ms/batch 517.57 | loss 21.3002
scGPT - INFO - | epoch   3 | 800/2371 batches | ms/batch 519.90 | loss 21.4881
scGPT - INFO - | epoch   3 | 900/2371 batches | ms/batch 519.30 | loss 20.2407
scGPT - INFO - | epoch   3 | 1000/2371 batches | ms/batch 518.55 | loss 22.1777
scGPT - INFO - | epoch   3 | 1100/2371 batches | ms/batch 515.09 | loss 18.7475
scGPT - INFO - | epoch   3 | 1200/2371 batches | ms/batch 518.31 | loss 18.8999
scGPT - INFO - | epoch   3 | 1300/2371 batches | ms/batch 518.99 | loss 19.7606
scGPT - INFO - | epoch   3 | 1400/2371 batches | ms/batch 518.92 | loss 19.9041
scGPT - INFO - | epoch   3 | 1500/2371 batches | ms/batch 516.13 | loss 22.6577
scGPT - INFO - | epoch   3 | 1600/2371 batches | ms/batch 516.81 | loss 22.0069
scGPT - INFO - | epoch   3 | 1700/2371 batches | ms/batch 516.69 | loss 19.5130
scGPT - INFO - | epoch   3 | 1800/2371 batches | ms/batch 517.72 | loss 21.2973
scGPT - INFO - | epoch   3 | 1900/2371 batches | ms/batch 517.80 | loss 21.9998
scGPT - INFO - | epoch   3 | 2000/2371 batches | ms/batch 519.44 | loss 20.4351
scGPT - INFO - | epoch   3 | 2100/2371 batches | ms/batch 516.95 | loss 18.3535
scGPT - INFO - | epoch   3 | 2200/2371 batches | ms/batch 518.68 | loss 21.6415
scGPT - INFO - | epoch   3 | 2300/2371 batches | ms/batch 517.84 | loss 19.7984
scGPT - INFO - | epoch   3 | time: 3849.56s | loss: 459.2501 | nPDS: 0.5278 | DES: 0.1902 | MAE: 6.5605
scGPT - INFO -   -> New best (combined=2.6327)
scGPT - INFO - | epoch   4 | 100/2371 batches | ms/batch 512.17 | loss 20.9364
scGPT - INFO - | epoch   4 | 200/2371 batches | ms/batch 516.18 | loss 20.2825
scGPT - INFO - | epoch   4 | 300/2371 batches | ms/batch 517.73 | loss 20.2130
scGPT - INFO - | epoch   4 | 400/2371 batches | ms/batch 516.91 | loss 21.0121
scGPT - INFO - | epoch   4 | 500/2371 batches | ms/batch 518.59 | loss 19.2740
scGPT - INFO - | epoch   4 | 600/2371 batches | ms/batch 517.04 | loss 20.3267
scGPT - INFO - | epoch   4 | 700/2371 batches | ms/batch 516.52 | loss 18.5867
scGPT - INFO - | epoch   4 | 800/2371 batches | ms/batch 519.96 | loss 20.5091
scGPT - INFO - | epoch   4 | 900/2371 batches | ms/batch 519.68 | loss 18.3874
scGPT - INFO - | epoch   4 | 1000/2371 batches | ms/batch 524.17 | loss 18.9922
scGPT - INFO - | epoch   4 | 1100/2371 batches | ms/batch 523.63 | loss 20.1726
scGPT - INFO - | epoch   4 | 1200/2371 batches | ms/batch 523.48 | loss 20.4568
scGPT - INFO - | epoch   4 | 1300/2371 batches | ms/batch 527.71 | loss 20.1566
scGPT - INFO - | epoch   4 | 1400/2371 batches | ms/batch 527.59 | loss 21.4689
scGPT - INFO - | epoch   4 | 1500/2371 batches | ms/batch 527.33 | loss 19.0806
scGPT - INFO - | epoch   4 | 1600/2371 batches | ms/batch 525.55 | loss 20.4206
scGPT - INFO - | epoch   4 | 1700/2371 batches | ms/batch 523.90 | loss 21.0909
scGPT - INFO - | epoch   4 | 1800/2371 batches | ms/batch 525.26 | loss 21.4611
scGPT - INFO - | epoch   4 | 1900/2371 batches | ms/batch 521.52 | loss 20.4078
scGPT - INFO - | epoch   4 | 2000/2371 batches | ms/batch 523.06 | loss 21.4077
scGPT - INFO - | epoch   4 | 2100/2371 batches | ms/batch 522.82 | loss 20.8848
scGPT - INFO - | epoch   4 | 2200/2371 batches | ms/batch 522.49 | loss 20.1962
scGPT - INFO - | epoch   4 | 2300/2371 batches | ms/batch 521.48 | loss 20.1219
scGPT - INFO - | epoch   4 | time: 3830.03s | loss: 462.1010 | nPDS: 0.5278 | DES: 0.1918 | MAE: 7.6478
scGPT - INFO - | epoch   5 | 100/2371 batches | ms/batch 514.69 | loss 20.2047
scGPT - INFO - | epoch   5 | 200/2371 batches | ms/batch 528.22 | loss 18.8210
scGPT - INFO - | epoch   5 | 300/2371 batches | ms/batch 530.78 | loss 20.3041
scGPT - INFO - | epoch   5 | 400/2371 batches | ms/batch 533.43 | loss 19.5845
scGPT - INFO - | epoch   5 | 500/2371 batches | ms/batch 528.95 | loss 19.9822
scGPT - INFO - | epoch   5 | 600/2371 batches | ms/batch 525.33 | loss 20.9402
scGPT - INFO - | epoch   5 | 700/2371 batches | ms/batch 527.39 | loss 20.7066
scGPT - INFO - | epoch   5 | 800/2371 batches | ms/batch 521.74 | loss 18.7133
scGPT - INFO - | epoch   5 | 900/2371 batches | ms/batch 524.21 | loss 18.6946
scGPT - INFO - | epoch   5 | 1000/2371 batches | ms/batch 521.45 | loss 20.8625
scGPT - INFO - | epoch   5 | 1100/2371 batches | ms/batch 522.22 | loss 18.9767
scGPT - INFO - | epoch   5 | 1200/2371 batches | ms/batch 522.04 | loss 22.1274
scGPT - INFO - | epoch   5 | 1300/2371 batches | ms/batch 519.84 | loss 19.6698
scGPT - INFO - | epoch   5 | 1400/2371 batches | ms/batch 517.05 | loss 20.4110
scGPT - INFO - | epoch   5 | 1500/2371 batches | ms/batch 519.40 | loss 20.8520
scGPT - INFO - | epoch   5 | 1600/2371 batches | ms/batch 516.30 | loss 18.7791
scGPT - INFO - | epoch   5 | 1700/2371 batches | ms/batch 519.32 | loss 22.4377
scGPT - INFO - | epoch   5 | 1800/2371 batches | ms/batch 518.70 | loss 18.6841
scGPT - INFO - | epoch   5 | 1900/2371 batches | ms/batch 516.66 | loss 19.6668
scGPT - INFO - | epoch   5 | 2000/2371 batches | ms/batch 517.05 | loss 21.1046
scGPT - INFO - | epoch   5 | 2100/2371 batches | ms/batch 520.92 | loss 21.3971
scGPT - INFO - | epoch   5 | 2200/2371 batches | ms/batch 518.95 | loss 21.6916
scGPT - INFO - | epoch   5 | 2300/2371 batches | ms/batch 516.09 | loss 20.4382
scGPT - INFO - | epoch   5 | time: 4181.75s | loss: 459.3259 | nPDS: 0.5278 | DES: 0.1918 | MAE: 7.6221
scGPT - INFO - | epoch   6 | 100/2371 batches | ms/batch 509.08 | loss 20.4798
scGPT - INFO - | epoch   6 | 200/2371 batches | ms/batch 515.03 | loss 20.8058
scGPT - INFO - | epoch   6 | 300/2371 batches | ms/batch 519.49 | loss 21.5749
scGPT - INFO - | epoch   6 | 400/2371 batches | ms/batch 515.40 | loss 18.7232
scGPT - INFO - | epoch   6 | 500/2371 batches | ms/batch 516.32 | loss 21.0321
scGPT - INFO - | epoch   6 | 600/2371 batches | ms/batch 515.29 | loss 20.4799
scGPT - INFO - | epoch   6 | 700/2371 batches | ms/batch 514.65 | loss 21.2627
scGPT - INFO - | epoch   6 | 800/2371 batches | ms/batch 514.56 | loss 21.4474
scGPT - INFO - | epoch   6 | 900/2371 batches | ms/batch 516.03 | loss 20.7704
scGPT - INFO - | epoch   6 | 1000/2371 batches | ms/batch 516.52 | loss 21.2000
scGPT - INFO - | epoch   6 | 1100/2371 batches | ms/batch 515.68 | loss 20.1864
scGPT - INFO - | epoch   6 | 1200/2371 batches | ms/batch 515.05 | loss 21.2737
scGPT - INFO - | epoch   6 | 1300/2371 batches | ms/batch 516.43 | loss 19.9572
scGPT - INFO - | epoch   6 | 1400/2371 batches | ms/batch 515.24 | loss 21.0840
scGPT - INFO - | epoch   6 | 1500/2371 batches | ms/batch 514.37 | loss 19.9214
scGPT - INFO - | epoch   6 | 1600/2371 batches | ms/batch 515.67 | loss 21.9735
scGPT - INFO - | epoch   6 | 1700/2371 batches | ms/batch 516.32 | loss 22.8263
scGPT - INFO - | epoch   6 | 1800/2371 batches | ms/batch 514.19 | loss 20.5295
scGPT - INFO - | epoch   6 | 1900/2371 batches | ms/batch 516.35 | loss 19.9868
scGPT - INFO - | epoch   6 | 2000/2371 batches | ms/batch 515.17 | loss 18.3089
scGPT - INFO - | epoch   6 | 2100/2371 batches | ms/batch 516.54 | loss 20.1884
scGPT - INFO - | epoch   6 | 2200/2371 batches | ms/batch 515.12 | loss 21.0835
scGPT - INFO - | epoch   6 | 2300/2371 batches | ms/batch 515.11 | loss 21.3898
scGPT - INFO - | epoch   6 | time: 4089.00s | loss: 451.4523 | nPDS: 0.5278 | DES: 0.1930 | MAE: 8.5266
scGPT - INFO - | epoch   7 | 100/2371 batches | ms/batch 511.71 | loss 21.2495
scGPT - INFO - | epoch   7 | 200/2371 batches | ms/batch 516.90 | loss 20.8376
scGPT - INFO - | epoch   7 | 300/2371 batches | ms/batch 517.76 | loss 19.3391
scGPT - INFO - | epoch   7 | 400/2371 batches | ms/batch 517.69 | loss 19.8219
scGPT - INFO - | epoch   7 | 500/2371 batches | ms/batch 517.80 | loss 20.9393
scGPT - INFO - | epoch   7 | 600/2371 batches | ms/batch 518.09 | loss 19.4379
scGPT - INFO - | epoch   7 | 700/2371 batches | ms/batch 517.45 | loss 19.2166
scGPT - INFO - | epoch   7 | 800/2371 batches | ms/batch 517.23 | loss 19.8802
scGPT - INFO - | epoch   7 | 900/2371 batches | ms/batch 517.57 | loss 18.4947
scGPT - INFO - | epoch   7 | 1000/2371 batches | ms/batch 517.36 | loss 21.4457
scGPT - INFO - | epoch   7 | 1100/2371 batches | ms/batch 516.77 | loss 19.7443
scGPT - INFO - | epoch   7 | 1200/2371 batches | ms/batch 515.17 | loss 22.3380
scGPT - INFO - | epoch   7 | 1300/2371 batches | ms/batch 514.96 | loss 17.6388
scGPT - INFO - | epoch   7 | 1400/2371 batches | ms/batch 515.74 | loss 19.5980
scGPT - INFO - | epoch   7 | 1500/2371 batches | ms/batch 515.02 | loss 19.8225
scGPT - INFO - | epoch   7 | 1600/2371 batches | ms/batch 515.79 | loss 19.7140
scGPT - INFO - | epoch   7 | 1700/2371 batches | ms/batch 516.07 | loss 18.1570
scGPT - INFO - | epoch   7 | 1800/2371 batches | ms/batch 514.93 | loss 19.9499
scGPT - INFO - | epoch   7 | 1900/2371 batches | ms/batch 515.84 | loss 21.1690
scGPT - INFO - | epoch   7 | 2000/2371 batches | ms/batch 515.64 | loss 17.7338
scGPT - INFO - | epoch   7 | 2100/2371 batches | ms/batch 514.77 | loss 20.4760
scGPT - INFO - | epoch   7 | 2200/2371 batches | ms/batch 516.99 | loss 17.3088
scGPT - INFO - | epoch   7 | 2300/2371 batches | ms/batch 515.27 | loss 19.6075
scGPT - INFO - | epoch   7 | time: 4254.71s | loss: 456.8343 | nPDS: 0.5278 | DES: 0.1916 | MAE: 7.4355
scGPT - INFO - | epoch   8 | 100/2371 batches | ms/batch 512.36 | loss 19.4209
scGPT - INFO - | epoch   8 | 200/2371 batches | ms/batch 516.88 | loss 20.0470
scGPT - INFO - | epoch   8 | 300/2371 batches | ms/batch 516.31 | loss 20.0015
scGPT - INFO - | epoch   8 | 400/2371 batches | ms/batch 519.48 | loss 19.4270
scGPT - INFO - | epoch   8 | 500/2371 batches | ms/batch 518.78 | loss 20.5108
scGPT - INFO - | epoch   8 | 600/2371 batches | ms/batch 518.14 | loss 20.0183
scGPT - INFO - | epoch   8 | 700/2371 batches | ms/batch 516.71 | loss 20.6528
scGPT - INFO - | epoch   8 | 800/2371 batches | ms/batch 515.54 | loss 22.3195
scGPT - INFO - | epoch   8 | 900/2371 batches | ms/batch 514.17 | loss 20.6590
scGPT - INFO - | epoch   8 | 1000/2371 batches | ms/batch 517.65 | loss 20.4178
scGPT - INFO - | epoch   8 | 1100/2371 batches | ms/batch 517.01 | loss 20.3531
scGPT - INFO - | epoch   8 | 1200/2371 batches | ms/batch 516.61 | loss 19.9424
scGPT - INFO - | epoch   8 | 1300/2371 batches | ms/batch 517.23 | loss 20.3271
scGPT - INFO - | epoch   8 | 1400/2371 batches | ms/batch 514.96 | loss 19.5837
scGPT - INFO - | epoch   8 | 1500/2371 batches | ms/batch 514.67 | loss 22.8351
scGPT - INFO - | epoch   8 | 1600/2371 batches | ms/batch 517.07 | loss 20.1199
scGPT - INFO - | epoch   8 | 1700/2371 batches | ms/batch 517.06 | loss 18.4837
scGPT - INFO - | epoch   8 | 1800/2371 batches | ms/batch 516.20 | loss 19.1730
scGPT - INFO - | epoch   8 | 1900/2371 batches | ms/batch 514.70 | loss 19.5380
scGPT - INFO - | epoch   8 | 2000/2371 batches | ms/batch 516.54 | loss 20.4176
scGPT - INFO - | epoch   8 | 2100/2371 batches | ms/batch 517.66 | loss 19.3248
scGPT - INFO - | epoch   8 | 2200/2371 batches | ms/batch 516.50 | loss 19.4517
scGPT - INFO - | epoch   8 | 2300/2371 batches | ms/batch 516.64 | loss 20.9247
scGPT - INFO - | epoch   8 | time: 4293.85s | loss: 448.4579 | nPDS: 0.5278 | DES: 0.1921 | MAE: 8.0387
scGPT - INFO - | epoch   9 | 100/2371 batches | ms/batch 536.67 | loss 20.8707
scGPT - INFO - | epoch   9 | 200/2371 batches | ms/batch 539.02 | loss 20.0645
scGPT - INFO - | epoch   9 | 300/2371 batches | ms/batch 543.23 | loss 20.1245
scGPT - INFO - | epoch   9 | 400/2371 batches | ms/batch 535.65 | loss 17.6738
scGPT - INFO - | epoch   9 | 500/2371 batches | ms/batch 538.55 | loss 19.9711
scGPT - INFO - | epoch   9 | 600/2371 batches | ms/batch 539.34 | loss 19.8842
scGPT - INFO - | epoch   9 | 700/2371 batches | ms/batch 537.80 | loss 19.4671
scGPT - INFO - | epoch   9 | 800/2371 batches | ms/batch 534.73 | loss 18.7380
scGPT - INFO - | epoch   9 | 900/2371 batches | ms/batch 538.50 | loss 18.8544
scGPT - INFO - | epoch   9 | 1000/2371 batches | ms/batch 534.54 | loss 21.8124
scGPT - INFO - | epoch   9 | 1100/2371 batches | ms/batch 535.34 | loss 20.1291
scGPT - INFO - | epoch   9 | 1200/2371 batches | ms/batch 530.82 | loss 18.6015
scGPT - INFO - | epoch   9 | 1300/2371 batches | ms/batch 538.72 | loss 20.2249
scGPT - INFO - | epoch   9 | 1400/2371 batches | ms/batch 534.58 | loss 21.5878
scGPT - INFO - | epoch   9 | 1500/2371 batches | ms/batch 533.96 | loss 20.7783
scGPT - INFO - | epoch   9 | 1600/2371 batches | ms/batch 537.28 | loss 19.5582
scGPT - INFO - | epoch   9 | 1700/2371 batches | ms/batch 536.25 | loss 19.7021
scGPT - INFO - | epoch   9 | 1800/2371 batches | ms/batch 537.07 | loss 18.8856
scGPT - INFO - | epoch   9 | 1900/2371 batches | ms/batch 533.69 | loss 20.9634
scGPT - INFO - | epoch   9 | 2000/2371 batches | ms/batch 532.80 | loss 20.4746
scGPT - INFO - | epoch   9 | 2100/2371 batches | ms/batch 534.38 | loss 19.0960
scGPT - INFO - | epoch   9 | 2200/2371 batches | ms/batch 536.06 | loss 21.3821
scGPT - INFO - | epoch   9 | 2300/2371 batches | ms/batch 531.84 | loss 19.8996
scGPT - INFO - | epoch   9 | time: 4225.77s | loss: 449.6163 | nPDS: 0.5278 | DES: 0.1929 | MAE: 8.4403
scGPT - INFO - | epoch  10 | 100/2371 batches | ms/batch 540.15 | loss 21.1893
scGPT - INFO - | epoch  10 | 200/2371 batches | ms/batch 540.70 | loss 19.3262
scGPT - INFO - | epoch  10 | 300/2371 batches | ms/batch 537.58 | loss 21.9180
scGPT - INFO - | epoch  10 | 400/2371 batches | ms/batch 542.01 | loss 19.2519
scGPT - INFO - | epoch  10 | 500/2371 batches | ms/batch 540.85 | loss 19.6687
scGPT - INFO - | epoch  10 | 600/2371 batches | ms/batch 533.07 | loss 20.9610
scGPT - INFO - | epoch  10 | 700/2371 batches | ms/batch 542.30 | loss 18.0357
scGPT - INFO - | epoch  10 | 800/2371 batches | ms/batch 534.96 | loss 19.2686
scGPT - INFO - | epoch  10 | 900/2371 batches | ms/batch 538.84 | loss 19.4955
scGPT - INFO - | epoch  10 | 1000/2371 batches | ms/batch 536.55 | loss 18.8271
scGPT - INFO - | epoch  10 | 1100/2371 batches | ms/batch 537.91 | loss 21.0358
scGPT - INFO - | epoch  10 | 1200/2371 batches | ms/batch 533.75 | loss 21.3854
scGPT - INFO - | epoch  10 | 1300/2371 batches | ms/batch 534.49 | loss 19.9318
scGPT - INFO - | epoch  10 | 1400/2371 batches | ms/batch 539.27 | loss 20.4550
scGPT - INFO - | epoch  10 | 1500/2371 batches | ms/batch 535.96 | loss 20.0655
scGPT - INFO - | epoch  10 | 1600/2371 batches | ms/batch 536.72 | loss 20.5856
scGPT - INFO - | epoch  10 | 1700/2371 batches | ms/batch 539.27 | loss 20.7651
scGPT - INFO - | epoch  10 | 1800/2371 batches | ms/batch 536.95 | loss 21.8485
scGPT - INFO - | epoch  10 | 1900/2371 batches | ms/batch 536.68 | loss 20.6927
scGPT - INFO - | epoch  10 | 2000/2371 batches | ms/batch 535.55 | loss 19.6585
scGPT - INFO - | epoch  10 | 2100/2371 batches | ms/batch 537.59 | loss 20.6606
scGPT - INFO - | epoch  10 | 2200/2371 batches | ms/batch 540.75 | loss 20.2748
scGPT - INFO - | epoch  10 | 2300/2371 batches | ms/batch 543.61 | loss 20.3774
scGPT - INFO - | epoch  10 | time: 4405.90s | loss: 450.4604 | nPDS: 0.5278 | DES: 0.1919 | MAE: 7.6256
scGPT - INFO - | epoch  11 | 100/2371 batches | ms/batch 535.28 | loss 18.8776
scGPT - INFO - | epoch  11 | 200/2371 batches | ms/batch 538.96 | loss 17.9211
scGPT - INFO - | epoch  11 | 300/2371 batches | ms/batch 541.31 | loss 21.4035
scGPT - INFO - | epoch  11 | 400/2371 batches | ms/batch 536.84 | loss 18.4564
scGPT - INFO - | epoch  11 | 500/2371 batches | ms/batch 536.10 | loss 19.9764
scGPT - INFO - | epoch  11 | 600/2371 batches | ms/batch 538.87 | loss 20.1214
scGPT - INFO - | epoch  11 | 700/2371 batches | ms/batch 536.10 | loss 20.7066
scGPT - INFO - | epoch  11 | 800/2371 batches | ms/batch 534.25 | loss 19.1886
scGPT - INFO - | epoch  11 | 900/2371 batches | ms/batch 535.21 | loss 17.9583
scGPT - INFO - | epoch  11 | 1000/2371 batches | ms/batch 534.83 | loss 18.8609
scGPT - INFO - | epoch  11 | 1100/2371 batches | ms/batch 539.27 | loss 21.1651
scGPT - INFO - | epoch  11 | 1200/2371 batches | ms/batch 537.91 | loss 21.2723
scGPT - INFO - | epoch  11 | 1300/2371 batches | ms/batch 535.88 | loss 20.7695
scGPT - INFO - | epoch  11 | 1400/2371 batches | ms/batch 534.67 | loss 19.0375
scGPT - INFO - | epoch  11 | 1500/2371 batches | ms/batch 537.01 | loss 22.8623
scGPT - INFO - | epoch  11 | 1600/2371 batches | ms/batch 537.01 | loss 22.2025
scGPT - INFO - | epoch  11 | 1700/2371 batches | ms/batch 536.27 | loss 18.8014
scGPT - INFO - | epoch  11 | 1800/2371 batches | ms/batch 533.29 | loss 21.5488
scGPT - INFO - | epoch  11 | 1900/2371 batches | ms/batch 533.69 | loss 17.6743
scGPT - INFO - | epoch  11 | 2000/2371 batches | ms/batch 532.70 | loss 19.7828
scGPT - INFO - | epoch  11 | 2100/2371 batches | ms/batch 531.74 | loss 20.6797
scGPT - INFO - | epoch  11 | 2200/2371 batches | ms/batch 534.66 | loss 20.3488
scGPT - INFO - | epoch  11 | 2300/2371 batches | ms/batch 534.84 | loss 19.4074
scGPT - INFO - | epoch  11 | time: 4446.11s | loss: 445.7731 | nPDS: 0.5278 | DES: 0.1923 | MAE: 8.2901
scGPT - INFO - | epoch  12 | 100/2371 batches | ms/batch 533.70 | loss 18.4117
scGPT - INFO - | epoch  12 | 200/2371 batches | ms/batch 536.19 | loss 20.5446
scGPT - INFO - | epoch  12 | 300/2371 batches | ms/batch 534.02 | loss 18.8386
scGPT - INFO - | epoch  12 | 400/2371 batches | ms/batch 534.87 | loss 20.2370
scGPT - INFO - | epoch  12 | 500/2371 batches | ms/batch 535.03 | loss 17.7185
scGPT - INFO - | epoch  12 | 600/2371 batches | ms/batch 534.44 | loss 21.7907
scGPT - INFO - | epoch  12 | 700/2371 batches | ms/batch 537.24 | loss 21.1792
scGPT - INFO - | epoch  12 | 800/2371 batches | ms/batch 537.40 | loss 21.6323
scGPT - INFO - | epoch  12 | 900/2371 batches | ms/batch 541.07 | loss 20.4848
scGPT - INFO - | epoch  12 | 1000/2371 batches | ms/batch 535.37 | loss 20.9214
scGPT - INFO - | epoch  12 | 1100/2371 batches | ms/batch 535.86 | loss 20.2242
scGPT - INFO - | epoch  12 | 1200/2371 batches | ms/batch 531.20 | loss 20.4309
scGPT - INFO - | epoch  12 | 1300/2371 batches | ms/batch 532.61 | loss 20.7589
scGPT - INFO - | epoch  12 | 1400/2371 batches | ms/batch 537.48 | loss 20.6844
scGPT - INFO - | epoch  12 | 1500/2371 batches | ms/batch 533.48 | loss 20.1269
scGPT - INFO - | epoch  12 | 1600/2371 batches | ms/batch 536.91 | loss 18.9688
scGPT - INFO - | epoch  12 | 1700/2371 batches | ms/batch 535.56 | loss 18.9785
scGPT - INFO - | epoch  12 | 1800/2371 batches | ms/batch 534.05 | loss 20.6166
scGPT - INFO - | epoch  12 | 1900/2371 batches | ms/batch 532.71 | loss 20.8086
scGPT - INFO - | epoch  12 | 2000/2371 batches | ms/batch 538.04 | loss 19.9194
scGPT - INFO - | epoch  12 | 2100/2371 batches | ms/batch 538.41 | loss 19.4597
scGPT - INFO - | epoch  12 | 2200/2371 batches | ms/batch 533.29 | loss 20.3081
scGPT - INFO - | epoch  12 | 2300/2371 batches | ms/batch 534.95 | loss 23.3606
scGPT - INFO - | epoch  12 | time: 4409.92s | loss: 449.8769 | nPDS: 0.5278 | DES: 0.1924 | MAE: 8.3422
scGPT - INFO - | epoch  13 | 100/2371 batches | ms/batch 531.29 | loss 17.7812
scGPT - INFO - | epoch  13 | 200/2371 batches | ms/batch 534.41 | loss 19.7981
scGPT - INFO - | epoch  13 | 300/2371 batches | ms/batch 535.68 | loss 20.8330
scGPT - INFO - | epoch  13 | 400/2371 batches | ms/batch 535.26 | loss 18.9666
scGPT - INFO - | epoch  13 | 500/2371 batches | ms/batch 536.68 | loss 20.0763
scGPT - INFO - | epoch  13 | 600/2371 batches | ms/batch 539.34 | loss 21.4839
scGPT - INFO - | epoch  13 | 700/2371 batches | ms/batch 535.62 | loss 21.8842
scGPT - INFO - | epoch  13 | 800/2371 batches | ms/batch 534.76 | loss 19.6331
scGPT - INFO - | epoch  13 | 900/2371 batches | ms/batch 538.38 | loss 21.3151
scGPT - INFO - | epoch  13 | 1000/2371 batches | ms/batch 534.48 | loss 22.2925
scGPT - INFO - | epoch  13 | 1100/2371 batches | ms/batch 532.10 | loss 20.1451
scGPT - INFO - | epoch  13 | 1200/2371 batches | ms/batch 533.18 | loss 20.2156
scGPT - INFO - | epoch  13 | 1300/2371 batches | ms/batch 534.48 | loss 20.4216
scGPT - INFO - | epoch  13 | 1400/2371 batches | ms/batch 535.14 | loss 20.8281
scGPT - INFO - | epoch  13 | 1500/2371 batches | ms/batch 534.50 | loss 19.9556
scGPT - INFO - | epoch  13 | 1600/2371 batches | ms/batch 536.84 | loss 18.5523
scGPT - INFO - | epoch  13 | 1700/2371 batches | ms/batch 535.20 | loss 19.6425
scGPT - INFO - | epoch  13 | 1800/2371 batches | ms/batch 534.25 | loss 20.0907
scGPT - INFO - | epoch  13 | 1900/2371 batches | ms/batch 534.09 | loss 20.4069
scGPT - INFO - | epoch  13 | 2000/2371 batches | ms/batch 535.49 | loss 19.5236
scGPT - INFO - | epoch  13 | 2100/2371 batches | ms/batch 532.51 | loss 21.6602
scGPT - INFO - | epoch  13 | 2200/2371 batches | ms/batch 535.69 | loss 20.8505
scGPT - INFO - | epoch  13 | 2300/2371 batches | ms/batch 535.71 | loss 19.8376
scGPT - INFO - | epoch  13 | time: 4459.23s | loss: 452.9308 | nPDS: 0.5278 | DES: 0.1918 | MAE: 7.6167
scGPT - INFO - Early stopping at epoch 13
scGPT - INFO - 
============================================================
Training complete! Best: {'pds': 0.5277777777777778, 'des': 0.1901616576128418, 'mae_top2k': 6.560503800710042, 'combined': 2.632706640291659}
============================================================
==========================================
Step 3: Evaluating finetuned model on held-out test genes...
==========================================
