23:00:00-scGPT-INFO-setup_logging: Running on 2025-12-01 23:00:00
23:00:00-scGPT-INFO-main: Config: {'paths': {'gears_data_dir': 'data/processed/gears', 'dataset_name': 'vcc', 'pretrained_model_dir': 'model/scGPT', 'output_dir': 'model/scGPT_finetuned'}, 'data': {'pad_token': '<pad>', 'special_tokens': ['<pad>', '<cls>', '<eoc>'], 'pad_value': 0, 'pert_pad_id': 0, 'include_zero_gene': 'batch-wise', 'max_seq_len': 1536}, 'training': {'MLM': True, 'CLS': False, 'CCE': False, 'MVC': False, 'ECS': False, 'amp': True}, 'optimizer': {'lr': 0.0001, 'batch_size': 64, 'eval_batch_size': 64, 'epochs': 15, 'schedule_interval': 1, 'schedule_gamma': 0.9, 'early_stop': 10, 'grad_clip': 1.0}, 'model': {'embsize': 512, 'd_hid': 512, 'nlayers': 12, 'nhead': 8, 'n_layers_cls': 3, 'dropout': 0.0, 'use_fast_transformer': False}, 'load_param_prefixes': ['encoder', 'value_encoder', 'transformer_encoder'], 'split': {'test_genes_file': 'data/raw/test_set.csv', 'train_ratio': 0.833, 'val_ratio': 0.167, 'seed': 42}, 'metrics': {'mae_top_k': 2000}, 'early_stopping': {'metric': 'combined'}, 'logging': {'log_interval': 100, 'save_interval': 1}, 'hardware': {'device': 'cuda'}}
23:00:00-scGPT-INFO-main: Loading data...
23:02:11-scGPT-INFO-main: Dataset: 18080 genes, 17840 in vocab
23:02:11-scGPT-INFO-main: Test: 30, Train: 99, Val: 21 perts
23:02:12-scGPT-INFO-main: Train cells: 149364, Val cells: 28977
23:02:12-scGPT-INFO-load_pretrained_model: Loading pretrained model from model/scGPT/best_model.pt
23:02:12-scGPT-INFO-load_pretrained_model: Model config: embsize=512, nlayers=12, nheads=8
23:02:14-scGPT-INFO-load_pretrained: Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
23:02:14-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
23:02:14-scGPT-INFO-main: 
============================================================
Starting training...
============================================================
23:03:04-scGPT-INFO-train_epoch: | epoch   1 | 100/2371 batches | ms/batch 498.83 | loss 30.2907
23:03:54-scGPT-INFO-train_epoch: | epoch   1 | 200/2371 batches | ms/batch 502.64 | loss 21.2855
23:04:46-scGPT-INFO-train_epoch: | epoch   1 | 300/2371 batches | ms/batch 516.55 | loss 20.8741
23:05:38-scGPT-INFO-train_epoch: | epoch   1 | 400/2371 batches | ms/batch 524.24 | loss 18.4933
23:06:30-scGPT-INFO-train_epoch: | epoch   1 | 500/2371 batches | ms/batch 523.34 | loss 24.2969
23:07:22-scGPT-INFO-train_epoch: | epoch   1 | 600/2371 batches | ms/batch 519.29 | loss 24.8555
23:08:15-scGPT-INFO-train_epoch: | epoch   1 | 700/2371 batches | ms/batch 521.35 | loss 21.7178
23:09:07-scGPT-INFO-train_epoch: | epoch   1 | 800/2371 batches | ms/batch 523.86 | loss 23.5433
23:09:59-scGPT-INFO-train_epoch: | epoch   1 | 900/2371 batches | ms/batch 519.06 | loss 22.0623
23:10:51-scGPT-INFO-train_epoch: | epoch   1 | 1000/2371 batches | ms/batch 524.34 | loss 23.4740
23:11:43-scGPT-INFO-train_epoch: | epoch   1 | 1100/2371 batches | ms/batch 522.09 | loss 20.7960
23:12:36-scGPT-INFO-train_epoch: | epoch   1 | 1200/2371 batches | ms/batch 521.44 | loss 20.4452
23:13:27-scGPT-INFO-train_epoch: | epoch   1 | 1300/2371 batches | ms/batch 518.30 | loss 21.1300
23:14:20-scGPT-INFO-train_epoch: | epoch   1 | 1400/2371 batches | ms/batch 520.94 | loss 20.6998
23:15:12-scGPT-INFO-train_epoch: | epoch   1 | 1500/2371 batches | ms/batch 522.52 | loss 23.6627
23:16:04-scGPT-INFO-train_epoch: | epoch   1 | 1600/2371 batches | ms/batch 520.18 | loss 22.6619
23:16:56-scGPT-INFO-train_epoch: | epoch   1 | 1700/2371 batches | ms/batch 522.06 | loss 21.1839
23:17:48-scGPT-INFO-train_epoch: | epoch   1 | 1800/2371 batches | ms/batch 520.05 | loss 21.1797
23:18:40-scGPT-INFO-train_epoch: | epoch   1 | 1900/2371 batches | ms/batch 517.79 | loss 18.7229
23:19:32-scGPT-INFO-train_epoch: | epoch   1 | 2000/2371 batches | ms/batch 518.44 | loss 20.4190
23:20:24-scGPT-INFO-train_epoch: | epoch   1 | 2100/2371 batches | ms/batch 519.00 | loss 20.1773
23:21:16-scGPT-INFO-train_epoch: | epoch   1 | 2200/2371 batches | ms/batch 519.58 | loss 21.6783
23:22:07-scGPT-INFO-train_epoch: | epoch   1 | 2300/2371 batches | ms/batch 519.28 | loss 19.3153
23:50:21-scGPT-INFO-main: | epoch   1 | time: 2887.29s | loss: 496.1612 | nPDS: 0.5278 | DES: 0.1931 | MAE: 9.1606
23:50:21-scGPT-INFO-main:   -> New best (combined=3.4984)
23:51:12-scGPT-INFO-train_epoch: | epoch   2 | 100/2371 batches | ms/batch 511.24 | loss 18.9554
23:52:04-scGPT-INFO-train_epoch: | epoch   2 | 200/2371 batches | ms/batch 519.58 | loss 21.9401
23:52:56-scGPT-INFO-train_epoch: | epoch   2 | 300/2371 batches | ms/batch 518.62 | loss 22.3275
23:53:48-scGPT-INFO-train_epoch: | epoch   2 | 400/2371 batches | ms/batch 520.51 | loss 19.0152
23:54:41-scGPT-INFO-train_epoch: | epoch   2 | 500/2371 batches | ms/batch 525.53 | loss 19.8470
23:55:33-scGPT-INFO-train_epoch: | epoch   2 | 600/2371 batches | ms/batch 518.71 | loss 20.0354
23:56:25-scGPT-INFO-train_epoch: | epoch   2 | 700/2371 batches | ms/batch 520.09 | loss 21.3540
23:57:17-scGPT-INFO-train_epoch: | epoch   2 | 800/2371 batches | ms/batch 519.00 | loss 19.2547
23:58:08-scGPT-INFO-train_epoch: | epoch   2 | 900/2371 batches | ms/batch 516.92 | loss 22.0072
23:59:00-scGPT-INFO-train_epoch: | epoch   2 | 1000/2371 batches | ms/batch 518.29 | loss 21.3445
23:59:52-scGPT-INFO-train_epoch: | epoch   2 | 1100/2371 batches | ms/batch 517.26 | loss 20.2333
00:00:44-scGPT-INFO-train_epoch: | epoch   2 | 1200/2371 batches | ms/batch 519.13 | loss 20.1850
00:01:36-scGPT-INFO-train_epoch: | epoch   2 | 1300/2371 batches | ms/batch 517.78 | loss 20.4085
00:02:27-scGPT-INFO-train_epoch: | epoch   2 | 1400/2371 batches | ms/batch 518.54 | loss 18.8436
00:03:19-scGPT-INFO-train_epoch: | epoch   2 | 1500/2371 batches | ms/batch 517.39 | loss 20.6867
00:04:11-scGPT-INFO-train_epoch: | epoch   2 | 1600/2371 batches | ms/batch 519.58 | loss 21.8091
00:05:03-scGPT-INFO-train_epoch: | epoch   2 | 1700/2371 batches | ms/batch 518.60 | loss 20.7000
00:05:55-scGPT-INFO-train_epoch: | epoch   2 | 1800/2371 batches | ms/batch 517.38 | loss 19.8186
00:06:47-scGPT-INFO-train_epoch: | epoch   2 | 1900/2371 batches | ms/batch 519.60 | loss 19.8885
00:07:38-scGPT-INFO-train_epoch: | epoch   2 | 2000/2371 batches | ms/batch 518.25 | loss 21.5448
00:08:31-scGPT-INFO-train_epoch: | epoch   2 | 2100/2371 batches | ms/batch 522.01 | loss 21.5978
00:09:23-scGPT-INFO-train_epoch: | epoch   2 | 2200/2371 batches | ms/batch 518.84 | loss 20.4727
00:10:14-scGPT-INFO-train_epoch: | epoch   2 | 2300/2371 batches | ms/batch 517.96 | loss 21.1960
00:38:46-scGPT-INFO-main: | epoch   2 | time: 2904.67s | loss: 462.0270 | nPDS: 0.5278 | DES: 0.1912 | MAE: 7.4118
00:38:46-scGPT-INFO-main:   -> New best (combined=2.9161)
00:39:37-scGPT-INFO-train_epoch: | epoch   3 | 100/2371 batches | ms/batch 513.28 | loss 21.0835
00:40:29-scGPT-INFO-train_epoch: | epoch   3 | 200/2371 batches | ms/batch 516.99 | loss 21.5332
00:41:21-scGPT-INFO-train_epoch: | epoch   3 | 300/2371 batches | ms/batch 523.71 | loss 20.3047
00:42:13-scGPT-INFO-train_epoch: | epoch   3 | 400/2371 batches | ms/batch 520.35 | loss 19.8965
00:43:06-scGPT-INFO-train_epoch: | epoch   3 | 500/2371 batches | ms/batch 524.10 | loss 22.3360
00:43:58-scGPT-INFO-train_epoch: | epoch   3 | 600/2371 batches | ms/batch 519.54 | loss 22.1042
00:44:50-scGPT-INFO-train_epoch: | epoch   3 | 700/2371 batches | ms/batch 517.57 | loss 21.3002
00:45:41-scGPT-INFO-train_epoch: | epoch   3 | 800/2371 batches | ms/batch 519.90 | loss 21.4881
00:46:33-scGPT-INFO-train_epoch: | epoch   3 | 900/2371 batches | ms/batch 519.30 | loss 20.2407
00:47:25-scGPT-INFO-train_epoch: | epoch   3 | 1000/2371 batches | ms/batch 518.55 | loss 22.1777
00:48:17-scGPT-INFO-train_epoch: | epoch   3 | 1100/2371 batches | ms/batch 515.09 | loss 18.7475
00:49:09-scGPT-INFO-train_epoch: | epoch   3 | 1200/2371 batches | ms/batch 518.31 | loss 18.8999
00:50:01-scGPT-INFO-train_epoch: | epoch   3 | 1300/2371 batches | ms/batch 518.99 | loss 19.7606
00:50:52-scGPT-INFO-train_epoch: | epoch   3 | 1400/2371 batches | ms/batch 518.92 | loss 19.9041
00:51:44-scGPT-INFO-train_epoch: | epoch   3 | 1500/2371 batches | ms/batch 516.13 | loss 22.6577
00:52:36-scGPT-INFO-train_epoch: | epoch   3 | 1600/2371 batches | ms/batch 516.81 | loss 22.0069
00:53:27-scGPT-INFO-train_epoch: | epoch   3 | 1700/2371 batches | ms/batch 516.69 | loss 19.5130
00:54:19-scGPT-INFO-train_epoch: | epoch   3 | 1800/2371 batches | ms/batch 517.72 | loss 21.2973
00:55:11-scGPT-INFO-train_epoch: | epoch   3 | 1900/2371 batches | ms/batch 517.80 | loss 21.9998
00:56:03-scGPT-INFO-train_epoch: | epoch   3 | 2000/2371 batches | ms/batch 519.44 | loss 20.4351
00:56:55-scGPT-INFO-train_epoch: | epoch   3 | 2100/2371 batches | ms/batch 516.95 | loss 18.3535
00:57:46-scGPT-INFO-train_epoch: | epoch   3 | 2200/2371 batches | ms/batch 518.68 | loss 21.6415
00:58:38-scGPT-INFO-train_epoch: | epoch   3 | 2300/2371 batches | ms/batch 517.84 | loss 19.7984
01:42:56-scGPT-INFO-main: | epoch   3 | time: 3849.56s | loss: 459.2501 | nPDS: 0.5278 | DES: 0.1902 | MAE: 6.5605
01:42:56-scGPT-INFO-main:   -> New best (combined=2.6327)
01:43:47-scGPT-INFO-train_epoch: | epoch   4 | 100/2371 batches | ms/batch 512.17 | loss 20.9364
01:44:38-scGPT-INFO-train_epoch: | epoch   4 | 200/2371 batches | ms/batch 516.18 | loss 20.2825
01:45:30-scGPT-INFO-train_epoch: | epoch   4 | 300/2371 batches | ms/batch 517.73 | loss 20.2130
01:46:22-scGPT-INFO-train_epoch: | epoch   4 | 400/2371 batches | ms/batch 516.91 | loss 21.0121
01:47:14-scGPT-INFO-train_epoch: | epoch   4 | 500/2371 batches | ms/batch 518.59 | loss 19.2740
01:48:05-scGPT-INFO-train_epoch: | epoch   4 | 600/2371 batches | ms/batch 517.04 | loss 20.3267
01:48:57-scGPT-INFO-train_epoch: | epoch   4 | 700/2371 batches | ms/batch 516.52 | loss 18.5867
01:49:49-scGPT-INFO-train_epoch: | epoch   4 | 800/2371 batches | ms/batch 519.96 | loss 20.5091
01:50:41-scGPT-INFO-train_epoch: | epoch   4 | 900/2371 batches | ms/batch 519.68 | loss 18.3874
01:51:33-scGPT-INFO-train_epoch: | epoch   4 | 1000/2371 batches | ms/batch 524.17 | loss 18.9922
01:52:26-scGPT-INFO-train_epoch: | epoch   4 | 1100/2371 batches | ms/batch 523.63 | loss 20.1726
01:53:18-scGPT-INFO-train_epoch: | epoch   4 | 1200/2371 batches | ms/batch 523.48 | loss 20.4568
01:54:11-scGPT-INFO-train_epoch: | epoch   4 | 1300/2371 batches | ms/batch 527.71 | loss 20.1566
01:55:04-scGPT-INFO-train_epoch: | epoch   4 | 1400/2371 batches | ms/batch 527.59 | loss 21.4689
01:55:56-scGPT-INFO-train_epoch: | epoch   4 | 1500/2371 batches | ms/batch 527.33 | loss 19.0806
01:56:49-scGPT-INFO-train_epoch: | epoch   4 | 1600/2371 batches | ms/batch 525.55 | loss 20.4206
01:57:41-scGPT-INFO-train_epoch: | epoch   4 | 1700/2371 batches | ms/batch 523.90 | loss 21.0909
01:58:34-scGPT-INFO-train_epoch: | epoch   4 | 1800/2371 batches | ms/batch 525.26 | loss 21.4611
01:59:26-scGPT-INFO-train_epoch: | epoch   4 | 1900/2371 batches | ms/batch 521.52 | loss 20.4078
02:00:18-scGPT-INFO-train_epoch: | epoch   4 | 2000/2371 batches | ms/batch 523.06 | loss 21.4077
02:01:11-scGPT-INFO-train_epoch: | epoch   4 | 2100/2371 batches | ms/batch 522.82 | loss 20.8848
02:02:03-scGPT-INFO-train_epoch: | epoch   4 | 2200/2371 batches | ms/batch 522.49 | loss 20.1962
02:02:55-scGPT-INFO-train_epoch: | epoch   4 | 2300/2371 batches | ms/batch 521.48 | loss 20.1219
02:46:46-scGPT-INFO-main: | epoch   4 | time: 3830.03s | loss: 462.1010 | nPDS: 0.5278 | DES: 0.1918 | MAE: 7.6478
02:47:37-scGPT-INFO-train_epoch: | epoch   5 | 100/2371 batches | ms/batch 514.69 | loss 20.2047
02:48:30-scGPT-INFO-train_epoch: | epoch   5 | 200/2371 batches | ms/batch 528.22 | loss 18.8210
02:49:23-scGPT-INFO-train_epoch: | epoch   5 | 300/2371 batches | ms/batch 530.78 | loss 20.3041
02:50:16-scGPT-INFO-train_epoch: | epoch   5 | 400/2371 batches | ms/batch 533.43 | loss 19.5845
02:51:09-scGPT-INFO-train_epoch: | epoch   5 | 500/2371 batches | ms/batch 528.95 | loss 19.9822
02:52:02-scGPT-INFO-train_epoch: | epoch   5 | 600/2371 batches | ms/batch 525.33 | loss 20.9402
02:52:54-scGPT-INFO-train_epoch: | epoch   5 | 700/2371 batches | ms/batch 527.39 | loss 20.7066
02:53:47-scGPT-INFO-train_epoch: | epoch   5 | 800/2371 batches | ms/batch 521.74 | loss 18.7133
02:54:39-scGPT-INFO-train_epoch: | epoch   5 | 900/2371 batches | ms/batch 524.21 | loss 18.6946
02:55:31-scGPT-INFO-train_epoch: | epoch   5 | 1000/2371 batches | ms/batch 521.45 | loss 20.8625
02:56:23-scGPT-INFO-train_epoch: | epoch   5 | 1100/2371 batches | ms/batch 522.22 | loss 18.9767
02:57:16-scGPT-INFO-train_epoch: | epoch   5 | 1200/2371 batches | ms/batch 522.04 | loss 22.1274
02:58:08-scGPT-INFO-train_epoch: | epoch   5 | 1300/2371 batches | ms/batch 519.84 | loss 19.6698
02:58:59-scGPT-INFO-train_epoch: | epoch   5 | 1400/2371 batches | ms/batch 517.05 | loss 20.4110
02:59:51-scGPT-INFO-train_epoch: | epoch   5 | 1500/2371 batches | ms/batch 519.40 | loss 20.8520
03:00:43-scGPT-INFO-train_epoch: | epoch   5 | 1600/2371 batches | ms/batch 516.30 | loss 18.7791
03:01:35-scGPT-INFO-train_epoch: | epoch   5 | 1700/2371 batches | ms/batch 519.32 | loss 22.4377
03:02:27-scGPT-INFO-train_epoch: | epoch   5 | 1800/2371 batches | ms/batch 518.70 | loss 18.6841
03:03:18-scGPT-INFO-train_epoch: | epoch   5 | 1900/2371 batches | ms/batch 516.66 | loss 19.6668
03:04:10-scGPT-INFO-train_epoch: | epoch   5 | 2000/2371 batches | ms/batch 517.05 | loss 21.1046
03:05:02-scGPT-INFO-train_epoch: | epoch   5 | 2100/2371 batches | ms/batch 520.92 | loss 21.3971
03:05:54-scGPT-INFO-train_epoch: | epoch   5 | 2200/2371 batches | ms/batch 518.95 | loss 21.6916
03:06:46-scGPT-INFO-train_epoch: | epoch   5 | 2300/2371 batches | ms/batch 516.09 | loss 20.4382
03:56:27-scGPT-INFO-main: | epoch   5 | time: 4181.75s | loss: 459.3259 | nPDS: 0.5278 | DES: 0.1918 | MAE: 7.6221
03:57:18-scGPT-INFO-train_epoch: | epoch   6 | 100/2371 batches | ms/batch 509.08 | loss 20.4798
03:58:10-scGPT-INFO-train_epoch: | epoch   6 | 200/2371 batches | ms/batch 515.03 | loss 20.8058
03:59:02-scGPT-INFO-train_epoch: | epoch   6 | 300/2371 batches | ms/batch 519.49 | loss 21.5749
03:59:53-scGPT-INFO-train_epoch: | epoch   6 | 400/2371 batches | ms/batch 515.40 | loss 18.7232
04:00:45-scGPT-INFO-train_epoch: | epoch   6 | 500/2371 batches | ms/batch 516.32 | loss 21.0321
04:01:36-scGPT-INFO-train_epoch: | epoch   6 | 600/2371 batches | ms/batch 515.29 | loss 20.4799
04:02:28-scGPT-INFO-train_epoch: | epoch   6 | 700/2371 batches | ms/batch 514.65 | loss 21.2627
04:03:19-scGPT-INFO-train_epoch: | epoch   6 | 800/2371 batches | ms/batch 514.56 | loss 21.4474
04:04:11-scGPT-INFO-train_epoch: | epoch   6 | 900/2371 batches | ms/batch 516.03 | loss 20.7704
04:05:03-scGPT-INFO-train_epoch: | epoch   6 | 1000/2371 batches | ms/batch 516.52 | loss 21.2000
04:05:54-scGPT-INFO-train_epoch: | epoch   6 | 1100/2371 batches | ms/batch 515.68 | loss 20.1864
04:06:46-scGPT-INFO-train_epoch: | epoch   6 | 1200/2371 batches | ms/batch 515.05 | loss 21.2737
04:07:37-scGPT-INFO-train_epoch: | epoch   6 | 1300/2371 batches | ms/batch 516.43 | loss 19.9572
04:08:29-scGPT-INFO-train_epoch: | epoch   6 | 1400/2371 batches | ms/batch 515.24 | loss 21.0840
04:09:20-scGPT-INFO-train_epoch: | epoch   6 | 1500/2371 batches | ms/batch 514.37 | loss 19.9214
04:10:12-scGPT-INFO-train_epoch: | epoch   6 | 1600/2371 batches | ms/batch 515.67 | loss 21.9735
04:11:03-scGPT-INFO-train_epoch: | epoch   6 | 1700/2371 batches | ms/batch 516.32 | loss 22.8263
04:11:55-scGPT-INFO-train_epoch: | epoch   6 | 1800/2371 batches | ms/batch 514.19 | loss 20.5295
04:12:47-scGPT-INFO-train_epoch: | epoch   6 | 1900/2371 batches | ms/batch 516.35 | loss 19.9868
04:13:38-scGPT-INFO-train_epoch: | epoch   6 | 2000/2371 batches | ms/batch 515.17 | loss 18.3089
04:14:30-scGPT-INFO-train_epoch: | epoch   6 | 2100/2371 batches | ms/batch 516.54 | loss 20.1884
04:15:21-scGPT-INFO-train_epoch: | epoch   6 | 2200/2371 batches | ms/batch 515.12 | loss 21.0835
04:16:13-scGPT-INFO-train_epoch: | epoch   6 | 2300/2371 batches | ms/batch 515.11 | loss 21.3898
05:04:36-scGPT-INFO-main: | epoch   6 | time: 4089.00s | loss: 451.4523 | nPDS: 0.5278 | DES: 0.1930 | MAE: 8.5266
05:05:28-scGPT-INFO-train_epoch: | epoch   7 | 100/2371 batches | ms/batch 511.71 | loss 21.2495
05:06:19-scGPT-INFO-train_epoch: | epoch   7 | 200/2371 batches | ms/batch 516.90 | loss 20.8376
05:07:11-scGPT-INFO-train_epoch: | epoch   7 | 300/2371 batches | ms/batch 517.76 | loss 19.3391
05:08:03-scGPT-INFO-train_epoch: | epoch   7 | 400/2371 batches | ms/batch 517.69 | loss 19.8219
05:08:55-scGPT-INFO-train_epoch: | epoch   7 | 500/2371 batches | ms/batch 517.80 | loss 20.9393
05:09:46-scGPT-INFO-train_epoch: | epoch   7 | 600/2371 batches | ms/batch 518.09 | loss 19.4379
05:10:38-scGPT-INFO-train_epoch: | epoch   7 | 700/2371 batches | ms/batch 517.45 | loss 19.2166
05:11:30-scGPT-INFO-train_epoch: | epoch   7 | 800/2371 batches | ms/batch 517.23 | loss 19.8802
05:12:22-scGPT-INFO-train_epoch: | epoch   7 | 900/2371 batches | ms/batch 517.57 | loss 18.4947
05:13:13-scGPT-INFO-train_epoch: | epoch   7 | 1000/2371 batches | ms/batch 517.36 | loss 21.4457
05:14:05-scGPT-INFO-train_epoch: | epoch   7 | 1100/2371 batches | ms/batch 516.77 | loss 19.7443
05:14:57-scGPT-INFO-train_epoch: | epoch   7 | 1200/2371 batches | ms/batch 515.17 | loss 22.3380
05:15:48-scGPT-INFO-train_epoch: | epoch   7 | 1300/2371 batches | ms/batch 514.96 | loss 17.6388
05:16:40-scGPT-INFO-train_epoch: | epoch   7 | 1400/2371 batches | ms/batch 515.74 | loss 19.5980
05:17:31-scGPT-INFO-train_epoch: | epoch   7 | 1500/2371 batches | ms/batch 515.02 | loss 19.8225
05:18:23-scGPT-INFO-train_epoch: | epoch   7 | 1600/2371 batches | ms/batch 515.79 | loss 19.7140
05:19:14-scGPT-INFO-train_epoch: | epoch   7 | 1700/2371 batches | ms/batch 516.07 | loss 18.1570
05:20:06-scGPT-INFO-train_epoch: | epoch   7 | 1800/2371 batches | ms/batch 514.93 | loss 19.9499
05:20:57-scGPT-INFO-train_epoch: | epoch   7 | 1900/2371 batches | ms/batch 515.84 | loss 21.1690
05:21:49-scGPT-INFO-train_epoch: | epoch   7 | 2000/2371 batches | ms/batch 515.64 | loss 17.7338
05:22:40-scGPT-INFO-train_epoch: | epoch   7 | 2100/2371 batches | ms/batch 514.77 | loss 20.4760
05:23:32-scGPT-INFO-train_epoch: | epoch   7 | 2200/2371 batches | ms/batch 516.99 | loss 17.3088
05:24:24-scGPT-INFO-train_epoch: | epoch   7 | 2300/2371 batches | ms/batch 515.27 | loss 19.6075
06:15:31-scGPT-INFO-main: | epoch   7 | time: 4254.71s | loss: 456.8343 | nPDS: 0.5278 | DES: 0.1916 | MAE: 7.4355
06:16:22-scGPT-INFO-train_epoch: | epoch   8 | 100/2371 batches | ms/batch 512.36 | loss 19.4209
06:17:14-scGPT-INFO-train_epoch: | epoch   8 | 200/2371 batches | ms/batch 516.88 | loss 20.0470
06:18:06-scGPT-INFO-train_epoch: | epoch   8 | 300/2371 batches | ms/batch 516.31 | loss 20.0015
06:18:58-scGPT-INFO-train_epoch: | epoch   8 | 400/2371 batches | ms/batch 519.48 | loss 19.4270
06:19:49-scGPT-INFO-train_epoch: | epoch   8 | 500/2371 batches | ms/batch 518.78 | loss 20.5108
06:20:41-scGPT-INFO-train_epoch: | epoch   8 | 600/2371 batches | ms/batch 518.14 | loss 20.0183
06:21:33-scGPT-INFO-train_epoch: | epoch   8 | 700/2371 batches | ms/batch 516.71 | loss 20.6528
06:22:25-scGPT-INFO-train_epoch: | epoch   8 | 800/2371 batches | ms/batch 515.54 | loss 22.3195
06:23:16-scGPT-INFO-train_epoch: | epoch   8 | 900/2371 batches | ms/batch 514.17 | loss 20.6590
06:24:08-scGPT-INFO-train_epoch: | epoch   8 | 1000/2371 batches | ms/batch 517.65 | loss 20.4178
06:24:59-scGPT-INFO-train_epoch: | epoch   8 | 1100/2371 batches | ms/batch 517.01 | loss 20.3531
06:25:51-scGPT-INFO-train_epoch: | epoch   8 | 1200/2371 batches | ms/batch 516.61 | loss 19.9424
06:26:43-scGPT-INFO-train_epoch: | epoch   8 | 1300/2371 batches | ms/batch 517.23 | loss 20.3271
06:27:34-scGPT-INFO-train_epoch: | epoch   8 | 1400/2371 batches | ms/batch 514.96 | loss 19.5837
06:28:26-scGPT-INFO-train_epoch: | epoch   8 | 1500/2371 batches | ms/batch 514.67 | loss 22.8351
06:29:17-scGPT-INFO-train_epoch: | epoch   8 | 1600/2371 batches | ms/batch 517.07 | loss 20.1199
06:30:09-scGPT-INFO-train_epoch: | epoch   8 | 1700/2371 batches | ms/batch 517.06 | loss 18.4837
06:31:01-scGPT-INFO-train_epoch: | epoch   8 | 1800/2371 batches | ms/batch 516.20 | loss 19.1730
06:31:52-scGPT-INFO-train_epoch: | epoch   8 | 1900/2371 batches | ms/batch 514.70 | loss 19.5380
06:32:44-scGPT-INFO-train_epoch: | epoch   8 | 2000/2371 batches | ms/batch 516.54 | loss 20.4176
06:33:36-scGPT-INFO-train_epoch: | epoch   8 | 2100/2371 batches | ms/batch 517.66 | loss 19.3248
06:34:27-scGPT-INFO-train_epoch: | epoch   8 | 2200/2371 batches | ms/batch 516.50 | loss 19.4517
06:35:19-scGPT-INFO-train_epoch: | epoch   8 | 2300/2371 batches | ms/batch 516.64 | loss 20.9247
07:27:05-scGPT-INFO-main: | epoch   8 | time: 4293.85s | loss: 448.4579 | nPDS: 0.5278 | DES: 0.1921 | MAE: 8.0387
07:27:59-scGPT-INFO-train_epoch: | epoch   9 | 100/2371 batches | ms/batch 536.67 | loss 20.8707
07:28:53-scGPT-INFO-train_epoch: | epoch   9 | 200/2371 batches | ms/batch 539.02 | loss 20.0645
07:29:47-scGPT-INFO-train_epoch: | epoch   9 | 300/2371 batches | ms/batch 543.23 | loss 20.1245
07:30:40-scGPT-INFO-train_epoch: | epoch   9 | 400/2371 batches | ms/batch 535.65 | loss 17.6738
07:31:34-scGPT-INFO-train_epoch: | epoch   9 | 500/2371 batches | ms/batch 538.55 | loss 19.9711
07:32:28-scGPT-INFO-train_epoch: | epoch   9 | 600/2371 batches | ms/batch 539.34 | loss 19.8842
07:33:22-scGPT-INFO-train_epoch: | epoch   9 | 700/2371 batches | ms/batch 537.80 | loss 19.4671
07:34:15-scGPT-INFO-train_epoch: | epoch   9 | 800/2371 batches | ms/batch 534.73 | loss 18.7380
07:35:09-scGPT-INFO-train_epoch: | epoch   9 | 900/2371 batches | ms/batch 538.50 | loss 18.8544
07:36:03-scGPT-INFO-train_epoch: | epoch   9 | 1000/2371 batches | ms/batch 534.54 | loss 21.8124
07:36:56-scGPT-INFO-train_epoch: | epoch   9 | 1100/2371 batches | ms/batch 535.34 | loss 20.1291
07:37:49-scGPT-INFO-train_epoch: | epoch   9 | 1200/2371 batches | ms/batch 530.82 | loss 18.6015
07:38:43-scGPT-INFO-train_epoch: | epoch   9 | 1300/2371 batches | ms/batch 538.72 | loss 20.2249
07:39:37-scGPT-INFO-train_epoch: | epoch   9 | 1400/2371 batches | ms/batch 534.58 | loss 21.5878
07:40:30-scGPT-INFO-train_epoch: | epoch   9 | 1500/2371 batches | ms/batch 533.96 | loss 20.7783
07:41:24-scGPT-INFO-train_epoch: | epoch   9 | 1600/2371 batches | ms/batch 537.28 | loss 19.5582
07:42:17-scGPT-INFO-train_epoch: | epoch   9 | 1700/2371 batches | ms/batch 536.25 | loss 19.7021
07:43:11-scGPT-INFO-train_epoch: | epoch   9 | 1800/2371 batches | ms/batch 537.07 | loss 18.8856
07:44:05-scGPT-INFO-train_epoch: | epoch   9 | 1900/2371 batches | ms/batch 533.69 | loss 20.9634
07:44:58-scGPT-INFO-train_epoch: | epoch   9 | 2000/2371 batches | ms/batch 532.80 | loss 20.4746
07:45:51-scGPT-INFO-train_epoch: | epoch   9 | 2100/2371 batches | ms/batch 534.38 | loss 19.0960
07:46:45-scGPT-INFO-train_epoch: | epoch   9 | 2200/2371 batches | ms/batch 536.06 | loss 21.3821
07:47:38-scGPT-INFO-train_epoch: | epoch   9 | 2300/2371 batches | ms/batch 531.84 | loss 19.8996
08:37:31-scGPT-INFO-main: | epoch   9 | time: 4225.77s | loss: 449.6163 | nPDS: 0.5278 | DES: 0.1929 | MAE: 8.4403
08:38:25-scGPT-INFO-train_epoch: | epoch  10 | 100/2371 batches | ms/batch 540.15 | loss 21.1893
08:39:19-scGPT-INFO-train_epoch: | epoch  10 | 200/2371 batches | ms/batch 540.70 | loss 19.3262
08:40:13-scGPT-INFO-train_epoch: | epoch  10 | 300/2371 batches | ms/batch 537.58 | loss 21.9180
08:41:07-scGPT-INFO-train_epoch: | epoch  10 | 400/2371 batches | ms/batch 542.01 | loss 19.2519
08:42:01-scGPT-INFO-train_epoch: | epoch  10 | 500/2371 batches | ms/batch 540.85 | loss 19.6687
08:42:54-scGPT-INFO-train_epoch: | epoch  10 | 600/2371 batches | ms/batch 533.07 | loss 20.9610
08:43:48-scGPT-INFO-train_epoch: | epoch  10 | 700/2371 batches | ms/batch 542.30 | loss 18.0357
08:44:42-scGPT-INFO-train_epoch: | epoch  10 | 800/2371 batches | ms/batch 534.96 | loss 19.2686
08:45:36-scGPT-INFO-train_epoch: | epoch  10 | 900/2371 batches | ms/batch 538.84 | loss 19.4955
08:46:29-scGPT-INFO-train_epoch: | epoch  10 | 1000/2371 batches | ms/batch 536.55 | loss 18.8271
08:47:23-scGPT-INFO-train_epoch: | epoch  10 | 1100/2371 batches | ms/batch 537.91 | loss 21.0358
08:48:17-scGPT-INFO-train_epoch: | epoch  10 | 1200/2371 batches | ms/batch 533.75 | loss 21.3854
08:49:10-scGPT-INFO-train_epoch: | epoch  10 | 1300/2371 batches | ms/batch 534.49 | loss 19.9318
08:50:04-scGPT-INFO-train_epoch: | epoch  10 | 1400/2371 batches | ms/batch 539.27 | loss 20.4550
08:50:58-scGPT-INFO-train_epoch: | epoch  10 | 1500/2371 batches | ms/batch 535.96 | loss 20.0655
08:51:51-scGPT-INFO-train_epoch: | epoch  10 | 1600/2371 batches | ms/batch 536.72 | loss 20.5856
08:52:45-scGPT-INFO-train_epoch: | epoch  10 | 1700/2371 batches | ms/batch 539.27 | loss 20.7651
08:53:39-scGPT-INFO-train_epoch: | epoch  10 | 1800/2371 batches | ms/batch 536.95 | loss 21.8485
08:54:33-scGPT-INFO-train_epoch: | epoch  10 | 1900/2371 batches | ms/batch 536.68 | loss 20.6927
08:55:26-scGPT-INFO-train_epoch: | epoch  10 | 2000/2371 batches | ms/batch 535.55 | loss 19.6585
08:56:20-scGPT-INFO-train_epoch: | epoch  10 | 2100/2371 batches | ms/batch 537.59 | loss 20.6606
08:57:14-scGPT-INFO-train_epoch: | epoch  10 | 2200/2371 batches | ms/batch 540.75 | loss 20.2748
08:58:08-scGPT-INFO-train_epoch: | epoch  10 | 2300/2371 batches | ms/batch 543.61 | loss 20.3774
09:50:57-scGPT-INFO-main: | epoch  10 | time: 4405.90s | loss: 450.4604 | nPDS: 0.5278 | DES: 0.1919 | MAE: 7.6256
09:51:50-scGPT-INFO-train_epoch: | epoch  11 | 100/2371 batches | ms/batch 535.28 | loss 18.8776
09:52:44-scGPT-INFO-train_epoch: | epoch  11 | 200/2371 batches | ms/batch 538.96 | loss 17.9211
09:53:38-scGPT-INFO-train_epoch: | epoch  11 | 300/2371 batches | ms/batch 541.31 | loss 21.4035
09:54:32-scGPT-INFO-train_epoch: | epoch  11 | 400/2371 batches | ms/batch 536.84 | loss 18.4564
09:55:26-scGPT-INFO-train_epoch: | epoch  11 | 500/2371 batches | ms/batch 536.10 | loss 19.9764
09:56:19-scGPT-INFO-train_epoch: | epoch  11 | 600/2371 batches | ms/batch 538.87 | loss 20.1214
09:57:13-scGPT-INFO-train_epoch: | epoch  11 | 700/2371 batches | ms/batch 536.10 | loss 20.7066
09:58:06-scGPT-INFO-train_epoch: | epoch  11 | 800/2371 batches | ms/batch 534.25 | loss 19.1886
09:59:00-scGPT-INFO-train_epoch: | epoch  11 | 900/2371 batches | ms/batch 535.21 | loss 17.9583
09:59:53-scGPT-INFO-train_epoch: | epoch  11 | 1000/2371 batches | ms/batch 534.83 | loss 18.8609
10:00:47-scGPT-INFO-train_epoch: | epoch  11 | 1100/2371 batches | ms/batch 539.27 | loss 21.1651
10:01:41-scGPT-INFO-train_epoch: | epoch  11 | 1200/2371 batches | ms/batch 537.91 | loss 21.2723
10:02:35-scGPT-INFO-train_epoch: | epoch  11 | 1300/2371 batches | ms/batch 535.88 | loss 20.7695
10:03:28-scGPT-INFO-train_epoch: | epoch  11 | 1400/2371 batches | ms/batch 534.67 | loss 19.0375
10:04:22-scGPT-INFO-train_epoch: | epoch  11 | 1500/2371 batches | ms/batch 537.01 | loss 22.8623
10:05:16-scGPT-INFO-train_epoch: | epoch  11 | 1600/2371 batches | ms/batch 537.01 | loss 22.2025
10:06:09-scGPT-INFO-train_epoch: | epoch  11 | 1700/2371 batches | ms/batch 536.27 | loss 18.8014
10:07:03-scGPT-INFO-train_epoch: | epoch  11 | 1800/2371 batches | ms/batch 533.29 | loss 21.5488
10:07:56-scGPT-INFO-train_epoch: | epoch  11 | 1900/2371 batches | ms/batch 533.69 | loss 17.6743
10:08:49-scGPT-INFO-train_epoch: | epoch  11 | 2000/2371 batches | ms/batch 532.70 | loss 19.7828
10:09:42-scGPT-INFO-train_epoch: | epoch  11 | 2100/2371 batches | ms/batch 531.74 | loss 20.6797
10:10:36-scGPT-INFO-train_epoch: | epoch  11 | 2200/2371 batches | ms/batch 534.66 | loss 20.3488
10:11:29-scGPT-INFO-train_epoch: | epoch  11 | 2300/2371 batches | ms/batch 534.84 | loss 19.4074
11:05:03-scGPT-INFO-main: | epoch  11 | time: 4446.11s | loss: 445.7731 | nPDS: 0.5278 | DES: 0.1923 | MAE: 8.2901
11:05:56-scGPT-INFO-train_epoch: | epoch  12 | 100/2371 batches | ms/batch 533.70 | loss 18.4117
11:06:50-scGPT-INFO-train_epoch: | epoch  12 | 200/2371 batches | ms/batch 536.19 | loss 20.5446
11:07:43-scGPT-INFO-train_epoch: | epoch  12 | 300/2371 batches | ms/batch 534.02 | loss 18.8386
11:08:37-scGPT-INFO-train_epoch: | epoch  12 | 400/2371 batches | ms/batch 534.87 | loss 20.2370
11:09:30-scGPT-INFO-train_epoch: | epoch  12 | 500/2371 batches | ms/batch 535.03 | loss 17.7185
11:10:24-scGPT-INFO-train_epoch: | epoch  12 | 600/2371 batches | ms/batch 534.44 | loss 21.7907
11:11:17-scGPT-INFO-train_epoch: | epoch  12 | 700/2371 batches | ms/batch 537.24 | loss 21.1792
11:12:11-scGPT-INFO-train_epoch: | epoch  12 | 800/2371 batches | ms/batch 537.40 | loss 21.6323
11:13:05-scGPT-INFO-train_epoch: | epoch  12 | 900/2371 batches | ms/batch 541.07 | loss 20.4848
11:13:59-scGPT-INFO-train_epoch: | epoch  12 | 1000/2371 batches | ms/batch 535.37 | loss 20.9214
11:14:52-scGPT-INFO-train_epoch: | epoch  12 | 1100/2371 batches | ms/batch 535.86 | loss 20.2242
11:15:45-scGPT-INFO-train_epoch: | epoch  12 | 1200/2371 batches | ms/batch 531.20 | loss 20.4309
11:16:39-scGPT-INFO-train_epoch: | epoch  12 | 1300/2371 batches | ms/batch 532.61 | loss 20.7589
11:17:32-scGPT-INFO-train_epoch: | epoch  12 | 1400/2371 batches | ms/batch 537.48 | loss 20.6844
11:18:26-scGPT-INFO-train_epoch: | epoch  12 | 1500/2371 batches | ms/batch 533.48 | loss 20.1269
11:19:19-scGPT-INFO-train_epoch: | epoch  12 | 1600/2371 batches | ms/batch 536.91 | loss 18.9688
11:20:13-scGPT-INFO-train_epoch: | epoch  12 | 1700/2371 batches | ms/batch 535.56 | loss 18.9785
11:21:06-scGPT-INFO-train_epoch: | epoch  12 | 1800/2371 batches | ms/batch 534.05 | loss 20.6166
11:22:00-scGPT-INFO-train_epoch: | epoch  12 | 1900/2371 batches | ms/batch 532.71 | loss 20.8086
11:22:54-scGPT-INFO-train_epoch: | epoch  12 | 2000/2371 batches | ms/batch 538.04 | loss 19.9194
11:23:47-scGPT-INFO-train_epoch: | epoch  12 | 2100/2371 batches | ms/batch 538.41 | loss 19.4597
11:24:41-scGPT-INFO-train_epoch: | epoch  12 | 2200/2371 batches | ms/batch 533.29 | loss 20.3081
11:25:34-scGPT-INFO-train_epoch: | epoch  12 | 2300/2371 batches | ms/batch 534.95 | loss 23.3606
12:18:33-scGPT-INFO-main: | epoch  12 | time: 4409.92s | loss: 449.8769 | nPDS: 0.5278 | DES: 0.1924 | MAE: 8.3422
12:19:26-scGPT-INFO-train_epoch: | epoch  13 | 100/2371 batches | ms/batch 531.29 | loss 17.7812
12:20:19-scGPT-INFO-train_epoch: | epoch  13 | 200/2371 batches | ms/batch 534.41 | loss 19.7981
12:21:13-scGPT-INFO-train_epoch: | epoch  13 | 300/2371 batches | ms/batch 535.68 | loss 20.8330
12:22:06-scGPT-INFO-train_epoch: | epoch  13 | 400/2371 batches | ms/batch 535.26 | loss 18.9666
12:23:00-scGPT-INFO-train_epoch: | epoch  13 | 500/2371 batches | ms/batch 536.68 | loss 20.0763
12:23:54-scGPT-INFO-train_epoch: | epoch  13 | 600/2371 batches | ms/batch 539.34 | loss 21.4839
12:24:48-scGPT-INFO-train_epoch: | epoch  13 | 700/2371 batches | ms/batch 535.62 | loss 21.8842
12:25:41-scGPT-INFO-train_epoch: | epoch  13 | 800/2371 batches | ms/batch 534.76 | loss 19.6331
12:26:35-scGPT-INFO-train_epoch: | epoch  13 | 900/2371 batches | ms/batch 538.38 | loss 21.3151
12:27:28-scGPT-INFO-train_epoch: | epoch  13 | 1000/2371 batches | ms/batch 534.48 | loss 22.2925
12:28:22-scGPT-INFO-train_epoch: | epoch  13 | 1100/2371 batches | ms/batch 532.10 | loss 20.1451
12:29:15-scGPT-INFO-train_epoch: | epoch  13 | 1200/2371 batches | ms/batch 533.18 | loss 20.2156
12:30:08-scGPT-INFO-train_epoch: | epoch  13 | 1300/2371 batches | ms/batch 534.48 | loss 20.4216
12:31:02-scGPT-INFO-train_epoch: | epoch  13 | 1400/2371 batches | ms/batch 535.14 | loss 20.8281
12:31:55-scGPT-INFO-train_epoch: | epoch  13 | 1500/2371 batches | ms/batch 534.50 | loss 19.9556
12:32:49-scGPT-INFO-train_epoch: | epoch  13 | 1600/2371 batches | ms/batch 536.84 | loss 18.5523
12:33:42-scGPT-INFO-train_epoch: | epoch  13 | 1700/2371 batches | ms/batch 535.20 | loss 19.6425
12:34:36-scGPT-INFO-train_epoch: | epoch  13 | 1800/2371 batches | ms/batch 534.25 | loss 20.0907
12:35:29-scGPT-INFO-train_epoch: | epoch  13 | 1900/2371 batches | ms/batch 534.09 | loss 20.4069
12:36:23-scGPT-INFO-train_epoch: | epoch  13 | 2000/2371 batches | ms/batch 535.49 | loss 19.5236
12:37:16-scGPT-INFO-train_epoch: | epoch  13 | 2100/2371 batches | ms/batch 532.51 | loss 21.6602
12:38:10-scGPT-INFO-train_epoch: | epoch  13 | 2200/2371 batches | ms/batch 535.69 | loss 20.8505
12:39:03-scGPT-INFO-train_epoch: | epoch  13 | 2300/2371 batches | ms/batch 535.71 | loss 19.8376
13:32:52-scGPT-INFO-main: | epoch  13 | time: 4459.23s | loss: 452.9308 | nPDS: 0.5278 | DES: 0.1918 | MAE: 7.6167
13:32:52-scGPT-INFO-main: Early stopping at epoch 13
13:32:52-scGPT-INFO-main: 
============================================================
Training complete! Best: {'pds': 0.5277777777777778, 'des': 0.1901616576128418, 'mae_top2k': 6.560503800710042, 'combined': 2.632706640291659}
============================================================
