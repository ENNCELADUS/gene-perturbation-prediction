20:48:38-scGPT-INFO-setup_logging: Running on 2025-12-30 20:48:38
20:48:38-scGPT-INFO-main: Config: {'paths': {'model_dir': 'model/scGPT', 'finetuned_model_dir': 'model/scGPT_finetuned', 'data_dir': 'data/processed', 'test_file': 'test.h5ad', 'train_file': 'train.h5ad', 'output_dir': 'results', 'gears_data_dir': 'data/processed/gears', 'dataset_name': 'vcc'}, 'model': {'pad_token': '<pad>', 'pad_value': 0, 'max_seq_len': 1536, 'include_zero_gene': 'batch-wise', 'embsize': 512, 'd_hid': 512, 'nlayers': 12, 'nhead': 8, 'n_layers_cls': 3, 'dropout': 0.0, 'use_fast_transformer': True}, 'inference': {'batch_size': 16, 'eval_batch_size': 16, 'seed': 42, 'control_target_gene': 'non-targeting'}, 'data': {'special_tokens': ['<pad>', '<cls>', '<eoc>'], 'pert_pad_id': 0}, 'training': {'MLM': True, 'CLS': False, 'CCE': False, 'MVC': False, 'ECS': False, 'amp': True, 'freeze_encoder': True, 'freeze_prefixes': ['encoder', 'value_encoder', 'transformer_encoder']}, 'optimizer': {'lr': 0.0001, 'batch_size': 64, 'eval_batch_size': 64, 'epochs': 15, 'schedule_interval': 1, 'schedule_gamma': 0.9, 'early_stop': 10, 'grad_clip': 1.0}, 'loss': {'sw1_weight': 0.6, 'proto_weight': 0.25, 'de_rank_weight': 0.1, 'dir_weight': 0.05, 'sw1_projections': 32, 'proto_tau': 0.1, 'de_rank_tau': 0.2, 'dir_tau': 0.2, 'de_rank_sample_de': 256, 'de_rank_sample_non_de': 256}, 'load_param_prefixes': ['encoder', 'value_encoder', 'transformer_encoder'], 'split': {'test_genes_file': 'data/raw/test_set.csv', 'train_ratio': 0.834, 'val_ratio': 0.166, 'seed': 42}, 'metrics': {'mae_top_k': 2000, 'des_top_k': None, 'de_fdr': 0.05}, 'early_stopping': {'metric': 'overall_score'}, 'logging': {'log_interval': 100, 'save_interval': 1}, 'hardware': {'device': 'cuda', 'ddp_timeout_minutes': 240}}
20:48:38-scGPT-INFO-main: Loading data...
20:50:38-scGPT-INFO-main: Dataset: 18080 genes, 17840 in vocab
20:50:38-scGPT-WARNING-main: DE gene map is empty; DE loss terms will be skipped
20:50:38-scGPT-INFO-main: Test: 30, Train: 100, Val: 20 perts
20:50:39-scGPT-INFO-main: Train cells: 149609, Val cells: 28732
20:50:39-scGPT-INFO-load_pretrained_model: Loading pretrained model from model/scGPT/best_model.pt
20:50:39-scGPT-INFO-load_pretrained_model: Model config: embsize=512, nlayers=12, nheads=8
20:50:40-scGPT-INFO-load_pretrained: Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.in_proj_weight with shape torch.Size([1536, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.in_proj_bias with shape torch.Size([1536])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
20:50:40-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
20:50:40-scGPT-INFO-freeze_encoder_layers: Froze 50,278,400 params, training 1,581,059 params
20:50:40-scGPT-INFO-freeze_encoder_layers: Trainable: 3.0% of model
20:50:40-scGPT-INFO-freeze_encoder_layers: Trainable components: {'pert_encoder', 'cls_decoder', 'decoder'}
20:50:47-scGPT-INFO-main: 
============================================================
Starting training...
============================================================
20:51:27-scGPT-INFO-train_epoch: | epoch   1 | 100/2375 batches | ms/batch 402.89 | loss 0.7949
20:51:27-scGPT-INFO-train_epoch:   dist 0.0884 | proto 2.9674 | de_rank 0.0000 | dir 0.0000
20:52:06-scGPT-INFO-train_epoch: | epoch   1 | 200/2375 batches | ms/batch 394.41 | loss 0.7175
20:52:06-scGPT-INFO-train_epoch:   dist 0.0629 | proto 2.7192 | de_rank 0.0000 | dir 0.0000
20:52:46-scGPT-INFO-train_epoch: | epoch   1 | 300/2375 batches | ms/batch 394.65 | loss 0.7257
20:52:46-scGPT-INFO-train_epoch:   dist 0.0636 | proto 2.7502 | de_rank 0.0000 | dir 0.0000
20:53:25-scGPT-INFO-train_epoch: | epoch   1 | 400/2375 batches | ms/batch 394.84 | loss 0.7184
20:53:25-scGPT-INFO-train_epoch:   dist 0.0664 | proto 2.7143 | de_rank 0.0000 | dir 0.0000
20:54:05-scGPT-INFO-train_epoch: | epoch   1 | 500/2375 batches | ms/batch 395.18 | loss 0.7175
20:54:05-scGPT-INFO-train_epoch:   dist 0.0626 | proto 2.7197 | de_rank 0.0000 | dir 0.0000
20:54:44-scGPT-INFO-train_epoch: | epoch   1 | 600/2375 batches | ms/batch 395.33 | loss 0.7182
20:54:44-scGPT-INFO-train_epoch:   dist 0.0642 | proto 2.7187 | de_rank 0.0000 | dir 0.0000
20:55:24-scGPT-INFO-train_epoch: | epoch   1 | 700/2375 batches | ms/batch 394.58 | loss 0.7262
20:55:24-scGPT-INFO-train_epoch:   dist 0.0691 | proto 2.7387 | de_rank 0.0000 | dir 0.0000
20:56:03-scGPT-INFO-train_epoch: | epoch   1 | 800/2375 batches | ms/batch 395.91 | loss 0.7179
20:56:03-scGPT-INFO-train_epoch:   dist 0.0648 | proto 2.7158 | de_rank 0.0000 | dir 0.0000
20:56:43-scGPT-INFO-train_epoch: | epoch   1 | 900/2375 batches | ms/batch 394.58 | loss 0.7122
20:56:43-scGPT-INFO-train_epoch:   dist 0.0619 | proto 2.7004 | de_rank 0.0000 | dir 0.0000
20:57:22-scGPT-INFO-train_epoch: | epoch   1 | 1000/2375 batches | ms/batch 394.91 | loss 0.7105
20:57:22-scGPT-INFO-train_epoch:   dist 0.0615 | proto 2.6942 | de_rank 0.0000 | dir 0.0000
20:58:02-scGPT-INFO-train_epoch: | epoch   1 | 1100/2375 batches | ms/batch 395.23 | loss 0.7151
20:58:02-scGPT-INFO-train_epoch:   dist 0.0635 | proto 2.7080 | de_rank 0.0000 | dir 0.0000
20:58:41-scGPT-INFO-train_epoch: | epoch   1 | 1200/2375 batches | ms/batch 395.32 | loss 0.7172
20:58:41-scGPT-INFO-train_epoch:   dist 0.0713 | proto 2.6979 | de_rank 0.0000 | dir 0.0000
20:59:21-scGPT-INFO-train_epoch: | epoch   1 | 1300/2375 batches | ms/batch 395.27 | loss 0.7159
20:59:21-scGPT-INFO-train_epoch:   dist 0.0689 | proto 2.6984 | de_rank 0.0000 | dir 0.0000
21:00:00-scGPT-INFO-train_epoch: | epoch   1 | 1400/2375 batches | ms/batch 395.47 | loss 0.7184
21:00:00-scGPT-INFO-train_epoch:   dist 0.0633 | proto 2.7215 | de_rank 0.0000 | dir 0.0000
21:00:40-scGPT-INFO-train_epoch: | epoch   1 | 1500/2375 batches | ms/batch 395.03 | loss 0.7026
21:00:40-scGPT-INFO-train_epoch:   dist 0.0600 | proto 2.6663 | de_rank 0.0000 | dir 0.0000
21:01:19-scGPT-INFO-train_epoch: | epoch   1 | 1600/2375 batches | ms/batch 394.58 | loss 0.7219
21:01:19-scGPT-INFO-train_epoch:   dist 0.0662 | proto 2.7288 | de_rank 0.0000 | dir 0.0000
21:01:59-scGPT-INFO-train_epoch: | epoch   1 | 1700/2375 batches | ms/batch 396.03 | loss 0.7060
21:01:59-scGPT-INFO-train_epoch:   dist 0.0575 | proto 2.6860 | de_rank 0.0000 | dir 0.0000
21:02:39-scGPT-INFO-train_epoch: | epoch   1 | 1800/2375 batches | ms/batch 395.47 | loss 0.7107
21:02:39-scGPT-INFO-train_epoch:   dist 0.0606 | proto 2.6974 | de_rank 0.0000 | dir 0.0000
21:03:18-scGPT-INFO-train_epoch: | epoch   1 | 1900/2375 batches | ms/batch 394.71 | loss 0.7062
21:03:18-scGPT-INFO-train_epoch:   dist 0.0508 | proto 2.7031 | de_rank 0.0000 | dir 0.0000
21:03:58-scGPT-INFO-train_epoch: | epoch   1 | 2000/2375 batches | ms/batch 395.28 | loss 0.7124
21:03:58-scGPT-INFO-train_epoch:   dist 0.0580 | proto 2.7104 | de_rank 0.0000 | dir 0.0000
21:04:37-scGPT-INFO-train_epoch: | epoch   1 | 2100/2375 batches | ms/batch 394.88 | loss 0.7168
21:04:37-scGPT-INFO-train_epoch:   dist 0.0607 | proto 2.7212 | de_rank 0.0000 | dir 0.0000
21:05:17-scGPT-INFO-train_epoch: | epoch   1 | 2200/2375 batches | ms/batch 394.71 | loss 0.7032
21:05:17-scGPT-INFO-train_epoch:   dist 0.0533 | proto 2.6850 | de_rank 0.0000 | dir 0.0000
21:05:56-scGPT-INFO-train_epoch: | epoch   1 | 2300/2375 batches | ms/batch 395.23 | loss 0.7012
21:05:56-scGPT-INFO-train_epoch:   dist 0.0589 | proto 2.6634 | de_rank 0.0000 | dir 0.0000
21:13:01-scGPT-INFO-setup_logging: Running on 2025-12-30 21:13:01
21:13:01-scGPT-INFO-main: Config: {'paths': {'model_dir': 'model/scGPT', 'finetuned_model_dir': 'model/scGPT_finetuned', 'data_dir': 'data/processed', 'test_file': 'test.h5ad', 'train_file': 'train.h5ad', 'output_dir': 'results', 'gears_data_dir': 'data/processed/gears', 'dataset_name': 'vcc'}, 'model': {'pad_token': '<pad>', 'pad_value': 0, 'max_seq_len': 1536, 'include_zero_gene': 'batch-wise', 'embsize': 512, 'd_hid': 512, 'nlayers': 12, 'nhead': 8, 'n_layers_cls': 3, 'dropout': 0.0, 'use_fast_transformer': True}, 'inference': {'batch_size': 16, 'eval_batch_size': 16, 'seed': 42, 'control_target_gene': 'non-targeting'}, 'data': {'special_tokens': ['<pad>', '<cls>', '<eoc>'], 'pert_pad_id': 0}, 'training': {'MLM': True, 'CLS': False, 'CCE': False, 'MVC': False, 'ECS': False, 'amp': True, 'freeze_encoder': True, 'freeze_prefixes': ['encoder', 'value_encoder', 'transformer_encoder']}, 'optimizer': {'lr': 0.0001, 'batch_size': 64, 'eval_batch_size': 64, 'epochs': 15, 'schedule_interval': 1, 'schedule_gamma': 0.9, 'early_stop': 10, 'grad_clip': 1.0}, 'loss': {'sw1_weight': 0.6, 'proto_weight': 0.25, 'de_rank_weight': 0.1, 'dir_weight': 0.05, 'sw1_projections': 32, 'proto_tau': 0.1, 'de_rank_tau': 0.2, 'dir_tau': 0.2, 'de_rank_sample_de': 256, 'de_rank_sample_non_de': 256}, 'load_param_prefixes': ['encoder', 'value_encoder', 'transformer_encoder'], 'split': {'test_genes_file': 'data/raw/test_set.csv', 'train_ratio': 0.834, 'val_ratio': 0.166, 'seed': 42}, 'metrics': {'mae_top_k': 2000, 'des_top_k': None, 'de_fdr': 0.05}, 'early_stopping': {'metric': 'overall_score'}, 'logging': {'log_interval': 100, 'save_interval': 1}, 'hardware': {'device': 'cuda', 'ddp_timeout_minutes': 240}}
21:13:01-scGPT-INFO-main: Loading data...
21:17:29-scGPT-INFO-setup_logging: Running on 2025-12-30 21:17:29
21:17:29-scGPT-INFO-main: Config: {'paths': {'model_dir': 'model/scGPT', 'finetuned_model_dir': 'model/scGPT_finetuned', 'data_dir': 'data/processed', 'test_file': 'test.h5ad', 'train_file': 'train.h5ad', 'output_dir': 'results', 'gears_data_dir': 'data/processed/gears', 'dataset_name': 'vcc'}, 'model': {'pad_token': '<pad>', 'pad_value': 0, 'max_seq_len': 1536, 'include_zero_gene': 'batch-wise', 'embsize': 512, 'd_hid': 512, 'nlayers': 12, 'nhead': 8, 'n_layers_cls': 3, 'dropout': 0.0, 'use_fast_transformer': True}, 'inference': {'batch_size': 16, 'eval_batch_size': 16, 'seed': 42, 'control_target_gene': 'non-targeting'}, 'data': {'special_tokens': ['<pad>', '<cls>', '<eoc>'], 'pert_pad_id': 0}, 'training': {'MLM': True, 'CLS': False, 'CCE': False, 'MVC': False, 'ECS': False, 'amp': True, 'freeze_encoder': True, 'freeze_prefixes': ['encoder', 'value_encoder', 'transformer_encoder']}, 'optimizer': {'lr': 0.0001, 'batch_size': 64, 'eval_batch_size': 64, 'epochs': 15, 'schedule_interval': 1, 'schedule_gamma': 0.9, 'early_stop': 10, 'grad_clip': 1.0}, 'loss': {'sw1_weight': 0.6, 'proto_weight': 0.25, 'de_rank_weight': 0.1, 'dir_weight': 0.05, 'sw1_projections': 32, 'proto_tau': 0.1, 'de_rank_tau': 0.2, 'dir_tau': 0.2, 'de_rank_sample_de': 256, 'de_rank_sample_non_de': 256}, 'load_param_prefixes': ['encoder', 'value_encoder', 'transformer_encoder'], 'split': {'test_genes_file': 'data/raw/test_set.csv', 'train_ratio': 0.834, 'val_ratio': 0.166, 'seed': 42}, 'metrics': {'mae_top_k': 2000, 'des_top_k': None, 'de_fdr': 0.05}, 'early_stopping': {'metric': 'overall_score'}, 'logging': {'log_interval': 100, 'save_interval': 1}, 'hardware': {'device': 'cuda', 'ddp_timeout_minutes': 240}}
21:17:29-scGPT-INFO-main: Loading data...
21:19:00-scGPT-INFO-main: Dataset: 18080 genes, 17840 in vocab
21:19:01-scGPT-INFO-main: DE gene map loaded for 120 perturbations
21:19:01-scGPT-INFO-main: Test: 30, Train: 100, Val: 20 perts
21:19:02-scGPT-INFO-main: Train cells: 149609, Val cells: 28732
21:19:02-scGPT-INFO-load_pretrained_model: Loading pretrained model from model/scGPT/best_model.pt
21:19:02-scGPT-INFO-load_pretrained_model: Model config: embsize=512, nlayers=12, nheads=8
21:19:02-scGPT-INFO-load_pretrained: Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
21:19:02-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
21:19:02-scGPT-INFO-freeze_encoder_layers: Froze 50,278,400 params, training 1,581,059 params
21:19:02-scGPT-INFO-freeze_encoder_layers: Trainable: 3.0% of model
21:19:02-scGPT-INFO-freeze_encoder_layers: Trainable components: {'cls_decoder', 'pert_encoder', 'decoder'}
21:19:09-scGPT-INFO-main: 
============================================================
Starting training...
============================================================
21:19:27-scGPT-INFO-train_epoch: | epoch   1 | 100/2375 batches | ms/batch 183.58 | loss 0.8135
21:19:27-scGPT-INFO-train_epoch:   dist 0.0858 | proto 3.0484 | de_rank 0.0000 | dir 0.0000
21:19:45-scGPT-INFO-train_epoch: | epoch   1 | 200/2375 batches | ms/batch 174.43 | loss 0.7231
21:19:45-scGPT-INFO-train_epoch:   dist 0.0644 | proto 2.7377 | de_rank 0.0000 | dir 0.0000
21:20:02-scGPT-INFO-train_epoch: | epoch   1 | 300/2375 batches | ms/batch 174.86 | loss 0.7257
21:20:02-scGPT-INFO-train_epoch:   dist 0.0656 | proto 2.7455 | de_rank 0.0000 | dir 0.0000
21:20:20-scGPT-INFO-train_epoch: | epoch   1 | 400/2375 batches | ms/batch 174.90 | loss 0.7181
21:20:20-scGPT-INFO-train_epoch:   dist 0.0679 | proto 2.7096 | de_rank 0.0000 | dir 0.0000
21:20:37-scGPT-INFO-train_epoch: | epoch   1 | 500/2375 batches | ms/batch 174.75 | loss 0.7123
21:20:37-scGPT-INFO-train_epoch:   dist 0.0633 | proto 2.6973 | de_rank 0.0000 | dir 0.0000
21:20:55-scGPT-INFO-train_epoch: | epoch   1 | 600/2375 batches | ms/batch 175.48 | loss 0.7183
21:20:55-scGPT-INFO-train_epoch:   dist 0.0665 | proto 2.7137 | de_rank 0.0000 | dir 0.0000
21:21:12-scGPT-INFO-train_epoch: | epoch   1 | 700/2375 batches | ms/batch 175.45 | loss 0.7237
21:21:12-scGPT-INFO-train_epoch:   dist 0.0677 | proto 2.7324 | de_rank 0.0000 | dir 0.0000
21:21:30-scGPT-INFO-train_epoch: | epoch   1 | 800/2375 batches | ms/batch 176.09 | loss 0.7197
21:21:30-scGPT-INFO-train_epoch:   dist 0.0645 | proto 2.7238 | de_rank 0.0000 | dir 0.0000
21:21:47-scGPT-INFO-train_epoch: | epoch   1 | 900/2375 batches | ms/batch 175.79 | loss 0.7067
21:21:47-scGPT-INFO-train_epoch:   dist 0.0605 | proto 2.6816 | de_rank 0.0000 | dir 0.0000
21:22:05-scGPT-INFO-train_epoch: | epoch   1 | 1000/2375 batches | ms/batch 175.46 | loss 0.7085
21:22:05-scGPT-INFO-train_epoch:   dist 0.0595 | proto 2.6912 | de_rank 0.0000 | dir 0.0000
21:22:23-scGPT-INFO-train_epoch: | epoch   1 | 1100/2375 batches | ms/batch 175.34 | loss 0.7140
21:22:23-scGPT-INFO-train_epoch:   dist 0.0633 | proto 2.7040 | de_rank 0.0000 | dir 0.0000
21:22:40-scGPT-INFO-train_epoch: | epoch   1 | 1200/2375 batches | ms/batch 175.40 | loss 0.7128
21:22:40-scGPT-INFO-train_epoch:   dist 0.0696 | proto 2.6839 | de_rank 0.0000 | dir 0.0000
21:22:58-scGPT-INFO-train_epoch: | epoch   1 | 1300/2375 batches | ms/batch 175.60 | loss 0.7135
21:22:58-scGPT-INFO-train_epoch:   dist 0.0676 | proto 2.6917 | de_rank 0.0000 | dir 0.0000
21:23:15-scGPT-INFO-train_epoch: | epoch   1 | 1400/2375 batches | ms/batch 175.54 | loss 0.7183
21:23:15-scGPT-INFO-train_epoch:   dist 0.0643 | proto 2.7188 | de_rank 0.0000 | dir 0.0000
21:23:33-scGPT-INFO-train_epoch: | epoch   1 | 1500/2375 batches | ms/batch 175.29 | loss 0.7075
21:23:33-scGPT-INFO-train_epoch:   dist 0.0605 | proto 2.6847 | de_rank 0.0000 | dir 0.0000
21:23:50-scGPT-INFO-train_epoch: | epoch   1 | 1600/2375 batches | ms/batch 175.87 | loss 0.7217
21:23:50-scGPT-INFO-train_epoch:   dist 0.0670 | proto 2.7260 | de_rank 0.0000 | dir 0.0000
21:24:08-scGPT-INFO-train_epoch: | epoch   1 | 1700/2375 batches | ms/batch 175.56 | loss 0.7016
21:24:08-scGPT-INFO-train_epoch:   dist 0.0543 | proto 2.6759 | de_rank 0.0000 | dir 0.0000
21:24:25-scGPT-INFO-train_epoch: | epoch   1 | 1800/2375 batches | ms/batch 175.86 | loss 0.7112
21:24:25-scGPT-INFO-train_epoch:   dist 0.0591 | proto 2.7028 | de_rank 0.0000 | dir 0.0000
21:24:43-scGPT-INFO-train_epoch: | epoch   1 | 1900/2375 batches | ms/batch 175.84 | loss 0.7069
21:24:43-scGPT-INFO-train_epoch:   dist 0.0512 | proto 2.7045 | de_rank 0.0000 | dir 0.0000
21:25:01-scGPT-INFO-train_epoch: | epoch   1 | 2000/2375 batches | ms/batch 176.24 | loss 0.7150
21:25:01-scGPT-INFO-train_epoch:   dist 0.0583 | proto 2.7201 | de_rank 0.0000 | dir 0.0000
21:25:18-scGPT-INFO-train_epoch: | epoch   1 | 2100/2375 batches | ms/batch 176.35 | loss 0.7153
21:25:18-scGPT-INFO-train_epoch:   dist 0.0589 | proto 2.7199 | de_rank 0.0000 | dir 0.0000
21:25:36-scGPT-INFO-train_epoch: | epoch   1 | 2200/2375 batches | ms/batch 175.79 | loss 0.7022
21:25:36-scGPT-INFO-train_epoch:   dist 0.0544 | proto 2.6785 | de_rank 0.0000 | dir 0.0000
21:25:53-scGPT-INFO-train_epoch: | epoch   1 | 2300/2375 batches | ms/batch 175.47 | loss 0.7021
21:25:53-scGPT-INFO-train_epoch:   dist 0.0593 | proto 2.6660 | de_rank 0.0000 | dir 0.0000
21:49:50-scGPT-INFO-setup_logging: Running on 2025-12-30 21:49:50
21:49:50-scGPT-INFO-main: Config: {'paths': {'model_dir': 'model/scGPT', 'finetuned_model_dir': 'model/scGPT_finetuned', 'data_dir': 'data/processed', 'test_file': 'test.h5ad', 'train_file': 'train.h5ad', 'output_dir': 'results', 'gears_data_dir': 'data/processed/gears', 'dataset_name': 'vcc'}, 'model': {'pad_token': '<pad>', 'pad_value': 0, 'max_seq_len': 1536, 'include_zero_gene': 'batch-wise', 'embsize': 512, 'd_hid': 512, 'nlayers': 12, 'nhead': 8, 'n_layers_cls': 3, 'dropout': 0.0, 'use_fast_transformer': True}, 'inference': {'batch_size': 16, 'eval_batch_size': 16, 'seed': 42, 'control_target_gene': 'non-targeting'}, 'data': {'special_tokens': ['<pad>', '<cls>', '<eoc>'], 'pert_pad_id': 0}, 'training': {'MLM': True, 'CLS': False, 'CCE': False, 'MVC': False, 'ECS': False, 'amp': True, 'freeze_encoder': True, 'freeze_prefixes': ['encoder', 'value_encoder', 'transformer_encoder']}, 'optimizer': {'lr': 0.0001, 'batch_size': 64, 'eval_batch_size': 64, 'epochs': 15, 'schedule_interval': 1, 'schedule_gamma': 0.9, 'early_stop': 10, 'grad_clip': 1.0}, 'loss': {'sw1_weight': 0.6, 'proto_weight': 0.25, 'de_rank_weight': 0.1, 'dir_weight': 0.05, 'sw1_projections': 32, 'proto_tau': 0.1, 'de_rank_tau': 0.2, 'dir_tau': 0.2, 'de_rank_sample_de': 256, 'de_rank_sample_non_de': 256, 'de_gene_top_k': 500, 'inject_de_genes': True, 'de_inject_max': 512}, 'load_param_prefixes': ['encoder', 'value_encoder', 'transformer_encoder'], 'split': {'test_genes_file': 'data/raw/test_set.csv', 'train_ratio': 0.834, 'val_ratio': 0.166, 'seed': 42}, 'metrics': {'mae_top_k': 2000, 'des_top_k': None, 'de_fdr': 0.05}, 'early_stopping': {'metric': 'overall_score'}, 'logging': {'log_interval': 100, 'save_interval': 1}, 'hardware': {'device': 'cuda', 'ddp_timeout_minutes': 240}}
21:49:50-scGPT-INFO-main: Loading data...
21:51:40-scGPT-INFO-main: Dataset: 18080 genes, 17840 in vocab
21:51:40-scGPT-INFO-main: DE gene map loaded for 120 perturbations
21:51:40-scGPT-INFO-main: DE genes per pert: mean 500.0 | min 500 | max 500
21:51:40-scGPT-INFO-main: DE map overlap: 120/120 conditions
21:51:40-scGPT-INFO-main: Test: 30, Train: 100, Val: 20 perts
21:51:41-scGPT-INFO-main: Train cells: 149609, Val cells: 28732
21:51:41-scGPT-INFO-load_pretrained_model: Loading pretrained model from model/scGPT/best_model.pt
21:51:41-scGPT-INFO-load_pretrained_model: Model config: embsize=512, nlayers=12, nheads=8
21:51:42-scGPT-INFO-load_pretrained: Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
21:51:42-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
21:51:42-scGPT-INFO-freeze_encoder_layers: Froze 50,278,400 params, training 1,581,059 params
21:51:42-scGPT-INFO-freeze_encoder_layers: Trainable: 3.0% of model
21:51:42-scGPT-INFO-freeze_encoder_layers: Trainable components: {'decoder', 'cls_decoder', 'pert_encoder'}
21:51:50-scGPT-INFO-main: 
============================================================
Starting training...
============================================================
21:52:20-scGPT-INFO-train_epoch: | epoch   1 | 100/2375 batches | ms/batch 298.24 | loss 1.0773
21:52:20-scGPT-INFO-train_epoch:   dist 0.0660 | proto 3.3574 | de_rank 1.0843 | dir 1.7987
21:52:57-scGPT-INFO-train_epoch: | epoch   1 | 200/2375 batches | ms/batch 373.13 | loss 0.9072
21:52:57-scGPT-INFO-train_epoch:   dist 0.0506 | proto 2.8340 | de_rank 0.9881 | dir 1.3913
21:53:38-scGPT-INFO-train_epoch: | epoch   1 | 300/2375 batches | ms/batch 409.64 | loss 0.9185
21:53:38-scGPT-INFO-train_epoch:   dist 0.0513 | proto 2.8950 | de_rank 0.9566 | dir 1.3663
21:54:20-scGPT-INFO-train_epoch: | epoch   1 | 400/2375 batches | ms/batch 418.79 | loss 0.9115
21:54:20-scGPT-INFO-train_epoch:   dist 0.0545 | proto 2.8629 | de_rank 0.9477 | dir 1.3648
21:55:02-scGPT-INFO-train_epoch: | epoch   1 | 500/2375 batches | ms/batch 420.61 | loss 0.9086
21:55:02-scGPT-INFO-train_epoch:   dist 0.0487 | proto 2.8687 | de_rank 0.9403 | dir 1.3635
21:55:45-scGPT-INFO-train_epoch: | epoch   1 | 600/2375 batches | ms/batch 427.71 | loss 0.9190
21:55:45-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.9045 | de_rank 0.9348 | dir 1.3646
21:56:27-scGPT-INFO-train_epoch: | epoch   1 | 700/2375 batches | ms/batch 425.66 | loss 0.8971
21:56:27-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.8245 | de_rank 0.9415 | dir 1.3138
21:57:10-scGPT-INFO-train_epoch: | epoch   1 | 800/2375 batches | ms/batch 429.13 | loss 0.9060
21:57:10-scGPT-INFO-train_epoch:   dist 0.0509 | proto 2.8618 | de_rank 0.9316 | dir 1.3381
21:57:53-scGPT-INFO-train_epoch: | epoch   1 | 900/2375 batches | ms/batch 429.47 | loss 0.8937
21:57:53-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.8268 | de_rank 0.9301 | dir 1.3016
21:58:36-scGPT-INFO-train_epoch: | epoch   1 | 1000/2375 batches | ms/batch 427.86 | loss 0.9034
21:58:36-scGPT-INFO-train_epoch:   dist 0.0490 | proto 2.8628 | de_rank 0.9188 | dir 1.3297
21:59:19-scGPT-INFO-train_epoch: | epoch   1 | 1100/2375 batches | ms/batch 429.85 | loss 0.9002
21:59:19-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.8441 | de_rank 0.9164 | dir 1.3281
22:00:02-scGPT-INFO-train_epoch: | epoch   1 | 1200/2375 batches | ms/batch 430.52 | loss 0.8988
22:00:02-scGPT-INFO-train_epoch:   dist 0.0555 | proto 2.8280 | de_rank 0.9204 | dir 1.3283
22:00:45-scGPT-INFO-train_epoch: | epoch   1 | 1300/2375 batches | ms/batch 431.63 | loss 0.8852
22:00:45-scGPT-INFO-train_epoch:   dist 0.0544 | proto 2.7831 | de_rank 0.9160 | dir 1.3034
22:01:28-scGPT-INFO-train_epoch: | epoch   1 | 1400/2375 batches | ms/batch 431.79 | loss 0.9025
22:01:28-scGPT-INFO-train_epoch:   dist 0.0512 | proto 2.8599 | de_rank 0.9163 | dir 1.3037
22:02:12-scGPT-INFO-train_epoch: | epoch   1 | 1500/2375 batches | ms/batch 432.86 | loss 0.8916
22:02:12-scGPT-INFO-train_epoch:   dist 0.0478 | proto 2.8284 | de_rank 0.9029 | dir 1.3102
22:02:55-scGPT-INFO-train_epoch: | epoch   1 | 1600/2375 batches | ms/batch 430.37 | loss 0.8982
22:02:55-scGPT-INFO-train_epoch:   dist 0.0552 | proto 2.8397 | de_rank 0.9065 | dir 1.2906
22:03:38-scGPT-INFO-train_epoch: | epoch   1 | 1700/2375 batches | ms/batch 431.80 | loss 0.8986
22:03:38-scGPT-INFO-train_epoch:   dist 0.0453 | proto 2.8578 | de_rank 0.9136 | dir 1.3116
22:04:21-scGPT-INFO-train_epoch: | epoch   1 | 1800/2375 batches | ms/batch 431.08 | loss 0.8943
22:04:21-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.8372 | de_rank 0.9072 | dir 1.3080
22:05:04-scGPT-INFO-train_epoch: | epoch   1 | 1900/2375 batches | ms/batch 431.22 | loss 0.8982
22:05:04-scGPT-INFO-train_epoch:   dist 0.0424 | proto 2.8636 | de_rank 0.9084 | dir 1.3200
22:05:48-scGPT-INFO-train_epoch: | epoch   1 | 2000/2375 batches | ms/batch 433.77 | loss 0.9026
22:05:48-scGPT-INFO-train_epoch:   dist 0.0457 | proto 2.8743 | de_rank 0.8994 | dir 1.3329
22:06:31-scGPT-INFO-train_epoch: | epoch   1 | 2100/2375 batches | ms/batch 436.63 | loss 0.8993
22:06:31-scGPT-INFO-train_epoch:   dist 0.0496 | proto 2.8509 | de_rank 0.9113 | dir 1.3138
22:07:15-scGPT-INFO-train_epoch: | epoch   1 | 2200/2375 batches | ms/batch 435.27 | loss 0.8954
22:07:15-scGPT-INFO-train_epoch:   dist 0.0444 | proto 2.8519 | de_rank 0.9122 | dir 1.2915
22:07:58-scGPT-INFO-train_epoch: | epoch   1 | 2300/2375 batches | ms/batch 430.91 | loss 0.8791
22:07:58-scGPT-INFO-train_epoch:   dist 0.0484 | proto 2.7762 | de_rank 0.9096 | dir 1.3005
22:19:35-scGPT-INFO-setup_logging: Running on 2025-12-30 22:19:35
22:19:35-scGPT-INFO-main: Config: {'paths': {'model_dir': 'model/scGPT', 'finetuned_model_dir': 'model/scGPT_finetuned', 'data_dir': 'data/processed', 'test_file': 'test.h5ad', 'train_file': 'train.h5ad', 'output_dir': 'results', 'gears_data_dir': 'data/processed/gears', 'dataset_name': 'vcc'}, 'model': {'pad_token': '<pad>', 'pad_value': 0, 'max_seq_len': 1536, 'include_zero_gene': 'batch-wise', 'embsize': 512, 'd_hid': 512, 'nlayers': 12, 'nhead': 8, 'n_layers_cls': 3, 'dropout': 0.0, 'use_fast_transformer': True}, 'inference': {'batch_size': 16, 'eval_batch_size': 16, 'seed': 42, 'control_target_gene': 'non-targeting'}, 'data': {'special_tokens': ['<pad>', '<cls>', '<eoc>'], 'pert_pad_id': 0}, 'training': {'MLM': True, 'CLS': False, 'CCE': False, 'MVC': False, 'ECS': False, 'amp': True, 'freeze_encoder': True, 'freeze_prefixes': ['encoder', 'value_encoder', 'transformer_encoder']}, 'optimizer': {'lr': 0.0001, 'batch_size': 64, 'eval_batch_size': 64, 'epochs': 15, 'schedule_interval': 1, 'schedule_gamma': 0.9, 'early_stop': 10, 'grad_clip': 1.0}, 'loss': {'sw1_weight': 0.6, 'proto_weight': 0.25, 'de_rank_weight': 0.1, 'dir_weight': 0.05, 'sw1_projections': 32, 'proto_tau': 0.1, 'de_rank_tau': 0.2, 'dir_tau': 0.2, 'de_rank_sample_de': 256, 'de_rank_sample_non_de': 256, 'de_gene_top_k': 500, 'inject_de_genes': True, 'de_inject_max': 512}, 'load_param_prefixes': ['encoder', 'value_encoder', 'transformer_encoder'], 'split': {'test_genes_file': 'data/raw/test_set.csv', 'train_ratio': 0.834, 'val_ratio': 0.166, 'seed': 42}, 'metrics': {'mae_top_k': 2000, 'des_top_k': None, 'de_fdr': 0.05}, 'early_stopping': {'metric': 'overall_score'}, 'logging': {'log_interval': 100, 'save_interval': 1}, 'hardware': {'device': 'cuda', 'ddp_timeout_minutes': 240}}
22:19:35-scGPT-INFO-main: Loading data...
22:21:21-scGPT-INFO-main: Dataset: 18080 genes, 17840 in vocab
22:21:21-scGPT-INFO-main: DE gene map loaded for 120 perturbations
22:21:21-scGPT-INFO-main: DE genes per pert: mean 500.0 | min 500 | max 500
22:21:21-scGPT-INFO-main: DE map overlap: 120/120 conditions
22:21:21-scGPT-INFO-main: Test: 30, Train: 100, Val: 20 perts
22:21:22-scGPT-INFO-main: Train cells: 149609, Val cells: 28732
22:21:22-scGPT-INFO-load_pretrained_model: Loading pretrained model from model/scGPT/best_model.pt
22:21:22-scGPT-INFO-load_pretrained_model: Model config: embsize=512, nlayers=12, nheads=8
22:21:23-scGPT-INFO-load_pretrained: Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
22:21:23-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
22:21:23-scGPT-INFO-freeze_encoder_layers: Froze 50,278,400 params, training 1,581,059 params
22:21:23-scGPT-INFO-freeze_encoder_layers: Trainable: 3.0% of model
22:21:23-scGPT-INFO-freeze_encoder_layers: Trainable components: {'decoder', 'cls_decoder', 'pert_encoder'}
22:21:33-scGPT-INFO-main: 
============================================================
Starting training...
============================================================
22:22:02-scGPT-INFO-train_epoch: | epoch   1 | 100/2375 batches | ms/batch 294.00 | loss 1.0773
22:22:02-scGPT-INFO-train_epoch:   dist 0.0660 | proto 3.3574 | de_rank 1.0843 | dir 1.7987
22:22:40-scGPT-INFO-train_epoch: | epoch   1 | 200/2375 batches | ms/batch 378.02 | loss 0.9072
22:22:40-scGPT-INFO-train_epoch:   dist 0.0506 | proto 2.8340 | de_rank 0.9881 | dir 1.3913
22:23:21-scGPT-INFO-train_epoch: | epoch   1 | 300/2375 batches | ms/batch 408.56 | loss 0.9185
22:23:21-scGPT-INFO-train_epoch:   dist 0.0513 | proto 2.8950 | de_rank 0.9566 | dir 1.3663
22:24:02-scGPT-INFO-train_epoch: | epoch   1 | 400/2375 batches | ms/batch 415.67 | loss 0.9115
22:24:02-scGPT-INFO-train_epoch:   dist 0.0545 | proto 2.8629 | de_rank 0.9477 | dir 1.3648
22:24:45-scGPT-INFO-train_epoch: | epoch   1 | 500/2375 batches | ms/batch 421.01 | loss 0.9086
22:24:45-scGPT-INFO-train_epoch:   dist 0.0487 | proto 2.8687 | de_rank 0.9403 | dir 1.3635
22:25:27-scGPT-INFO-train_epoch: | epoch   1 | 600/2375 batches | ms/batch 424.02 | loss 0.9190
22:25:27-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.9045 | de_rank 0.9348 | dir 1.3646
22:26:10-scGPT-INFO-train_epoch: | epoch   1 | 700/2375 batches | ms/batch 425.98 | loss 0.8971
22:26:10-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.8245 | de_rank 0.9415 | dir 1.3138
22:26:52-scGPT-INFO-train_epoch: | epoch   1 | 800/2375 batches | ms/batch 425.88 | loss 0.9060
22:26:52-scGPT-INFO-train_epoch:   dist 0.0509 | proto 2.8618 | de_rank 0.9316 | dir 1.3381
22:27:35-scGPT-INFO-train_epoch: | epoch   1 | 900/2375 batches | ms/batch 426.18 | loss 0.8937
22:27:35-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.8268 | de_rank 0.9301 | dir 1.3016
22:28:17-scGPT-INFO-train_epoch: | epoch   1 | 1000/2375 batches | ms/batch 425.64 | loss 0.9034
22:28:17-scGPT-INFO-train_epoch:   dist 0.0490 | proto 2.8628 | de_rank 0.9188 | dir 1.3297
22:29:00-scGPT-INFO-train_epoch: | epoch   1 | 1100/2375 batches | ms/batch 426.34 | loss 0.9002
22:29:00-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.8441 | de_rank 0.9164 | dir 1.3281
22:29:43-scGPT-INFO-train_epoch: | epoch   1 | 1200/2375 batches | ms/batch 427.64 | loss 0.8988
22:29:43-scGPT-INFO-train_epoch:   dist 0.0555 | proto 2.8280 | de_rank 0.9204 | dir 1.3283
22:30:26-scGPT-INFO-train_epoch: | epoch   1 | 1300/2375 batches | ms/batch 429.05 | loss 0.8852
22:30:26-scGPT-INFO-train_epoch:   dist 0.0544 | proto 2.7831 | de_rank 0.9160 | dir 1.3034
22:31:08-scGPT-INFO-train_epoch: | epoch   1 | 1400/2375 batches | ms/batch 427.59 | loss 0.9025
22:31:08-scGPT-INFO-train_epoch:   dist 0.0512 | proto 2.8599 | de_rank 0.9163 | dir 1.3037
22:31:51-scGPT-INFO-train_epoch: | epoch   1 | 1500/2375 batches | ms/batch 427.61 | loss 0.8916
22:31:51-scGPT-INFO-train_epoch:   dist 0.0478 | proto 2.8284 | de_rank 0.9029 | dir 1.3102
22:32:34-scGPT-INFO-train_epoch: | epoch   1 | 1600/2375 batches | ms/batch 425.86 | loss 0.8982
22:32:34-scGPT-INFO-train_epoch:   dist 0.0552 | proto 2.8397 | de_rank 0.9065 | dir 1.2906
22:33:16-scGPT-INFO-train_epoch: | epoch   1 | 1700/2375 batches | ms/batch 424.33 | loss 0.8986
22:33:16-scGPT-INFO-train_epoch:   dist 0.0453 | proto 2.8578 | de_rank 0.9136 | dir 1.3116
22:33:59-scGPT-INFO-train_epoch: | epoch   1 | 1800/2375 batches | ms/batch 429.00 | loss 0.8943
22:33:59-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.8372 | de_rank 0.9072 | dir 1.3080
22:34:42-scGPT-INFO-train_epoch: | epoch   1 | 1900/2375 batches | ms/batch 427.21 | loss 0.8982
22:34:42-scGPT-INFO-train_epoch:   dist 0.0424 | proto 2.8636 | de_rank 0.9084 | dir 1.3200
22:35:25-scGPT-INFO-train_epoch: | epoch   1 | 2000/2375 batches | ms/batch 428.95 | loss 0.9026
22:35:25-scGPT-INFO-train_epoch:   dist 0.0457 | proto 2.8743 | de_rank 0.8994 | dir 1.3329
22:36:08-scGPT-INFO-train_epoch: | epoch   1 | 2100/2375 batches | ms/batch 430.81 | loss 0.8993
22:36:08-scGPT-INFO-train_epoch:   dist 0.0496 | proto 2.8509 | de_rank 0.9113 | dir 1.3138
22:36:51-scGPT-INFO-train_epoch: | epoch   1 | 2200/2375 batches | ms/batch 432.47 | loss 0.8954
22:36:51-scGPT-INFO-train_epoch:   dist 0.0444 | proto 2.8519 | de_rank 0.9122 | dir 1.2915
22:37:34-scGPT-INFO-train_epoch: | epoch   1 | 2300/2375 batches | ms/batch 433.33 | loss 0.8791
22:37:34-scGPT-INFO-train_epoch:   dist 0.0484 | proto 2.7762 | de_rank 0.9096 | dir 1.3005
22:42:47-scGPT-INFO-setup_logging: Running on 2025-12-30 22:42:47
22:42:47-scGPT-INFO-main: Config: {'paths': {'model_dir': 'model/scGPT', 'finetuned_model_dir': 'model/scGPT_finetuned', 'data_dir': 'data/processed', 'test_file': 'test.h5ad', 'train_file': 'train.h5ad', 'output_dir': 'results', 'gears_data_dir': 'data/processed/gears', 'dataset_name': 'vcc'}, 'model': {'pad_token': '<pad>', 'pad_value': 0, 'max_seq_len': 1536, 'include_zero_gene': 'batch-wise', 'embsize': 512, 'd_hid': 512, 'nlayers': 12, 'nhead': 8, 'n_layers_cls': 3, 'dropout': 0.0, 'use_fast_transformer': True}, 'inference': {'batch_size': 16, 'eval_batch_size': 16, 'seed': 42, 'control_target_gene': 'non-targeting'}, 'data': {'special_tokens': ['<pad>', '<cls>', '<eoc>'], 'pert_pad_id': 0}, 'training': {'MLM': True, 'CLS': False, 'CCE': False, 'MVC': False, 'ECS': False, 'amp': True, 'freeze_encoder': True, 'freeze_prefixes': ['encoder', 'value_encoder', 'transformer_encoder']}, 'optimizer': {'lr': 0.0001, 'batch_size': 64, 'eval_batch_size': 64, 'epochs': 15, 'schedule_interval': 1, 'schedule_gamma': 0.9, 'early_stop': 10, 'grad_clip': 1.0}, 'loss': {'sw1_weight': 0.6, 'proto_weight': 0.25, 'de_rank_weight': 0.1, 'dir_weight': 0.05, 'sw1_projections': 32, 'proto_tau': 0.1, 'de_rank_tau': 0.2, 'dir_tau': 0.2, 'de_rank_sample_de': 256, 'de_rank_sample_non_de': 256, 'de_gene_top_k': 500, 'inject_de_genes': True, 'de_inject_max': 512}, 'load_param_prefixes': ['encoder', 'value_encoder', 'transformer_encoder'], 'split': {'test_genes_file': 'data/raw/test_set.csv', 'train_ratio': 0.834, 'val_ratio': 0.166, 'seed': 42}, 'metrics': {'mae_top_k': 2000, 'des_top_k': None, 'de_fdr': 0.05}, 'early_stopping': {'metric': 'overall_score'}, 'logging': {'log_interval': 100, 'save_interval': 1}, 'hardware': {'device': 'cuda', 'ddp_timeout_minutes': 240}}
22:42:47-scGPT-INFO-main: Loading data...
22:44:31-scGPT-INFO-main: Dataset: 18080 genes, 17840 in vocab
22:44:32-scGPT-INFO-main: DE gene map loaded for 120 perturbations
22:44:32-scGPT-INFO-main: DE genes per pert: mean 500.0 | min 500 | max 500
22:44:32-scGPT-INFO-main: DE map overlap: 120/120 conditions
22:44:32-scGPT-INFO-main: Test: 30, Train: 100, Val: 20 perts
22:44:33-scGPT-INFO-main: Train cells: 149609, Val cells: 28732
22:44:33-scGPT-INFO-load_pretrained_model: Loading pretrained model from model/scGPT/best_model.pt
22:44:33-scGPT-INFO-load_pretrained_model: Model config: embsize=512, nlayers=12, nheads=8
22:44:34-scGPT-INFO-load_pretrained: Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
22:44:34-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
22:44:34-scGPT-INFO-freeze_encoder_layers: Froze 50,278,400 params, training 1,581,059 params
22:44:34-scGPT-INFO-freeze_encoder_layers: Trainable: 3.0% of model
22:44:34-scGPT-INFO-freeze_encoder_layers: Trainable components: {'cls_decoder', 'decoder', 'pert_encoder'}
22:44:45-scGPT-INFO-main: 
============================================================
Starting training...
============================================================
22:45:15-scGPT-INFO-train_epoch: | epoch   1 | 100/2375 batches | ms/batch 303.17 | loss 1.0773
22:45:15-scGPT-INFO-train_epoch:   dist 0.0660 | proto 3.3574 | de_rank 1.0843 | dir 1.7987
22:45:53-scGPT-INFO-train_epoch: | epoch   1 | 200/2375 batches | ms/batch 382.96 | loss 0.9072
22:45:53-scGPT-INFO-train_epoch:   dist 0.0506 | proto 2.8340 | de_rank 0.9881 | dir 1.3913
22:46:34-scGPT-INFO-train_epoch: | epoch   1 | 300/2375 batches | ms/batch 408.57 | loss 0.9185
22:46:34-scGPT-INFO-train_epoch:   dist 0.0513 | proto 2.8950 | de_rank 0.9566 | dir 1.3663
22:47:15-scGPT-INFO-train_epoch: | epoch   1 | 400/2375 batches | ms/batch 413.04 | loss 0.9115
22:47:15-scGPT-INFO-train_epoch:   dist 0.0545 | proto 2.8629 | de_rank 0.9477 | dir 1.3648
22:47:57-scGPT-INFO-train_epoch: | epoch   1 | 500/2375 batches | ms/batch 416.58 | loss 0.9086
22:47:57-scGPT-INFO-train_epoch:   dist 0.0487 | proto 2.8687 | de_rank 0.9403 | dir 1.3635
22:48:39-scGPT-INFO-train_epoch: | epoch   1 | 600/2375 batches | ms/batch 420.91 | loss 0.9190
22:48:39-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.9045 | de_rank 0.9348 | dir 1.3646
22:49:21-scGPT-INFO-train_epoch: | epoch   1 | 700/2375 batches | ms/batch 423.52 | loss 0.8971
22:49:21-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.8245 | de_rank 0.9415 | dir 1.3138
22:50:04-scGPT-INFO-train_epoch: | epoch   1 | 800/2375 batches | ms/batch 423.40 | loss 0.9060
22:50:04-scGPT-INFO-train_epoch:   dist 0.0509 | proto 2.8618 | de_rank 0.9316 | dir 1.3381
22:50:46-scGPT-INFO-train_epoch: | epoch   1 | 900/2375 batches | ms/batch 425.88 | loss 0.8937
22:50:46-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.8268 | de_rank 0.9301 | dir 1.3016
22:51:29-scGPT-INFO-train_epoch: | epoch   1 | 1000/2375 batches | ms/batch 425.05 | loss 0.9034
22:51:29-scGPT-INFO-train_epoch:   dist 0.0490 | proto 2.8628 | de_rank 0.9188 | dir 1.3297
22:52:11-scGPT-INFO-train_epoch: | epoch   1 | 1100/2375 batches | ms/batch 424.49 | loss 0.9002
22:52:11-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.8441 | de_rank 0.9164 | dir 1.3281
22:52:54-scGPT-INFO-train_epoch: | epoch   1 | 1200/2375 batches | ms/batch 425.65 | loss 0.8988
22:52:54-scGPT-INFO-train_epoch:   dist 0.0555 | proto 2.8280 | de_rank 0.9204 | dir 1.3283
22:53:36-scGPT-INFO-train_epoch: | epoch   1 | 1300/2375 batches | ms/batch 424.21 | loss 0.8852
22:53:36-scGPT-INFO-train_epoch:   dist 0.0544 | proto 2.7831 | de_rank 0.9160 | dir 1.3034
22:54:19-scGPT-INFO-train_epoch: | epoch   1 | 1400/2375 batches | ms/batch 422.32 | loss 0.9025
22:54:19-scGPT-INFO-train_epoch:   dist 0.0512 | proto 2.8599 | de_rank 0.9163 | dir 1.3037
22:55:01-scGPT-INFO-train_epoch: | epoch   1 | 1500/2375 batches | ms/batch 425.90 | loss 0.8916
22:55:01-scGPT-INFO-train_epoch:   dist 0.0478 | proto 2.8284 | de_rank 0.9029 | dir 1.3102
22:55:44-scGPT-INFO-train_epoch: | epoch   1 | 1600/2375 batches | ms/batch 427.29 | loss 0.8982
22:55:44-scGPT-INFO-train_epoch:   dist 0.0552 | proto 2.8397 | de_rank 0.9065 | dir 1.2906
22:56:26-scGPT-INFO-train_epoch: | epoch   1 | 1700/2375 batches | ms/batch 424.67 | loss 0.8986
22:56:26-scGPT-INFO-train_epoch:   dist 0.0453 | proto 2.8578 | de_rank 0.9136 | dir 1.3116
22:57:09-scGPT-INFO-train_epoch: | epoch   1 | 1800/2375 batches | ms/batch 425.93 | loss 0.8943
22:57:09-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.8372 | de_rank 0.9072 | dir 1.3080
22:57:52-scGPT-INFO-train_epoch: | epoch   1 | 1900/2375 batches | ms/batch 426.50 | loss 0.8982
22:57:52-scGPT-INFO-train_epoch:   dist 0.0424 | proto 2.8636 | de_rank 0.9084 | dir 1.3200
22:58:34-scGPT-INFO-train_epoch: | epoch   1 | 2000/2375 batches | ms/batch 426.99 | loss 0.9026
22:58:34-scGPT-INFO-train_epoch:   dist 0.0457 | proto 2.8743 | de_rank 0.8994 | dir 1.3329
22:59:17-scGPT-INFO-train_epoch: | epoch   1 | 2100/2375 batches | ms/batch 427.92 | loss 0.8993
22:59:17-scGPT-INFO-train_epoch:   dist 0.0496 | proto 2.8509 | de_rank 0.9113 | dir 1.3138
23:00:00-scGPT-INFO-train_epoch: | epoch   1 | 2200/2375 batches | ms/batch 429.94 | loss 0.8954
23:00:00-scGPT-INFO-train_epoch:   dist 0.0444 | proto 2.8519 | de_rank 0.9122 | dir 1.2915
23:00:43-scGPT-INFO-train_epoch: | epoch   1 | 2300/2375 batches | ms/batch 427.82 | loss 0.8791
23:00:43-scGPT-INFO-train_epoch:   dist 0.0484 | proto 2.7762 | de_rank 0.9096 | dir 1.3005
00:10:35-scGPT-INFO-main: | epoch   1 | time: 5150.70s | loss: 19.4788 | PDS: 0.5294 | DES: 0.2722 | MAE: 1.2803
00:10:35-scGPT-INFO-main:   overall_score: 6.15
00:10:36-scGPT-INFO-main:   -> New best (overall_score=6.15)
00:11:19-scGPT-INFO-train_epoch: | epoch   2 | 100/2375 batches | ms/batch 430.80 | loss 0.8880
00:11:19-scGPT-INFO-train_epoch:   dist 0.0518 | proto 2.7999 | de_rank 0.9122 | dir 1.3149
00:12:02-scGPT-INFO-train_epoch: | epoch   2 | 200/2375 batches | ms/batch 426.36 | loss 0.8875
00:12:02-scGPT-INFO-train_epoch:   dist 0.0480 | proto 2.8144 | de_rank 0.8978 | dir 1.3069
00:12:44-scGPT-INFO-train_epoch: | epoch   2 | 300/2375 batches | ms/batch 424.73 | loss 0.8938
00:12:44-scGPT-INFO-train_epoch:   dist 0.0462 | proto 2.8415 | de_rank 0.8978 | dir 1.3180
00:13:27-scGPT-INFO-train_epoch: | epoch   2 | 400/2375 batches | ms/batch 425.63 | loss 0.8773
00:13:27-scGPT-INFO-train_epoch:   dist 0.0498 | proto 2.7743 | de_rank 0.9026 | dir 1.2719
00:14:09-scGPT-INFO-train_epoch: | epoch   2 | 500/2375 batches | ms/batch 427.51 | loss 0.8913
00:14:09-scGPT-INFO-train_epoch:   dist 0.0472 | proto 2.8334 | de_rank 0.9062 | dir 1.2798
00:14:52-scGPT-INFO-train_epoch: | epoch   2 | 600/2375 batches | ms/batch 426.90 | loss 0.8713
00:14:52-scGPT-INFO-train_epoch:   dist 0.0489 | proto 2.7463 | de_rank 0.9037 | dir 1.2997
00:15:35-scGPT-INFO-train_epoch: | epoch   2 | 700/2375 batches | ms/batch 427.43 | loss 0.8839
00:15:35-scGPT-INFO-train_epoch:   dist 0.0508 | proto 2.7993 | de_rank 0.8931 | dir 1.2863
00:16:18-scGPT-INFO-train_epoch: | epoch   2 | 800/2375 batches | ms/batch 432.15 | loss 0.8922
00:16:18-scGPT-INFO-train_epoch:   dist 0.0493 | proto 2.8337 | de_rank 0.9001 | dir 1.2838
00:17:01-scGPT-INFO-train_epoch: | epoch   2 | 900/2375 batches | ms/batch 429.54 | loss 0.8842
00:17:01-scGPT-INFO-train_epoch:   dist 0.0502 | proto 2.7994 | de_rank 0.8936 | dir 1.2972
00:17:44-scGPT-INFO-train_epoch: | epoch   2 | 1000/2375 batches | ms/batch 431.61 | loss 0.8854
00:17:44-scGPT-INFO-train_epoch:   dist 0.0475 | proto 2.8127 | de_rank 0.9019 | dir 1.2698
00:18:27-scGPT-INFO-train_epoch: | epoch   2 | 1100/2375 batches | ms/batch 430.13 | loss 0.8764
00:18:27-scGPT-INFO-train_epoch:   dist 0.0531 | proto 2.7607 | de_rank 0.9022 | dir 1.2827
00:19:10-scGPT-INFO-train_epoch: | epoch   2 | 1200/2375 batches | ms/batch 431.53 | loss 0.8877
00:19:10-scGPT-INFO-train_epoch:   dist 0.0457 | proto 2.8239 | de_rank 0.9075 | dir 1.2709
00:19:53-scGPT-INFO-train_epoch: | epoch   2 | 1300/2375 batches | ms/batch 431.42 | loss 0.8774
00:19:53-scGPT-INFO-train_epoch:   dist 0.0514 | proto 2.7731 | de_rank 0.9018 | dir 1.2610
00:20:37-scGPT-INFO-train_epoch: | epoch   2 | 1400/2375 batches | ms/batch 431.32 | loss 0.8850
00:20:37-scGPT-INFO-train_epoch:   dist 0.0509 | proto 2.8039 | de_rank 0.8976 | dir 1.2737
00:21:20-scGPT-INFO-train_epoch: | epoch   2 | 1500/2375 batches | ms/batch 431.80 | loss 0.8877
00:21:20-scGPT-INFO-train_epoch:   dist 0.0519 | proto 2.8096 | de_rank 0.8947 | dir 1.2942
00:22:03-scGPT-INFO-train_epoch: | epoch   2 | 1600/2375 batches | ms/batch 432.56 | loss 0.8820
00:22:03-scGPT-INFO-train_epoch:   dist 0.0446 | proto 2.8025 | de_rank 0.9023 | dir 1.2874
00:22:46-scGPT-INFO-train_epoch: | epoch   2 | 1700/2375 batches | ms/batch 430.96 | loss 0.8830
00:22:46-scGPT-INFO-train_epoch:   dist 0.0557 | proto 2.7844 | de_rank 0.8983 | dir 1.2739
00:23:30-scGPT-INFO-train_epoch: | epoch   2 | 1800/2375 batches | ms/batch 433.79 | loss 0.8900
00:23:30-scGPT-INFO-train_epoch:   dist 0.0448 | proto 2.8318 | de_rank 0.9105 | dir 1.2828
00:24:13-scGPT-INFO-train_epoch: | epoch   2 | 1900/2375 batches | ms/batch 434.02 | loss 0.8796
00:24:13-scGPT-INFO-train_epoch:   dist 0.0427 | proto 2.7977 | de_rank 0.9002 | dir 1.2910
00:24:56-scGPT-INFO-train_epoch: | epoch   2 | 2000/2375 batches | ms/batch 433.47 | loss 0.8783
00:24:56-scGPT-INFO-train_epoch:   dist 0.0531 | proto 2.7723 | de_rank 0.8978 | dir 1.2718
00:25:40-scGPT-INFO-train_epoch: | epoch   2 | 2100/2375 batches | ms/batch 432.56 | loss 0.8735
00:25:40-scGPT-INFO-train_epoch:   dist 0.0518 | proto 2.7564 | de_rank 0.8990 | dir 1.2680
00:26:23-scGPT-INFO-train_epoch: | epoch   2 | 2200/2375 batches | ms/batch 432.40 | loss 0.8794
00:26:23-scGPT-INFO-train_epoch:   dist 0.0490 | proto 2.7894 | de_rank 0.8918 | dir 1.2688
00:27:06-scGPT-INFO-train_epoch: | epoch   2 | 2300/2375 batches | ms/batch 429.60 | loss 0.8845
00:27:06-scGPT-INFO-train_epoch:   dist 0.0451 | proto 2.8149 | de_rank 0.8879 | dir 1.2995
01:36:45-scGPT-INFO-main: | epoch   2 | time: 5169.28s | loss: 18.9626 | PDS: 0.5294 | DES: 0.2719 | MAE: 1.2898
01:36:45-scGPT-INFO-main:   overall_score: 6.14
01:37:28-scGPT-INFO-train_epoch: | epoch   3 | 100/2375 batches | ms/batch 428.71 | loss 0.8715
01:37:28-scGPT-INFO-train_epoch:   dist 0.0450 | proto 2.7588 | de_rank 0.9047 | dir 1.2861
01:38:10-scGPT-INFO-train_epoch: | epoch   3 | 200/2375 batches | ms/batch 422.17 | loss 0.8923
01:38:10-scGPT-INFO-train_epoch:   dist 0.0478 | proto 2.8441 | de_rank 0.8885 | dir 1.2744
01:38:53-scGPT-INFO-train_epoch: | epoch   3 | 300/2375 batches | ms/batch 422.53 | loss 0.8867
01:38:53-scGPT-INFO-train_epoch:   dist 0.0491 | proto 2.8164 | de_rank 0.8865 | dir 1.2888
01:39:35-scGPT-INFO-train_epoch: | epoch   3 | 400/2375 batches | ms/batch 423.41 | loss 0.8830
01:39:35-scGPT-INFO-train_epoch:   dist 0.0435 | proto 2.8121 | de_rank 0.8880 | dir 1.3018
01:40:17-scGPT-INFO-train_epoch: | epoch   3 | 500/2375 batches | ms/batch 423.43 | loss 0.8797
01:40:17-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.7897 | de_rank 0.8923 | dir 1.2821
01:41:00-scGPT-INFO-train_epoch: | epoch   3 | 600/2375 batches | ms/batch 425.95 | loss 0.8839
01:41:00-scGPT-INFO-train_epoch:   dist 0.0484 | proto 2.8058 | de_rank 0.8949 | dir 1.2782
01:41:43-scGPT-INFO-train_epoch: | epoch   3 | 700/2375 batches | ms/batch 428.33 | loss 0.8758
01:41:43-scGPT-INFO-train_epoch:   dist 0.0439 | proto 2.7871 | de_rank 0.8858 | dir 1.2815
01:42:26-scGPT-INFO-train_epoch: | epoch   3 | 800/2375 batches | ms/batch 430.14 | loss 0.8775
01:42:26-scGPT-INFO-train_epoch:   dist 0.0506 | proto 2.7791 | de_rank 0.8880 | dir 1.2716
01:43:09-scGPT-INFO-train_epoch: | epoch   3 | 900/2375 batches | ms/batch 430.93 | loss 0.8794
01:43:09-scGPT-INFO-train_epoch:   dist 0.0493 | proto 2.7887 | de_rank 0.8872 | dir 1.2789
01:43:52-scGPT-INFO-train_epoch: | epoch   3 | 1000/2375 batches | ms/batch 431.24 | loss 0.8702
01:43:52-scGPT-INFO-train_epoch:   dist 0.0506 | proto 2.7450 | de_rank 0.8887 | dir 1.2940
01:44:35-scGPT-INFO-train_epoch: | epoch   3 | 1100/2375 batches | ms/batch 430.79 | loss 0.8845
01:44:35-scGPT-INFO-train_epoch:   dist 0.0546 | proto 2.7971 | de_rank 0.8857 | dir 1.2775
01:45:18-scGPT-INFO-train_epoch: | epoch   3 | 1200/2375 batches | ms/batch 433.00 | loss 0.8872
01:45:18-scGPT-INFO-train_epoch:   dist 0.0513 | proto 2.8157 | de_rank 0.8914 | dir 1.2671
01:46:01-scGPT-INFO-train_epoch: | epoch   3 | 1300/2375 batches | ms/batch 431.51 | loss 0.8699
01:46:01-scGPT-INFO-train_epoch:   dist 0.0478 | proto 2.7522 | de_rank 0.8855 | dir 1.2920
01:46:45-scGPT-INFO-train_epoch: | epoch   3 | 1400/2375 batches | ms/batch 432.03 | loss 0.8881
01:46:45-scGPT-INFO-train_epoch:   dist 0.0504 | proto 2.8199 | de_rank 0.8777 | dir 1.3030
01:47:28-scGPT-INFO-train_epoch: | epoch   3 | 1500/2375 batches | ms/batch 432.41 | loss 0.8731
01:47:28-scGPT-INFO-train_epoch:   dist 0.0493 | proto 2.7639 | de_rank 0.8886 | dir 1.2744
01:48:11-scGPT-INFO-train_epoch: | epoch   3 | 1600/2375 batches | ms/batch 434.73 | loss 0.8748
01:48:11-scGPT-INFO-train_epoch:   dist 0.0495 | proto 2.7695 | de_rank 0.8852 | dir 1.2844
01:48:55-scGPT-INFO-train_epoch: | epoch   3 | 1700/2375 batches | ms/batch 435.13 | loss 0.8723
01:48:55-scGPT-INFO-train_epoch:   dist 0.0476 | proto 2.7619 | de_rank 0.8859 | dir 1.2928
01:49:38-scGPT-INFO-train_epoch: | epoch   3 | 1800/2375 batches | ms/batch 432.73 | loss 0.8856
01:49:38-scGPT-INFO-train_epoch:   dist 0.0496 | proto 2.8120 | de_rank 0.8909 | dir 1.2741
01:50:21-scGPT-INFO-train_epoch: | epoch   3 | 1900/2375 batches | ms/batch 431.39 | loss 0.8765
01:50:21-scGPT-INFO-train_epoch:   dist 0.0478 | proto 2.7822 | de_rank 0.8835 | dir 1.2783
01:51:04-scGPT-INFO-train_epoch: | epoch   3 | 2000/2375 batches | ms/batch 428.95 | loss 0.8924
01:51:04-scGPT-INFO-train_epoch:   dist 0.0455 | proto 2.8463 | de_rank 0.8855 | dir 1.2991
01:51:47-scGPT-INFO-train_epoch: | epoch   3 | 2100/2375 batches | ms/batch 428.68 | loss 0.8836
01:51:47-scGPT-INFO-train_epoch:   dist 0.0535 | proto 2.7950 | de_rank 0.8880 | dir 1.2784
01:52:30-scGPT-INFO-train_epoch: | epoch   3 | 2200/2375 batches | ms/batch 427.71 | loss 0.8794
01:52:30-scGPT-INFO-train_epoch:   dist 0.0459 | proto 2.7973 | de_rank 0.8871 | dir 1.2774
01:53:12-scGPT-INFO-train_epoch: | epoch   3 | 2300/2375 batches | ms/batch 425.69 | loss 0.8809
01:53:12-scGPT-INFO-train_epoch:   dist 0.0463 | proto 2.7989 | de_rank 0.8909 | dir 1.2854
03:02:37-scGPT-INFO-main: | epoch   3 | time: 5151.95s | loss: 18.8879 | PDS: 0.5294 | DES: 0.2735 | MAE: 1.2852
03:02:37-scGPT-INFO-main:   overall_score: 6.20
03:02:38-scGPT-INFO-main:   -> New best (overall_score=6.20)
03:03:21-scGPT-INFO-train_epoch: | epoch   4 | 100/2375 batches | ms/batch 430.83 | loss 0.8880
03:03:21-scGPT-INFO-train_epoch:   dist 0.0524 | proto 2.8122 | de_rank 0.8912 | dir 1.2878
03:04:03-scGPT-INFO-train_epoch: | epoch   4 | 200/2375 batches | ms/batch 426.68 | loss 0.8823
03:04:03-scGPT-INFO-train_epoch:   dist 0.0435 | proto 2.8156 | de_rank 0.8807 | dir 1.2858
03:04:46-scGPT-INFO-train_epoch: | epoch   4 | 300/2375 batches | ms/batch 426.24 | loss 0.8846
03:04:46-scGPT-INFO-train_epoch:   dist 0.0489 | proto 2.8095 | de_rank 0.8741 | dir 1.3101
03:05:29-scGPT-INFO-train_epoch: | epoch   4 | 400/2375 batches | ms/batch 426.30 | loss 0.8834
03:05:29-scGPT-INFO-train_epoch:   dist 0.0510 | proto 2.7990 | de_rank 0.8863 | dir 1.2878
03:06:12-scGPT-INFO-train_epoch: | epoch   4 | 500/2375 batches | ms/batch 428.27 | loss 0.8784
03:06:12-scGPT-INFO-train_epoch:   dist 0.0518 | proto 2.7810 | de_rank 0.8792 | dir 1.2824
03:06:55-scGPT-INFO-train_epoch: | epoch   4 | 600/2375 batches | ms/batch 429.70 | loss 0.8675
03:06:55-scGPT-INFO-train_epoch:   dist 0.0516 | proto 2.7390 | de_rank 0.8833 | dir 1.2690
03:07:38-scGPT-INFO-train_epoch: | epoch   4 | 700/2375 batches | ms/batch 430.67 | loss 0.8880
03:07:38-scGPT-INFO-train_epoch:   dist 0.0440 | proto 2.8367 | de_rank 0.8825 | dir 1.2836
03:08:21-scGPT-INFO-train_epoch: | epoch   4 | 800/2375 batches | ms/batch 431.06 | loss 0.8721
03:08:21-scGPT-INFO-train_epoch:   dist 0.0511 | proto 2.7574 | de_rank 0.8810 | dir 1.2788
03:09:04-scGPT-INFO-train_epoch: | epoch   4 | 900/2375 batches | ms/batch 430.17 | loss 0.8688
03:09:04-scGPT-INFO-train_epoch:   dist 0.0525 | proto 2.7447 | de_rank 0.8744 | dir 1.2739
03:09:47-scGPT-INFO-train_epoch: | epoch   4 | 1000/2375 batches | ms/batch 430.88 | loss 0.8783
03:09:47-scGPT-INFO-train_epoch:   dist 0.0559 | proto 2.7734 | de_rank 0.8787 | dir 1.2706
03:10:30-scGPT-INFO-train_epoch: | epoch   4 | 1100/2375 batches | ms/batch 429.85 | loss 0.8757
03:10:30-scGPT-INFO-train_epoch:   dist 0.0430 | proto 2.7873 | de_rank 0.8801 | dir 1.2999
03:11:13-scGPT-INFO-train_epoch: | epoch   4 | 1200/2375 batches | ms/batch 431.51 | loss 0.8834
03:11:13-scGPT-INFO-train_epoch:   dist 0.0483 | proto 2.8094 | de_rank 0.8713 | dir 1.2985
03:11:56-scGPT-INFO-train_epoch: | epoch   4 | 1300/2375 batches | ms/batch 431.05 | loss 0.8747
03:11:56-scGPT-INFO-train_epoch:   dist 0.0523 | proto 2.7616 | de_rank 0.8805 | dir 1.2962
03:12:39-scGPT-INFO-train_epoch: | epoch   4 | 1400/2375 batches | ms/batch 430.78 | loss 0.8731
03:12:39-scGPT-INFO-train_epoch:   dist 0.0497 | proto 2.7652 | de_rank 0.8797 | dir 1.2787
03:13:22-scGPT-INFO-train_epoch: | epoch   4 | 1500/2375 batches | ms/batch 428.73 | loss 0.8757
03:13:22-scGPT-INFO-train_epoch:   dist 0.0452 | proto 2.7892 | de_rank 0.8784 | dir 1.2699
03:14:05-scGPT-INFO-train_epoch: | epoch   4 | 1600/2375 batches | ms/batch 428.72 | loss 0.8757
03:14:05-scGPT-INFO-train_epoch:   dist 0.0501 | proto 2.7754 | de_rank 0.8733 | dir 1.2884
03:14:48-scGPT-INFO-train_epoch: | epoch   4 | 1700/2375 batches | ms/batch 427.00 | loss 0.8824
03:14:48-scGPT-INFO-train_epoch:   dist 0.0488 | proto 2.8059 | de_rank 0.8771 | dir 1.2798
03:15:30-scGPT-INFO-train_epoch: | epoch   4 | 1800/2375 batches | ms/batch 424.66 | loss 0.8815
03:15:30-scGPT-INFO-train_epoch:   dist 0.0512 | proto 2.7973 | de_rank 0.8742 | dir 1.2802
03:16:12-scGPT-INFO-train_epoch: | epoch   4 | 1900/2375 batches | ms/batch 424.41 | loss 0.8740
03:16:12-scGPT-INFO-train_epoch:   dist 0.0449 | proto 2.7803 | de_rank 0.8779 | dir 1.2847
03:16:55-scGPT-INFO-train_epoch: | epoch   4 | 2000/2375 batches | ms/batch 428.96 | loss 0.8801
03:16:55-scGPT-INFO-train_epoch:   dist 0.0471 | proto 2.7997 | de_rank 0.8785 | dir 1.2809
03:17:38-scGPT-INFO-train_epoch: | epoch   4 | 2100/2375 batches | ms/batch 427.71 | loss 0.8773
03:17:38-scGPT-INFO-train_epoch:   dist 0.0496 | proto 2.7862 | de_rank 0.8693 | dir 1.2809
03:18:21-scGPT-INFO-train_epoch: | epoch   4 | 2200/2375 batches | ms/batch 428.33 | loss 0.8801
03:18:21-scGPT-INFO-train_epoch:   dist 0.0497 | proto 2.7941 | de_rank 0.8764 | dir 1.2824
03:19:04-scGPT-INFO-train_epoch: | epoch   4 | 2300/2375 batches | ms/batch 432.32 | loss 0.8784
03:19:04-scGPT-INFO-train_epoch:   dist 0.0445 | proto 2.7984 | de_rank 0.8795 | dir 1.2825
04:28:00-scGPT-INFO-main: | epoch   4 | time: 5122.67s | loss: 18.7860 | PDS: 0.5294 | DES: 0.2696 | MAE: 1.2839
04:28:00-scGPT-INFO-main:   overall_score: 6.05
04:28:43-scGPT-INFO-train_epoch: | epoch   5 | 100/2375 batches | ms/batch 423.03 | loss 0.8834
04:28:43-scGPT-INFO-train_epoch:   dist 0.0598 | proto 2.7791 | de_rank 0.8848 | dir 1.2861
04:29:25-scGPT-INFO-train_epoch: | epoch   5 | 200/2375 batches | ms/batch 419.50 | loss 0.8711
04:29:25-scGPT-INFO-train_epoch:   dist 0.0505 | proto 2.7559 | de_rank 0.8779 | dir 1.2806
04:30:07-scGPT-INFO-train_epoch: | epoch   5 | 300/2375 batches | ms/batch 420.79 | loss 0.8812
04:30:07-scGPT-INFO-train_epoch:   dist 0.0424 | proto 2.8182 | de_rank 0.8760 | dir 1.2714
04:30:49-scGPT-INFO-train_epoch: | epoch   5 | 400/2375 batches | ms/batch 420.50 | loss 0.8773
04:30:49-scGPT-INFO-train_epoch:   dist 0.0515 | proto 2.7827 | de_rank 0.8696 | dir 1.2744
04:31:31-scGPT-INFO-train_epoch: | epoch   5 | 500/2375 batches | ms/batch 420.74 | loss 0.8703
04:31:31-scGPT-INFO-train_epoch:   dist 0.0575 | proto 2.7393 | de_rank 0.8735 | dir 1.2725
04:32:13-scGPT-INFO-train_epoch: | epoch   5 | 600/2375 batches | ms/batch 420.82 | loss 0.8785
04:32:13-scGPT-INFO-train_epoch:   dist 0.0422 | proto 2.8080 | de_rank 0.8693 | dir 1.2840
04:32:55-scGPT-INFO-train_epoch: | epoch   5 | 700/2375 batches | ms/batch 422.55 | loss 0.8623
04:32:55-scGPT-INFO-train_epoch:   dist 0.0400 | proto 2.7466 | de_rank 0.8781 | dir 1.2767
04:33:38-scGPT-INFO-train_epoch: | epoch   5 | 800/2375 batches | ms/batch 425.96 | loss 0.8768
04:33:38-scGPT-INFO-train_epoch:   dist 0.0510 | proto 2.7783 | de_rank 0.8765 | dir 1.2781
04:34:20-scGPT-INFO-train_epoch: | epoch   5 | 900/2375 batches | ms/batch 426.71 | loss 0.8784
04:34:20-scGPT-INFO-train_epoch:   dist 0.0506 | proto 2.7876 | de_rank 0.8713 | dir 1.2799
04:35:04-scGPT-INFO-train_epoch: | epoch   5 | 1000/2375 batches | ms/batch 432.92 | loss 0.8742
04:35:04-scGPT-INFO-train_epoch:   dist 0.0523 | proto 2.7641 | de_rank 0.8760 | dir 1.2836
04:35:47-scGPT-INFO-train_epoch: | epoch   5 | 1100/2375 batches | ms/batch 429.89 | loss 0.8835
04:35:47-scGPT-INFO-train_epoch:   dist 0.0495 | proto 2.8116 | de_rank 0.8649 | dir 1.2880
04:36:30-scGPT-INFO-train_epoch: | epoch   5 | 1200/2375 batches | ms/batch 429.59 | loss 0.8721
04:36:30-scGPT-INFO-train_epoch:   dist 0.0526 | proto 2.7572 | de_rank 0.8718 | dir 1.2816
04:37:13-scGPT-INFO-train_epoch: | epoch   5 | 1300/2375 batches | ms/batch 429.85 | loss 0.8742
04:37:13-scGPT-INFO-train_epoch:   dist 0.0505 | proto 2.7698 | de_rank 0.8730 | dir 1.2839
04:37:56-scGPT-INFO-train_epoch: | epoch   5 | 1400/2375 batches | ms/batch 429.99 | loss 0.8826
04:37:56-scGPT-INFO-train_epoch:   dist 0.0486 | proto 2.8096 | de_rank 0.8657 | dir 1.2885
04:38:39-scGPT-INFO-train_epoch: | epoch   5 | 1500/2375 batches | ms/batch 428.42 | loss 0.8722
04:38:39-scGPT-INFO-train_epoch:   dist 0.0462 | proto 2.7716 | de_rank 0.8780 | dir 1.2740
04:39:22-scGPT-INFO-train_epoch: | epoch   5 | 1600/2375 batches | ms/batch 429.59 | loss 0.8691
04:39:22-scGPT-INFO-train_epoch:   dist 0.0473 | proto 2.7585 | de_rank 0.8645 | dir 1.2936
04:40:05-scGPT-INFO-train_epoch: | epoch   5 | 1700/2375 batches | ms/batch 431.88 | loss 0.8907
04:40:05-scGPT-INFO-train_epoch:   dist 0.0533 | proto 2.8287 | de_rank 0.8619 | dir 1.3077
04:40:48-scGPT-INFO-train_epoch: | epoch   5 | 1800/2375 batches | ms/batch 435.97 | loss 0.8704
04:40:48-scGPT-INFO-train_epoch:   dist 0.0551 | proto 2.7522 | de_rank 0.8674 | dir 1.2516
04:41:32-scGPT-INFO-train_epoch: | epoch   5 | 1900/2375 batches | ms/batch 434.77 | loss 0.8773
04:41:32-scGPT-INFO-train_epoch:   dist 0.0516 | proto 2.7821 | de_rank 0.8670 | dir 1.2815
04:42:15-scGPT-INFO-train_epoch: | epoch   5 | 2000/2375 batches | ms/batch 434.68 | loss 0.8807
04:42:15-scGPT-INFO-train_epoch:   dist 0.0458 | proto 2.8050 | de_rank 0.8698 | dir 1.2997
04:42:59-scGPT-INFO-train_epoch: | epoch   5 | 2100/2375 batches | ms/batch 436.78 | loss 0.8701
04:42:59-scGPT-INFO-train_epoch:   dist 0.0487 | proto 2.7589 | de_rank 0.8719 | dir 1.2798
04:43:42-scGPT-INFO-train_epoch: | epoch   5 | 2200/2375 batches | ms/batch 434.48 | loss 0.8698
04:43:42-scGPT-INFO-train_epoch:   dist 0.0475 | proto 2.7633 | de_rank 0.8702 | dir 1.2685
04:44:26-scGPT-INFO-train_epoch: | epoch   5 | 2300/2375 batches | ms/batch 434.76 | loss 0.8731
04:44:26-scGPT-INFO-train_epoch:   dist 0.0465 | proto 2.7774 | de_rank 0.8688 | dir 1.2789
05:54:16-scGPT-INFO-main: | epoch   5 | time: 5175.77s | loss: 18.7005 | PDS: 0.5294 | DES: 0.2715 | MAE: 1.2880
05:54:16-scGPT-INFO-main:   overall_score: 6.12
05:54:59-scGPT-INFO-train_epoch: | epoch   6 | 100/2375 batches | ms/batch 428.06 | loss 0.8801
05:54:59-scGPT-INFO-train_epoch:   dist 0.0546 | proto 2.7799 | de_rank 0.8750 | dir 1.2973
05:55:41-scGPT-INFO-train_epoch: | epoch   6 | 200/2375 batches | ms/batch 422.83 | loss 0.8675
05:55:41-scGPT-INFO-train_epoch:   dist 0.0507 | proto 2.7469 | de_rank 0.8672 | dir 1.2736
05:56:24-scGPT-INFO-train_epoch: | epoch   6 | 300/2375 batches | ms/batch 424.15 | loss 0.8727
05:56:24-scGPT-INFO-train_epoch:   dist 0.0457 | proto 2.7764 | de_rank 0.8659 | dir 1.2922
05:57:06-scGPT-INFO-train_epoch: | epoch   6 | 400/2375 batches | ms/batch 423.77 | loss 0.8708
05:57:06-scGPT-INFO-train_epoch:   dist 0.0441 | proto 2.7757 | de_rank 0.8660 | dir 1.2766
05:57:48-scGPT-INFO-train_epoch: | epoch   6 | 500/2375 batches | ms/batch 422.89 | loss 0.8651
05:57:48-scGPT-INFO-train_epoch:   dist 0.0534 | proto 2.7330 | de_rank 0.8660 | dir 1.2644
05:58:31-scGPT-INFO-train_epoch: | epoch   6 | 600/2375 batches | ms/batch 424.19 | loss 0.8708
05:58:31-scGPT-INFO-train_epoch:   dist 0.0481 | proto 2.7636 | de_rank 0.8717 | dir 1.2774
05:59:13-scGPT-INFO-train_epoch: | epoch   6 | 700/2375 batches | ms/batch 425.17 | loss 0.8657
05:59:13-scGPT-INFO-train_epoch:   dist 0.0449 | proto 2.7523 | de_rank 0.8588 | dir 1.2952
05:59:56-scGPT-INFO-train_epoch: | epoch   6 | 800/2375 batches | ms/batch 427.64 | loss 0.8713
05:59:56-scGPT-INFO-train_epoch:   dist 0.0457 | proto 2.7757 | de_rank 0.8606 | dir 1.2784
06:00:39-scGPT-INFO-train_epoch: | epoch   6 | 900/2375 batches | ms/batch 427.91 | loss 0.8894
06:00:39-scGPT-INFO-train_epoch:   dist 0.0526 | proto 2.8264 | de_rank 0.8626 | dir 1.3001
06:01:22-scGPT-INFO-train_epoch: | epoch   6 | 1000/2375 batches | ms/batch 427.89 | loss 0.8728
06:01:22-scGPT-INFO-train_epoch:   dist 0.0547 | proto 2.7568 | de_rank 0.8665 | dir 1.2826
06:02:05-scGPT-INFO-train_epoch: | epoch   6 | 1100/2375 batches | ms/batch 429.44 | loss 0.8679
06:02:05-scGPT-INFO-train_epoch:   dist 0.0461 | proto 2.7576 | de_rank 0.8681 | dir 1.2809
06:02:48-scGPT-INFO-train_epoch: | epoch   6 | 1200/2375 batches | ms/batch 430.90 | loss 0.8715
06:02:48-scGPT-INFO-train_epoch:   dist 0.0500 | proto 2.7623 | de_rank 0.8641 | dir 1.2899
06:03:31-scGPT-INFO-train_epoch: | epoch   6 | 1300/2375 batches | ms/batch 430.93 | loss 0.8714
06:03:31-scGPT-INFO-train_epoch:   dist 0.0435 | proto 2.7824 | de_rank 0.8596 | dir 1.2740
06:04:14-scGPT-INFO-train_epoch: | epoch   6 | 1400/2375 batches | ms/batch 429.90 | loss 0.8691
06:04:14-scGPT-INFO-train_epoch:   dist 0.0469 | proto 2.7642 | de_rank 0.8666 | dir 1.2644
06:04:57-scGPT-INFO-train_epoch: | epoch   6 | 1500/2375 batches | ms/batch 428.39 | loss 0.8802
06:04:57-scGPT-INFO-train_epoch:   dist 0.0578 | proto 2.7830 | de_rank 0.8546 | dir 1.2851
06:05:39-scGPT-INFO-train_epoch: | epoch   6 | 1600/2375 batches | ms/batch 428.02 | loss 0.8675
06:05:39-scGPT-INFO-train_epoch:   dist 0.0556 | proto 2.7376 | de_rank 0.8560 | dir 1.2829
06:06:22-scGPT-INFO-train_epoch: | epoch   6 | 1700/2375 batches | ms/batch 426.20 | loss 0.8725
06:06:22-scGPT-INFO-train_epoch:   dist 0.0454 | proto 2.7791 | de_rank 0.8663 | dir 1.2784
06:07:05-scGPT-INFO-train_epoch: | epoch   6 | 1800/2375 batches | ms/batch 425.37 | loss 0.8732
06:07:05-scGPT-INFO-train_epoch:   dist 0.0542 | proto 2.7629 | de_rank 0.8543 | dir 1.2908
06:07:47-scGPT-INFO-train_epoch: | epoch   6 | 1900/2375 batches | ms/batch 426.38 | loss 0.8860
06:07:47-scGPT-INFO-train_epoch:   dist 0.0435 | proto 2.8337 | de_rank 0.8631 | dir 1.3044
06:08:30-scGPT-INFO-train_epoch: | epoch   6 | 2000/2375 batches | ms/batch 426.85 | loss 0.8654
06:08:30-scGPT-INFO-train_epoch:   dist 0.0528 | proto 2.7360 | de_rank 0.8657 | dir 1.2627
06:09:13-scGPT-INFO-train_epoch: | epoch   6 | 2100/2375 batches | ms/batch 427.71 | loss 0.8784
06:09:13-scGPT-INFO-train_epoch:   dist 0.0448 | proto 2.8044 | de_rank 0.8534 | dir 1.3030
06:09:55-scGPT-INFO-train_epoch: | epoch   6 | 2200/2375 batches | ms/batch 426.85 | loss 0.8667
06:09:55-scGPT-INFO-train_epoch:   dist 0.0524 | proto 2.7423 | de_rank 0.8591 | dir 1.2745
06:10:38-scGPT-INFO-train_epoch: | epoch   6 | 2300/2375 batches | ms/batch 423.69 | loss 0.8723
06:10:38-scGPT-INFO-train_epoch:   dist 0.0509 | proto 2.7679 | de_rank 0.8623 | dir 1.2705
07:19:40-scGPT-INFO-main: | epoch   6 | time: 5124.06s | loss: 18.6871 | PDS: 0.5294 | DES: 0.2738 | MAE: 1.2892
07:19:40-scGPT-INFO-main:   overall_score: 6.21
07:19:41-scGPT-INFO-main:   -> New best (overall_score=6.21)
07:20:24-scGPT-INFO-train_epoch: | epoch   7 | 100/2375 batches | ms/batch 428.56 | loss 0.8783
07:20:24-scGPT-INFO-train_epoch:   dist 0.0518 | proto 2.7851 | de_rank 0.8674 | dir 1.2845
07:21:06-scGPT-INFO-train_epoch: | epoch   7 | 200/2375 batches | ms/batch 421.49 | loss 0.8619
07:21:06-scGPT-INFO-train_epoch:   dist 0.0517 | proto 2.7290 | de_rank 0.8605 | dir 1.2517
07:21:48-scGPT-INFO-train_epoch: | epoch   7 | 300/2375 batches | ms/batch 422.55 | loss 0.8668
07:21:48-scGPT-INFO-train_epoch:   dist 0.0514 | proto 2.7480 | de_rank 0.8526 | dir 1.2744
07:22:30-scGPT-INFO-train_epoch: | epoch   7 | 400/2375 batches | ms/batch 420.68 | loss 0.8727
07:22:30-scGPT-INFO-train_epoch:   dist 0.0476 | proto 2.7769 | de_rank 0.8610 | dir 1.2760
07:23:12-scGPT-INFO-train_epoch: | epoch   7 | 500/2375 batches | ms/batch 422.12 | loss 0.8737
07:23:12-scGPT-INFO-train_epoch:   dist 0.0484 | proto 2.7778 | de_rank 0.8610 | dir 1.2810
07:23:55-scGPT-INFO-train_epoch: | epoch   7 | 600/2375 batches | ms/batch 422.99 | loss 0.8664
07:23:55-scGPT-INFO-train_epoch:   dist 0.0471 | proto 2.7555 | de_rank 0.8558 | dir 1.2735
07:24:37-scGPT-INFO-train_epoch: | epoch   7 | 700/2375 batches | ms/batch 424.40 | loss 0.8747
07:24:37-scGPT-INFO-train_epoch:   dist 0.0483 | proto 2.7837 | de_rank 0.8592 | dir 1.2770
07:25:20-scGPT-INFO-train_epoch: | epoch   7 | 800/2375 batches | ms/batch 423.99 | loss 0.8654
07:25:20-scGPT-INFO-train_epoch:   dist 0.0488 | proto 2.7447 | de_rank 0.8622 | dir 1.2745
07:26:02-scGPT-INFO-train_epoch: | epoch   7 | 900/2375 batches | ms/batch 425.91 | loss 0.8705
07:26:02-scGPT-INFO-train_epoch:   dist 0.0435 | proto 2.7765 | de_rank 0.8640 | dir 1.2783
07:26:45-scGPT-INFO-train_epoch: | epoch   7 | 1000/2375 batches | ms/batch 425.66 | loss 0.8690
07:26:45-scGPT-INFO-train_epoch:   dist 0.0494 | proto 2.7559 | de_rank 0.8528 | dir 1.3015
07:27:27-scGPT-INFO-train_epoch: | epoch   7 | 1100/2375 batches | ms/batch 424.24 | loss 0.8772
07:27:27-scGPT-INFO-train_epoch:   dist 0.0473 | proto 2.7973 | de_rank 0.8603 | dir 1.2691
07:28:10-scGPT-INFO-train_epoch: | epoch   7 | 1200/2375 batches | ms/batch 426.01 | loss 0.8647
07:28:10-scGPT-INFO-train_epoch:   dist 0.0489 | proto 2.7404 | de_rank 0.8610 | dir 1.2838
07:28:52-scGPT-INFO-train_epoch: | epoch   7 | 1300/2375 batches | ms/batch 426.79 | loss 0.8576
07:28:52-scGPT-INFO-train_epoch:   dist 0.0460 | proto 2.7211 | de_rank 0.8615 | dir 1.2715
07:29:35-scGPT-INFO-train_epoch: | epoch   7 | 1400/2375 batches | ms/batch 425.54 | loss 0.8617
07:29:35-scGPT-INFO-train_epoch:   dist 0.0453 | proto 2.7423 | de_rank 0.8597 | dir 1.2598
07:30:17-scGPT-INFO-train_epoch: | epoch   7 | 1500/2375 batches | ms/batch 424.96 | loss 0.8610
07:30:17-scGPT-INFO-train_epoch:   dist 0.0487 | proto 2.7298 | de_rank 0.8589 | dir 1.2682
07:31:00-scGPT-INFO-train_epoch: | epoch   7 | 1600/2375 batches | ms/batch 424.03 | loss 0.8697
07:31:00-scGPT-INFO-train_epoch:   dist 0.0509 | proto 2.7584 | de_rank 0.8544 | dir 1.2830
07:31:42-scGPT-INFO-train_epoch: | epoch   7 | 1700/2375 batches | ms/batch 425.03 | loss 0.8822
07:31:42-scGPT-INFO-train_epoch:   dist 0.0467 | proto 2.8189 | de_rank 0.8549 | dir 1.2803
07:32:25-scGPT-INFO-train_epoch: | epoch   7 | 1800/2375 batches | ms/batch 425.60 | loss 0.8673
07:32:25-scGPT-INFO-train_epoch:   dist 0.0530 | proto 2.7425 | de_rank 0.8595 | dir 1.2794
07:33:07-scGPT-INFO-train_epoch: | epoch   7 | 1900/2375 batches | ms/batch 424.22 | loss 0.8802
07:33:07-scGPT-INFO-train_epoch:   dist 0.0529 | proto 2.7928 | de_rank 0.8636 | dir 1.2794
07:33:50-scGPT-INFO-train_epoch: | epoch   7 | 2000/2375 batches | ms/batch 422.00 | loss 0.8708
07:33:50-scGPT-INFO-train_epoch:   dist 0.0443 | proto 2.7779 | de_rank 0.8551 | dir 1.2840
07:34:32-scGPT-INFO-train_epoch: | epoch   7 | 2100/2375 batches | ms/batch 423.31 | loss 0.8647
07:34:32-scGPT-INFO-train_epoch:   dist 0.0473 | proto 2.7468 | de_rank 0.8535 | dir 1.2852
07:35:14-scGPT-INFO-train_epoch: | epoch   7 | 2200/2375 batches | ms/batch 426.01 | loss 0.8809
07:35:14-scGPT-INFO-train_epoch:   dist 0.0489 | proto 2.8058 | de_rank 0.8558 | dir 1.2918
07:35:57-scGPT-INFO-train_epoch: | epoch   7 | 2300/2375 batches | ms/batch 424.47 | loss 0.8723
07:35:57-scGPT-INFO-train_epoch:   dist 0.0472 | proto 2.7768 | de_rank 0.8594 | dir 1.2755
08:45:47-scGPT-INFO-main: | epoch   7 | time: 5166.18s | loss: 18.6569 | PDS: 0.5294 | DES: 0.2712 | MAE: 1.2854
08:45:47-scGPT-INFO-main:   overall_score: 6.11
08:46:30-scGPT-INFO-train_epoch: | epoch   8 | 100/2375 batches | ms/batch 425.23 | loss 0.8826
08:46:30-scGPT-INFO-train_epoch:   dist 0.0417 | proto 2.8283 | de_rank 0.8648 | dir 1.2815
08:47:11-scGPT-INFO-train_epoch: | epoch   8 | 200/2375 batches | ms/batch 417.64 | loss 0.8775
08:47:11-scGPT-INFO-train_epoch:   dist 0.0529 | proto 2.7836 | de_rank 0.8511 | dir 1.2950
08:47:53-scGPT-INFO-train_epoch: | epoch   8 | 300/2375 batches | ms/batch 419.55 | loss 0.8799
08:47:53-scGPT-INFO-train_epoch:   dist 0.0501 | proto 2.8007 | de_rank 0.8566 | dir 1.2799
08:48:35-scGPT-INFO-train_epoch: | epoch   8 | 400/2375 batches | ms/batch 418.01 | loss 0.8693
08:48:35-scGPT-INFO-train_epoch:   dist 0.0458 | proto 2.7701 | de_rank 0.8552 | dir 1.2759
08:49:17-scGPT-INFO-train_epoch: | epoch   8 | 500/2375 batches | ms/batch 419.96 | loss 0.8687
08:49:17-scGPT-INFO-train_epoch:   dist 0.0504 | proto 2.7571 | de_rank 0.8555 | dir 1.2731
08:49:59-scGPT-INFO-train_epoch: | epoch   8 | 600/2375 batches | ms/batch 421.53 | loss 0.8674
08:49:59-scGPT-INFO-train_epoch:   dist 0.0514 | proto 2.7507 | de_rank 0.8484 | dir 1.2812
08:50:42-scGPT-INFO-train_epoch: | epoch   8 | 700/2375 batches | ms/batch 426.48 | loss 0.8699
08:50:42-scGPT-INFO-train_epoch:   dist 0.0452 | proto 2.7707 | de_rank 0.8568 | dir 1.2893
08:51:25-scGPT-INFO-train_epoch: | epoch   8 | 800/2375 batches | ms/batch 428.64 | loss 0.8670
08:51:25-scGPT-INFO-train_epoch:   dist 0.0457 | proto 2.7614 | de_rank 0.8537 | dir 1.2781
08:52:07-scGPT-INFO-train_epoch: | epoch   8 | 900/2375 batches | ms/batch 427.50 | loss 0.8703
08:52:07-scGPT-INFO-train_epoch:   dist 0.0489 | proto 2.7658 | de_rank 0.8555 | dir 1.2798
08:52:51-scGPT-INFO-train_epoch: | epoch   8 | 1000/2375 batches | ms/batch 430.93 | loss 0.8615
08:52:51-scGPT-INFO-train_epoch:   dist 0.0495 | proto 2.7294 | de_rank 0.8575 | dir 1.2734
08:53:34-scGPT-INFO-train_epoch: | epoch   8 | 1100/2375 batches | ms/batch 429.87 | loss 0.8647
08:53:34-scGPT-INFO-train_epoch:   dist 0.0536 | proto 2.7354 | de_rank 0.8506 | dir 1.2728
08:54:16-scGPT-INFO-train_epoch: | epoch   8 | 1200/2375 batches | ms/batch 428.47 | loss 0.8683
08:54:16-scGPT-INFO-train_epoch:   dist 0.0488 | proto 2.7587 | de_rank 0.8525 | dir 1.2811
08:54:59-scGPT-INFO-train_epoch: | epoch   8 | 1300/2375 batches | ms/batch 427.17 | loss 0.8645
08:54:59-scGPT-INFO-train_epoch:   dist 0.0504 | proto 2.7405 | de_rank 0.8555 | dir 1.2706
08:55:42-scGPT-INFO-train_epoch: | epoch   8 | 1400/2375 batches | ms/batch 429.15 | loss 0.8620
08:55:42-scGPT-INFO-train_epoch:   dist 0.0543 | proto 2.7223 | de_rank 0.8519 | dir 1.2729
08:56:25-scGPT-INFO-train_epoch: | epoch   8 | 1500/2375 batches | ms/batch 429.99 | loss 0.8679
08:56:25-scGPT-INFO-train_epoch:   dist 0.0423 | proto 2.7714 | de_rank 0.8535 | dir 1.2878
08:57:08-scGPT-INFO-train_epoch: | epoch   8 | 1600/2375 batches | ms/batch 430.83 | loss 0.8677
08:57:08-scGPT-INFO-train_epoch:   dist 0.0527 | proto 2.7487 | de_rank 0.8583 | dir 1.2627
08:57:51-scGPT-INFO-train_epoch: | epoch   8 | 1700/2375 batches | ms/batch 430.27 | loss 0.8618
08:57:51-scGPT-INFO-train_epoch:   dist 0.0487 | proto 2.7350 | de_rank 0.8537 | dir 1.2699
08:58:34-scGPT-INFO-train_epoch: | epoch   8 | 1800/2375 batches | ms/batch 428.43 | loss 0.8656
08:58:34-scGPT-INFO-train_epoch:   dist 0.0490 | proto 2.7504 | de_rank 0.8436 | dir 1.2845
08:59:17-scGPT-INFO-train_epoch: | epoch   8 | 1900/2375 batches | ms/batch 426.30 | loss 0.8650
08:59:17-scGPT-INFO-train_epoch:   dist 0.0479 | proto 2.7485 | de_rank 0.8508 | dir 1.2809
08:59:59-scGPT-INFO-train_epoch: | epoch   8 | 2000/2375 batches | ms/batch 428.12 | loss 0.8693
08:59:59-scGPT-INFO-train_epoch:   dist 0.0457 | proto 2.7700 | de_rank 0.8479 | dir 1.2908
09:00:42-scGPT-INFO-train_epoch: | epoch   8 | 2100/2375 batches | ms/batch 426.09 | loss 0.8630
09:00:42-scGPT-INFO-train_epoch:   dist 0.0485 | proto 2.7415 | de_rank 0.8486 | dir 1.2734
09:01:25-scGPT-INFO-train_epoch: | epoch   8 | 2200/2375 batches | ms/batch 426.69 | loss 0.8660
09:01:25-scGPT-INFO-train_epoch:   dist 0.0466 | proto 2.7551 | de_rank 0.8470 | dir 1.2905
09:02:07-scGPT-INFO-train_epoch: | epoch   8 | 2300/2375 batches | ms/batch 426.27 | loss 0.8609
09:02:07-scGPT-INFO-train_epoch:   dist 0.0458 | proto 2.7362 | de_rank 0.8467 | dir 1.2937
10:12:00-scGPT-INFO-main: | epoch   8 | time: 5173.37s | loss: 18.6351 | PDS: 0.5294 | DES: 0.2753 | MAE: 1.2927
10:12:00-scGPT-INFO-main:   overall_score: 6.27
10:12:01-scGPT-INFO-main:   -> New best (overall_score=6.27)
10:12:44-scGPT-INFO-train_epoch: | epoch   9 | 100/2375 batches | ms/batch 430.28 | loss 0.8771
10:12:44-scGPT-INFO-train_epoch:   dist 0.0470 | proto 2.7961 | de_rank 0.8565 | dir 1.2838
10:13:26-scGPT-INFO-train_epoch: | epoch   9 | 200/2375 batches | ms/batch 420.79 | loss 0.8651
10:13:26-scGPT-INFO-train_epoch:   dist 0.0495 | proto 2.7479 | de_rank 0.8522 | dir 1.2639
10:14:08-scGPT-INFO-train_epoch: | epoch   9 | 300/2375 batches | ms/batch 421.74 | loss 0.8682
10:14:08-scGPT-INFO-train_epoch:   dist 0.0508 | proto 2.7528 | de_rank 0.8486 | dir 1.2938
10:14:51-scGPT-INFO-train_epoch: | epoch   9 | 400/2375 batches | ms/batch 422.19 | loss 0.8614
10:14:51-scGPT-INFO-train_epoch:   dist 0.0489 | proto 2.7312 | de_rank 0.8564 | dir 1.2728
10:15:33-scGPT-INFO-train_epoch: | epoch   9 | 500/2375 batches | ms/batch 422.95 | loss 0.8760
10:15:33-scGPT-INFO-train_epoch:   dist 0.0467 | proto 2.7964 | de_rank 0.8543 | dir 1.2688
10:16:15-scGPT-INFO-train_epoch: | epoch   9 | 600/2375 batches | ms/batch 426.04 | loss 0.8676
10:16:15-scGPT-INFO-train_epoch:   dist 0.0411 | proto 2.7768 | de_rank 0.8523 | dir 1.2695
10:16:58-scGPT-INFO-train_epoch: | epoch   9 | 700/2375 batches | ms/batch 426.63 | loss 0.8594
10:16:58-scGPT-INFO-train_epoch:   dist 0.0511 | proto 2.7192 | de_rank 0.8541 | dir 1.2697
10:17:41-scGPT-INFO-train_epoch: | epoch   9 | 800/2375 batches | ms/batch 428.33 | loss 0.8689
10:17:41-scGPT-INFO-train_epoch:   dist 0.0413 | proto 2.7801 | de_rank 0.8538 | dir 1.2742
10:18:24-scGPT-INFO-train_epoch: | epoch   9 | 900/2375 batches | ms/batch 426.23 | loss 0.8585
10:18:24-scGPT-INFO-train_epoch:   dist 0.0513 | proto 2.7153 | de_rank 0.8545 | dir 1.2677
10:19:06-scGPT-INFO-train_epoch: | epoch   9 | 1000/2375 batches | ms/batch 428.03 | loss 0.8642
10:19:06-scGPT-INFO-train_epoch:   dist 0.0452 | proto 2.7554 | de_rank 0.8467 | dir 1.2705
10:19:49-scGPT-INFO-train_epoch: | epoch   9 | 1100/2375 batches | ms/batch 426.95 | loss 0.8615
10:19:49-scGPT-INFO-train_epoch:   dist 0.0486 | proto 2.7346 | de_rank 0.8524 | dir 1.2693
10:20:31-scGPT-INFO-train_epoch: | epoch   9 | 1200/2375 batches | ms/batch 423.90 | loss 0.8637
10:20:31-scGPT-INFO-train_epoch:   dist 0.0485 | proto 2.7445 | de_rank 0.8485 | dir 1.2729
10:21:14-scGPT-INFO-train_epoch: | epoch   9 | 1300/2375 batches | ms/batch 420.97 | loss 0.8680
10:21:14-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.7669 | de_rank 0.8362 | dir 1.2747
10:21:56-scGPT-INFO-train_epoch: | epoch   9 | 1400/2375 batches | ms/batch 426.75 | loss 0.8650
10:21:56-scGPT-INFO-train_epoch:   dist 0.0478 | proto 2.7511 | de_rank 0.8406 | dir 1.2892
10:22:38-scGPT-INFO-train_epoch: | epoch   9 | 1500/2375 batches | ms/batch 421.63 | loss 0.8699
10:22:38-scGPT-INFO-train_epoch:   dist 0.0476 | proto 2.7730 | de_rank 0.8480 | dir 1.2647
10:23:20-scGPT-INFO-train_epoch: | epoch   9 | 1600/2375 batches | ms/batch 420.52 | loss 0.8626
10:23:20-scGPT-INFO-train_epoch:   dist 0.0500 | proto 2.7392 | de_rank 0.8473 | dir 1.2612
10:24:03-scGPT-INFO-train_epoch: | epoch   9 | 1700/2375 batches | ms/batch 421.80 | loss 0.8720
10:24:03-scGPT-INFO-train_epoch:   dist 0.0449 | proto 2.7860 | de_rank 0.8480 | dir 1.2750
10:24:45-scGPT-INFO-train_epoch: | epoch   9 | 1800/2375 batches | ms/batch 424.72 | loss 0.8685
10:24:45-scGPT-INFO-train_epoch:   dist 0.0481 | proto 2.7634 | de_rank 0.8439 | dir 1.2891
10:25:27-scGPT-INFO-train_epoch: | epoch   9 | 1900/2375 batches | ms/batch 421.65 | loss 0.8647
10:25:27-scGPT-INFO-train_epoch:   dist 0.0497 | proto 2.7459 | de_rank 0.8505 | dir 1.2682
10:26:10-scGPT-INFO-train_epoch: | epoch   9 | 2000/2375 batches | ms/batch 422.02 | loss 0.8724
10:26:10-scGPT-INFO-train_epoch:   dist 0.0501 | proto 2.7764 | de_rank 0.8443 | dir 1.2760
10:26:52-scGPT-INFO-train_epoch: | epoch   9 | 2100/2375 batches | ms/batch 424.87 | loss 0.8648
10:26:52-scGPT-INFO-train_epoch:   dist 0.0448 | proto 2.7566 | de_rank 0.8484 | dir 1.2785
10:27:35-scGPT-INFO-train_epoch: | epoch   9 | 2200/2375 batches | ms/batch 425.67 | loss 0.8645
10:27:35-scGPT-INFO-train_epoch:   dist 0.0484 | proto 2.7487 | de_rank 0.8512 | dir 1.2625
10:28:18-scGPT-INFO-train_epoch: | epoch   9 | 2300/2375 batches | ms/batch 431.23 | loss 0.8741
10:28:18-scGPT-INFO-train_epoch:   dist 0.0452 | proto 2.7916 | de_rank 0.8503 | dir 1.2817
11:38:14-scGPT-INFO-main: | epoch   9 | time: 5172.83s | loss: 18.5563 | PDS: 0.5294 | DES: 0.2742 | MAE: 1.2890
11:38:14-scGPT-INFO-main:   overall_score: 6.23
11:38:56-scGPT-INFO-train_epoch: | epoch  10 | 100/2375 batches | ms/batch 424.10 | loss 0.8793
11:38:56-scGPT-INFO-train_epoch:   dist 0.0533 | proto 2.7925 | de_rank 0.8547 | dir 1.2746
11:39:38-scGPT-INFO-train_epoch: | epoch  10 | 200/2375 batches | ms/batch 417.80 | loss 0.8530
11:39:38-scGPT-INFO-train_epoch:   dist 0.0395 | proto 2.7247 | de_rank 0.8468 | dir 1.2678
11:40:20-scGPT-INFO-train_epoch: | epoch  10 | 300/2375 batches | ms/batch 420.50 | loss 0.8666
11:40:20-scGPT-INFO-train_epoch:   dist 0.0486 | proto 2.7613 | de_rank 0.8416 | dir 1.2589
11:41:02-scGPT-INFO-train_epoch: | epoch  10 | 400/2375 batches | ms/batch 420.44 | loss 0.8637
11:41:02-scGPT-INFO-train_epoch:   dist 0.0514 | proto 2.7418 | de_rank 0.8421 | dir 1.2630
11:41:44-scGPT-INFO-train_epoch: | epoch  10 | 500/2375 batches | ms/batch 419.33 | loss 0.8684
11:41:44-scGPT-INFO-train_epoch:   dist 0.0538 | proto 2.7550 | de_rank 0.8384 | dir 1.2709
11:42:26-scGPT-INFO-train_epoch: | epoch  10 | 600/2375 batches | ms/batch 421.17 | loss 0.8674
11:42:26-scGPT-INFO-train_epoch:   dist 0.0481 | proto 2.7664 | de_rank 0.8431 | dir 1.2539
11:43:08-scGPT-INFO-train_epoch: | epoch  10 | 700/2375 batches | ms/batch 421.87 | loss 0.8661
11:43:08-scGPT-INFO-train_epoch:   dist 0.0454 | proto 2.7662 | de_rank 0.8456 | dir 1.2549
11:43:51-scGPT-INFO-train_epoch: | epoch  10 | 800/2375 batches | ms/batch 423.73 | loss 0.8721
11:43:51-scGPT-INFO-train_epoch:   dist 0.0490 | proto 2.7757 | de_rank 0.8513 | dir 1.2722
11:44:33-scGPT-INFO-train_epoch: | epoch  10 | 900/2375 batches | ms/batch 424.55 | loss 0.8722
11:44:33-scGPT-INFO-train_epoch:   dist 0.0517 | proto 2.7695 | de_rank 0.8479 | dir 1.2799
11:45:16-scGPT-INFO-train_epoch: | epoch  10 | 1000/2375 batches | ms/batch 425.01 | loss 0.8654
11:45:16-scGPT-INFO-train_epoch:   dist 0.0488 | proto 2.7520 | de_rank 0.8443 | dir 1.2750
11:45:58-scGPT-INFO-train_epoch: | epoch  10 | 1100/2375 batches | ms/batch 425.97 | loss 0.8665
11:45:58-scGPT-INFO-train_epoch:   dist 0.0475 | proto 2.7619 | de_rank 0.8441 | dir 1.2618
11:46:41-scGPT-INFO-train_epoch: | epoch  10 | 1200/2375 batches | ms/batch 425.68 | loss 0.8610
11:46:41-scGPT-INFO-train_epoch:   dist 0.0551 | proto 2.7246 | de_rank 0.8362 | dir 1.2645
11:47:24-scGPT-INFO-train_epoch: | epoch  10 | 1300/2375 batches | ms/batch 425.62 | loss 0.8605
11:47:24-scGPT-INFO-train_epoch:   dist 0.0488 | proto 2.7333 | de_rank 0.8455 | dir 1.2677
11:48:06-scGPT-INFO-train_epoch: | epoch  10 | 1400/2375 batches | ms/batch 426.03 | loss 0.8563
11:48:06-scGPT-INFO-train_epoch:   dist 0.0478 | proto 2.7215 | de_rank 0.8397 | dir 1.2653
11:48:49-scGPT-INFO-train_epoch: | epoch  10 | 1500/2375 batches | ms/batch 424.41 | loss 0.8632
11:48:49-scGPT-INFO-train_epoch:   dist 0.0508 | proto 2.7406 | de_rank 0.8395 | dir 1.2731
11:49:31-scGPT-INFO-train_epoch: | epoch  10 | 1600/2375 batches | ms/batch 426.71 | loss 0.8652
11:49:31-scGPT-INFO-train_epoch:   dist 0.0516 | proto 2.7441 | de_rank 0.8441 | dir 1.2770
11:50:14-scGPT-INFO-train_epoch: | epoch  10 | 1700/2375 batches | ms/batch 426.73 | loss 0.8675
11:50:14-scGPT-INFO-train_epoch:   dist 0.0512 | proto 2.7563 | de_rank 0.8424 | dir 1.2697
11:50:57-scGPT-INFO-train_epoch: | epoch  10 | 1800/2375 batches | ms/batch 426.40 | loss 0.8609
11:50:57-scGPT-INFO-train_epoch:   dist 0.0462 | proto 2.7392 | de_rank 0.8458 | dir 1.2755
11:51:39-scGPT-INFO-train_epoch: | epoch  10 | 1900/2375 batches | ms/batch 426.04 | loss 0.8631
11:51:39-scGPT-INFO-train_epoch:   dist 0.0467 | proto 2.7488 | de_rank 0.8465 | dir 1.2639
11:52:22-scGPT-INFO-train_epoch: | epoch  10 | 2000/2375 batches | ms/batch 425.51 | loss 0.8631
11:52:22-scGPT-INFO-train_epoch:   dist 0.0498 | proto 2.7412 | de_rank 0.8398 | dir 1.2787
11:53:04-scGPT-INFO-train_epoch: | epoch  10 | 2100/2375 batches | ms/batch 427.26 | loss 0.8696
11:53:04-scGPT-INFO-train_epoch:   dist 0.0505 | proto 2.7646 | de_rank 0.8428 | dir 1.2765
11:53:47-scGPT-INFO-train_epoch: | epoch  10 | 2200/2375 batches | ms/batch 427.20 | loss 0.8610
11:53:47-scGPT-INFO-train_epoch:   dist 0.0488 | proto 2.7371 | de_rank 0.8460 | dir 1.2566
11:54:30-scGPT-INFO-train_epoch: | epoch  10 | 2300/2375 batches | ms/batch 428.33 | loss 0.8570
11:54:30-scGPT-INFO-train_epoch:   dist 0.0456 | proto 2.7275 | de_rank 0.8401 | dir 1.2757
13:04:33-scGPT-INFO-main: | epoch  10 | time: 5179.16s | loss: 18.5283 | PDS: 0.5294 | DES: 0.2758 | MAE: 1.2932
13:04:33-scGPT-INFO-main:   overall_score: 6.29
13:04:34-scGPT-INFO-main:   -> New best (overall_score=6.29)
13:05:16-scGPT-INFO-train_epoch: | epoch  11 | 100/2375 batches | ms/batch 418.74 | loss 0.8677
13:05:16-scGPT-INFO-train_epoch:   dist 0.0497 | proto 2.7546 | de_rank 0.8528 | dir 1.2783
13:05:57-scGPT-INFO-train_epoch: | epoch  11 | 200/2375 batches | ms/batch 414.11 | loss 0.8592
13:05:57-scGPT-INFO-train_epoch:   dist 0.0537 | proto 2.7197 | de_rank 0.8375 | dir 1.2660
13:06:39-scGPT-INFO-train_epoch: | epoch  11 | 300/2375 batches | ms/batch 418.95 | loss 0.8676
13:06:39-scGPT-INFO-train_epoch:   dist 0.0513 | proto 2.7522 | de_rank 0.8424 | dir 1.2904
13:07:21-scGPT-INFO-train_epoch: | epoch  11 | 400/2375 batches | ms/batch 421.30 | loss 0.8775
13:07:21-scGPT-INFO-train_epoch:   dist 0.0558 | proto 2.7817 | de_rank 0.8435 | dir 1.2851
13:08:03-scGPT-INFO-train_epoch: | epoch  11 | 500/2375 batches | ms/batch 421.64 | loss 0.8662
13:08:03-scGPT-INFO-train_epoch:   dist 0.0474 | proto 2.7607 | de_rank 0.8435 | dir 1.2654
13:08:46-scGPT-INFO-train_epoch: | epoch  11 | 600/2375 batches | ms/batch 423.10 | loss 0.8678
13:08:46-scGPT-INFO-train_epoch:   dist 0.0500 | proto 2.7600 | de_rank 0.8418 | dir 1.2720
13:09:28-scGPT-INFO-train_epoch: | epoch  11 | 700/2375 batches | ms/batch 423.21 | loss 0.8614
13:09:28-scGPT-INFO-train_epoch:   dist 0.0468 | proto 2.7418 | de_rank 0.8419 | dir 1.2743
13:10:10-scGPT-INFO-train_epoch: | epoch  11 | 800/2375 batches | ms/batch 420.48 | loss 0.8604
13:10:10-scGPT-INFO-train_epoch:   dist 0.0425 | proto 2.7495 | de_rank 0.8427 | dir 1.2650
13:10:52-scGPT-INFO-train_epoch: | epoch  11 | 900/2375 batches | ms/batch 423.31 | loss 0.8686
13:10:52-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.7660 | de_rank 0.8348 | dir 1.2940
13:11:35-scGPT-INFO-train_epoch: | epoch  11 | 1000/2375 batches | ms/batch 423.64 | loss 0.8633
13:11:35-scGPT-INFO-train_epoch:   dist 0.0493 | proto 2.7413 | de_rank 0.8457 | dir 1.2766
13:12:17-scGPT-INFO-train_epoch: | epoch  11 | 1100/2375 batches | ms/batch 425.56 | loss 0.8660
13:12:17-scGPT-INFO-train_epoch:   dist 0.0486 | proto 2.7567 | de_rank 0.8377 | dir 1.2766
13:13:00-scGPT-INFO-train_epoch: | epoch  11 | 1200/2375 batches | ms/batch 426.84 | loss 0.8562
13:13:00-scGPT-INFO-train_epoch:   dist 0.0474 | proto 2.7205 | de_rank 0.8416 | dir 1.2706
13:13:42-scGPT-INFO-train_epoch: | epoch  11 | 1300/2375 batches | ms/batch 424.66 | loss 0.8802
13:13:42-scGPT-INFO-train_epoch:   dist 0.0513 | proto 2.8043 | de_rank 0.8417 | dir 1.2824
13:14:25-scGPT-INFO-train_epoch: | epoch  11 | 1400/2375 batches | ms/batch 425.32 | loss 0.8629
13:14:25-scGPT-INFO-train_epoch:   dist 0.0510 | proto 2.7397 | de_rank 0.8412 | dir 1.2648
13:15:07-scGPT-INFO-train_epoch: | epoch  11 | 1500/2375 batches | ms/batch 426.64 | loss 0.8622
13:15:07-scGPT-INFO-train_epoch:   dist 0.0486 | proto 2.7395 | de_rank 0.8476 | dir 1.2667
13:15:50-scGPT-INFO-train_epoch: | epoch  11 | 1600/2375 batches | ms/batch 427.98 | loss 0.8738
13:15:50-scGPT-INFO-train_epoch:   dist 0.0518 | proto 2.7794 | de_rank 0.8403 | dir 1.2766
13:16:33-scGPT-INFO-train_epoch: | epoch  11 | 1700/2375 batches | ms/batch 425.64 | loss 0.8776
13:16:33-scGPT-INFO-train_epoch:   dist 0.0479 | proto 2.8032 | de_rank 0.8413 | dir 1.2776
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          13:17:16-scGPT-INFO-train_epoch: | epoch  11 | 1800/2375 batches | ms/batch 426.80 | loss 0.8592
13:17:16-scGPT-INFO-train_epoch:   dist 0.0492 | proto 2.7313 | de_rank 0.8410 | dir 1.2540
13:17:58-scGPT-INFO-train_epoch: | epoch  11 | 1900/2375 batches | ms/batch 426.79 | loss 0.8606
13:17:58-scGPT-INFO-train_epoch:   dist 0.0477 | proto 2.7391 | de_rank 0.8432 | dir 1.2568
13:18:23-scGPT-INFO-main: Dataset: 18080 genes, 17840 in vocab
13:18:23-scGPT-INFO-main: DE gene map loaded for 120 perturbations
13:18:23-scGPT-INFO-main: DE genes per pert: mean 500.0 | min 500 | max 500
13:18:23-scGPT-INFO-main: DE map overlap: 120/120 conditions
13:18:23-scGPT-INFO-main: Test: 30, Train: 100, Val: 20 perts
13:18:24-scGPT-INFO-main: Train cells: 149609, Val cells: 28732
13:18:24-scGPT-INFO-load_pretrained_model: Loading pretrained model from model/scGPT/best_model.pt
13:18:24-scGPT-INFO-load_pretrained_model: Model config: embsize=512, nlayers=12, nheads=8
13:18:25-scGPT-INFO-load_pretrained: Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter encoder.enc_norm.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter value_encoder.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter value_encoder.norm.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])
13:18:25-scGPT-INFO-load_pretrained: Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])
13:18:25-scGPT-INFO-freeze_encoder_layers: Froze 50,278,400 params, training 1,581,059 params
13:18:25-scGPT-INFO-freeze_encoder_layers: Trainable: 3.0% of model
13:18:25-scGPT-INFO-freeze_encoder_layers: Trainable components: {'decoder', 'pert_encoder', 'cls_decoder'}
13:18:33-scGPT-INFO-main: 
============================================================
Starting training...
============================================================
13:18:53-scGPT-INFO-train_epoch: | epoch   1 | 100/2375 batches | ms/batch 201.11 | loss 1.0879
13:18:53-scGPT-INFO-train_epoch:   dist 0.0653 | proto 3.4033 | de_rank 1.1021 | dir 1.7541
13:19:12-scGPT-INFO-train_epoch: | epoch   1 | 200/2375 batches | ms/batch 193.58 | loss 0.8904
13:19:12-scGPT-INFO-train_epoch:   dist 0.0554 | proto 2.8141 | de_rank 1.0625 | dir 0.9477
13:19:32-scGPT-INFO-train_epoch: | epoch   1 | 300/2375 batches | ms/batch 193.85 | loss 0.8762
13:19:32-scGPT-INFO-train_epoch:   dist 0.0573 | proto 2.7970 | de_rank 0.9813 | dir 0.8896
13:19:51-scGPT-INFO-train_epoch: | epoch   1 | 400/2375 batches | ms/batch 194.03 | loss 0.8888
13:19:51-scGPT-INFO-train_epoch:   dist 0.0610 | proto 2.8375 | de_rank 0.9880 | dir 0.8810
13:20:10-scGPT-INFO-train_epoch: | epoch   1 | 500/2375 batches | ms/batch 194.02 | loss 0.8781
13:20:10-scGPT-INFO-train_epoch:   dist 0.0551 | proto 2.8251 | de_rank 0.9518 | dir 0.8707

13:20:30-scGPT-INFO-train_epoch: | epoch   1 | 600/2375 batches | ms/batch 194.73 | loss 0.8702
13:20:30-scGPT-INFO-train_epoch:   dist 0.0586 | proto 2.7843 | de_rank 0.9524 | dir 0.8739
13:20:49-scGPT-INFO-train_epoch: | epoch   1 | 700/2375 batches | ms/batch 194.19 | loss 0.8668
13:20:49-scGPT-INFO-train_epoch:   dist 0.0582 | proto 2.7793 | de_rank 0.9433 | dir 0.8546
13:21:09-scGPT-INFO-train_epoch: | epoch   1 | 800/2375 batches | ms/batch 194.64 | loss 0.8734
13:21:09-scGPT-INFO-train_epoch:   dist 0.0568 | proto 2.8145 | de_rank 0.9368 | dir 0.8412
13:21:28-scGPT-INFO-train_epoch: | epoch   1 | 900/2375 batches | ms/batch 194.63 | loss 0.8549
13:21:28-scGPT-INFO-train_epoch:   dist 0.0546 | proto 2.7484 | de_rank 0.9320 | dir 0.8377
13:21:48-scGPT-INFO-train_epoch: | epoch   1 | 1000/2375 batches | ms/batch 194.61 | loss 0.8587
13:21:48-scGPT-INFO-train_epoch:   dist 0.0541 | proto 2.7699 | de_rank 0.9233 | dir 0.8294
13:22:07-scGPT-INFO-train_epoch: | epoch   1 | 1100/2375 batches | ms/batch 194.28 | loss 0.8556
13:22:07-scGPT-INFO-train_epoch:   dist 0.0572 | proto 2.7538 | de_rank 0.9096 | dir 0.8370
13:22:27-scGPT-INFO-train_epoch: | epoch   1 | 1200/2375 batches | ms/batch 194.47 | loss 0.8540
13:22:27-scGPT-INFO-train_epoch:   dist 0.0604 | proto 2.7448 | de_rank 0.9026 | dir 0.8264
13:22:46-scGPT-INFO-train_epoch: | epoch   1 | 1300/2375 batches | ms/batch 194.59 | loss 0.8531
13:22:46-scGPT-INFO-train_epoch:   dist 0.0616 | proto 2.7427 | de_rank 0.9003 | dir 0.8097
13:23:06-scGPT-INFO-train_epoch: | epoch   1 | 1400/2375 batches | ms/batch 194.39 | loss 0.8546
13:23:06-scGPT-INFO-train_epoch:   dist 0.0575 | proto 2.7657 | de_rank 0.8916 | dir 0.7910
13:23:25-scGPT-INFO-train_epoch: | epoch   1 | 1500/2375 batches | ms/batch 193.92 | loss 0.8583
13:23:25-scGPT-INFO-train_epoch:   dist 0.0532 | proto 2.7852 | de_rank 0.8990 | dir 0.8037
13:23:44-scGPT-INFO-train_epoch: | epoch   1 | 1600/2375 batches | ms/batch 194.73 | loss 0.8577
13:23:44-scGPT-INFO-train_epoch:   dist 0.0617 | proto 2.7617 | de_rank 0.8915 | dir 0.8217
13:24:04-scGPT-INFO-train_epoch: | epoch   1 | 1700/2375 batches | ms/batch 194.28 | loss 0.8560
13:24:04-scGPT-INFO-train_epoch:   dist 0.0511 | proto 2.7797 | de_rank 0.8978 | dir 0.8143
13:24:23-scGPT-INFO-train_epoch: | epoch   1 | 1800/2375 batches | ms/batch 194.24 | loss 0.8497
13:24:23-scGPT-INFO-train_epoch:   dist 0.0546 | proto 2.7414 | de_rank 0.9032 | dir 0.8249
13:24:43-scGPT-INFO-train_epoch: | epoch   1 | 1900/2375 batches | ms/batch 194.02 | loss 0.8465
13:24:43-scGPT-INFO-train_epoch:   dist 0.0467 | proto 2.7610 | de_rank 0.8863 | dir 0.7916
13:25:02-scGPT-INFO-train_epoch: | epoch   1 | 2000/2375 batches | ms/batch 194.41 | loss 0.8589
13:25:02-scGPT-INFO-train_epoch:   dist 0.0511 | proto 2.7956 | de_rank 0.8907 | dir 0.8050
13:25:22-scGPT-INFO-train_epoch: | epoch   1 | 2100/2375 batches | ms/batch 194.72 | loss 0.8556
13:25:22-scGPT-INFO-train_epoch:   dist 0.0552 | proto 2.7832 | de_rank 0.8704 | dir 0.7932
13:25:41-scGPT-INFO-train_epoch: | epoch   1 | 2200/2375 batches | ms/batch 194.31 | loss 0.8526
13:25:41-scGPT-INFO-train_epoch:   dist 0.0499 | proto 2.7749 | de_rank 0.8844 | dir 0.8096
13:26:00-scGPT-INFO-train_epoch: | epoch   1 | 2300/2375 batches | ms/batch 194.18 | loss 0.8440
13:26:00-scGPT-INFO-train_epoch:   dist 0.0546 | proto 2.7347 | de_rank 0.8703 | dir 0.8104
14:31:06-scGPT-INFO-main: | epoch   1 | time: 4352.87s | loss: 18.7078 | PDS: 0.5294 | DES: 0.2756 | MAE: 1.2870
14:31:06-scGPT-INFO-main:   overall_score: 6.28
14:31:06-scGPT-INFO-main:   -> New best (overall_score=6.28)
14:31:26-scGPT-INFO-train_epoch: | epoch   2 | 100/2375 batches | ms/batch 196.20 | loss 0.8489
14:31:26-scGPT-INFO-train_epoch:   dist 0.0573 | proto 2.7449 | de_rank 0.8775 | dir 0.8110
14:31:45-scGPT-INFO-train_epoch: | epoch   2 | 200/2375 batches | ms/batch 193.52 | loss 0.8498
14:31:45-scGPT-INFO-train_epoch:   dist 0.0547 | proto 2.7656 | de_rank 0.8596 | dir 0.7934
14:32:05-scGPT-INFO-train_epoch: | epoch   2 | 300/2375 batches | ms/batch 194.39 | loss 0.8508
14:32:05-scGPT-INFO-train_epoch:   dist 0.0522 | proto 2.7650 | de_rank 0.8728 | dir 0.8191
14:32:13-scGPT-INFO-train_epoch: | epoch  12 | 200/2375 batches | ms/batch 417.40 | loss 0.8555
14:32:13-scGPT-INFO-train_epoch:   dist 0.0474 | proto 2.7194 | de_rank 0.8385 | dir 1.2673
14:32:24-scGPT-INFO-train_epoch: | epoch   2 | 400/2375 batches | ms/batch 193.95 | loss 0.8433
14:32:24-scGPT-INFO-train_epoch:   dist 0.0556 | proto 2.7353 | de_rank 0.8572 | dir 0.8091
14:32:44-scGPT-INFO-train_epoch: | epoch   2 | 500/2375 batches | ms/batch 195.31 | loss 0.8469
14:32:44-scGPT-INFO-train_epoch:   dist 0.0522 | proto 2.7557 | de_rank 0.8658 | dir 0.8015
14:33:03-scGPT-INFO-train_epoch: | epoch   2 | 600/2375 batches | ms/batch 193.81 | loss 0.8381
14:33:03-scGPT-INFO-train_epoch:   dist 0.0548 | proto 2.7194 | de_rank 0.8517 | dir 0.8036
14:33:22-scGPT-INFO-train_epoch: | epoch   2 | 700/2375 batches | ms/batch 194.43 | loss 0.8340
14:33:22-scGPT-INFO-train_epoch:   dist 0.0564 | proto 2.7051 | de_rank 0.8477 | dir 0.7824
14:33:42-scGPT-INFO-train_epoch: | epoch   2 | 800/2375 batches | ms/batch 194.69 | loss 0.8548
14:33:42-scGPT-INFO-train_epoch:   dist 0.0552 | proto 2.7839 | de_rank 0.8589 | dir 0.7956
14:34:01-scGPT-INFO-train_epoch: | epoch   2 | 900/2375 batches | ms/batch 193.83 | loss 0.8428
14:34:01-scGPT-INFO-train_epoch:   dist 0.0563 | proto 2.7391 | de_rank 0.8464 | dir 0.7923
14:34:21-scGPT-INFO-train_epoch: | epoch   2 | 1000/2375 batches | ms/batch 195.96 | loss 0.8502
14:34:21-scGPT-INFO-train_epoch:   dist 0.0541 | proto 2.7687 | de_rank 0.8570 | dir 0.7963
14:34:40-scGPT-INFO-train_epoch: | epoch   2 | 1100/2375 batches | ms/batch 195.16 | loss 0.8333
14:34:40-scGPT-INFO-train_epoch:   dist 0.0589 | proto 2.6979 | de_rank 0.8381 | dir 0.7938
14:35:00-scGPT-INFO-train_epoch: | epoch   2 | 1200/2375 batches | ms/batch 194.53 | loss 0.8477
14:35:00-scGPT-INFO-train_epoch:   dist 0.0509 | proto 2.7721 | de_rank 0.8439 | dir 0.7954
14:35:19-scGPT-INFO-train_epoch: | epoch   2 | 1300/2375 batches | ms/batch 194.42 | loss 0.8471
14:35:19-scGPT-INFO-train_epoch:   dist 0.0583 | proto 2.7538 | de_rank 0.8412 | dir 0.7908
14:35:39-scGPT-INFO-train_epoch: | epoch   2 | 1400/2375 batches | ms/batch 194.93 | loss 0.8485
14:35:39-scGPT-INFO-train_epoch:   dist 0.0565 | proto 2.7548 | de_rank 0.8515 | dir 0.8139
14:35:58-scGPT-INFO-train_epoch: | epoch   2 | 1500/2375 batches | ms/batch 194.53 | loss 0.8488
14:35:58-scGPT-INFO-train_epoch:   dist 0.0577 | proto 2.7622 | de_rank 0.8397 | dir 0.7937
14:36:18-scGPT-INFO-train_epoch: | epoch   2 | 1600/2375 batches | ms/batch 194.92 | loss 0.8403
14:36:18-scGPT-INFO-train_epoch:   dist 0.0498 | proto 2.7502 | de_rank 0.8322 | dir 0.7939
14:36:37-scGPT-INFO-train_epoch: | epoch   2 | 1700/2375 batches | ms/batch 194.13 | loss 0.8458
14:36:37-scGPT-INFO-train_epoch:   dist 0.0619 | proto 2.7427 | de_rank 0.8354 | dir 0.7893
14:36:57-scGPT-INFO-train_epoch: | epoch   2 | 1800/2375 batches | ms/batch 194.42 | loss 0.8463
14:36:57-scGPT-INFO-train_epoch:   dist 0.0502 | proto 2.7723 | de_rank 0.8368 | dir 0.7885
14:37:16-scGPT-INFO-train_epoch: | epoch   2 | 1900/2375 batches | ms/batch 194.92 | loss 0.8380
14:37:16-scGPT-INFO-train_epoch:   dist 0.0485 | proto 2.7483 | de_rank 0.8259 | dir 0.7847
14:37:35-scGPT-INFO-train_epoch: | epoch   2 | 2000/2375 batches | ms/batch 193.98 | loss 0.8461
14:37:35-scGPT-INFO-train_epoch:   dist 0.0593 | proto 2.7485 | de_rank 0.8378 | dir 0.7930
14:37:49-scGPT-INFO-train_epoch: | epoch  12 | 1000/2375 batches | ms/batch 419.06 | loss 0.8631
14:37:49-scGPT-INFO-train_epoch:   dist 0.0523 | proto 2.7363 | de_rank 0.8429 | dir 1.2673
14:38:31-scGPT-INFO-train_epoch: | epoch  12 | 1100/2375 batches | ms/batch 421.86 | loss 0.8683
14:38:31-scGPT-INFO-train_epoch:   dist 0.0499 | proto 2.7657 | de_rank 0.8319 | dir 1.2743
14:38:34-scGPT-INFO-train_epoch: | epoch   2 | 2300/2375 batches | ms/batch 194.33 | loss 0.8476
14:38:34-scGPT-INFO-train_epoch:   dist 0.0509 | proto 2.7783 | de_rank 0.8309 | dir 0.7890
14:39:13-scGPT-INFO-train_epoch: | epoch  12 | 1200/2375 batches | ms/batch 425.58 | loss 0.8578
14:39:13-scGPT-INFO-train_epoch:   dist 0.0498 | proto 2.7271 | de_rank 0.8337 | dir 1.2552
14:39:56-scGPT-INFO-train_epoch: | epoch  12 | 1300/2375 batches | ms/batch 424.90 | loss 0.8582
14:39:56-scGPT-INFO-train_epoch:   dist 0.0440 | proto 2.7380 | de_rank 0.8400 | dir 1.2658
14:40:38-scGPT-INFO-train_epoch: | epoch  12 | 1400/2375 batches | ms/batch 424.34 | loss 0.8677
14:40:38-scGPT-INFO-train_epoch:   dist 0.0462 | proto 2.7737 | de_rank 0.8406 | dir 1.2495
14:41:21-scGPT-INFO-train_epoch: | epoch  12 | 1500/2375 batches | ms/batch 424.02 | loss 0.8574
14:41:21-scGPT-INFO-train_epoch:   dist 0.0506 | proto 2.7197 | de_rank 0.8366 | dir 1.2689
14:42:03-scGPT-INFO-train_epoch: | epoch  12 | 1600/2375 batches | ms/batch 423.81 | loss 0.8577
14:42:03-scGPT-INFO-train_epoch:   dist 0.0448 | proto 2.7361 | de_rank 0.8428 | dir 1.2498
14:42:45-scGPT-INFO-train_epoch: | epoch  12 | 1700/2375 batches | ms/batch 423.02 | loss 0.8638
14:42:45-scGPT-INFO-train_epoch:   dist 0.0493 | proto 2.7470 | de_rank 0.8441 | dir 1.2606
14:43:28-scGPT-INFO-train_epoch: | epoch  12 | 1800/2375 batches | ms/batch 425.10 | loss 0.8642
14:43:28-scGPT-INFO-train_epoch:   dist 0.0506 | proto 2.7483 | de_rank 0.8365 | dir 1.2630
14:44:10-scGPT-INFO-train_epoch: | epoch  12 | 1900/2375 batches | ms/batch 424.71 | loss 0.8569
14:44:10-scGPT-INFO-train_epoch:   dist 0.0488 | proto 2.7236 | de_rank 0.8378 | dir 1.2598
14:44:53-scGPT-INFO-train_epoch: | epoch  12 | 2000/2375 batches | ms/batch 424.54 | loss 0.8599
14:44:53-scGPT-INFO-train_epoch:   dist 0.0556 | proto 2.7190 | de_rank 0.8329 | dir 1.2694
14:45:36-scGPT-INFO-train_epoch: | epoch  12 | 2100/2375 batches | ms/batch 428.32 | loss 0.8757
14:45:36-scGPT-INFO-train_epoch:   dist 0.0486 | proto 2.7997 | de_rank 0.8392 | dir 1.2532
14:46:18-scGPT-INFO-train_epoch: | epoch  12 | 2200/2375 batches | ms/batch 428.21 | loss 0.8546
14:46:18-scGPT-INFO-train_epoch:   dist 0.0493 | proto 2.7118 | de_rank 0.8378 | dir 1.2651
14:47:01-scGPT-INFO-train_epoch: | epoch  12 | 2300/2375 batches | ms/batch 427.07 | loss 0.8677
14:47:01-scGPT-INFO-train_epoch:   dist 0.0437 | proto 2.7785 | de_rank 0.8356 | dir 1.2663
15:57:14-scGPT-INFO-main: | epoch  12 | time: 5185.13s | loss: 18.4876 | PDS: 0.5294 | DES: 0.2715 | MAE: 1.2863
15:57:14-scGPT-INFO-main:   overall_score: 6.12
15:57:57-scGPT-INFO-train_epoch: | epoch  13 | 100/2375 batches | ms/batch 422.99 | loss 0.8686
15:57:57-scGPT-INFO-train_epoch:   dist 0.0469 | proto 2.7675 | de_rank 0.8437 | dir 1.2835
15:58:39-scGPT-INFO-train_epoch: | epoch  13 | 200/2375 batches | ms/batch 418.58 | loss 0.8602
15:58:39-scGPT-INFO-train_epoch:   dist 0.0537 | proto 2.7270 | de_rank 0.8303 | dir 1.2637
15:59:21-scGPT-INFO-train_epoch: | epoch  13 | 300/2375 batches | ms/batch 421.34 | loss 0.8608
15:59:21-scGPT-INFO-train_epoch:   dist 0.0505 | proto 2.7363 | de_rank 0.8356 | dir 1.2563
16:00:03-scGPT-INFO-train_epoch: | epoch  13 | 400/2375 batches | ms/batch 419.31 | loss 0.8624
16:00:03-scGPT-INFO-train_epoch:   dist 0.0511 | proto 2.7435 | de_rank 0.8317 | dir 1.2554
16:00:45-scGPT-INFO-train_epoch: | epoch  13 | 500/2375 batches | ms/batch 420.50 | loss 0.8613
16:00:45-scGPT-INFO-train_epoch:   dist 0.0501 | proto 2.7418 | de_rank 0.8336 | dir 1.2488
16:01:27-scGPT-INFO-train_epoch: | epoch  13 | 600/2375 batches | ms/batch 424.62 | loss 0.8656
16:01:27-scGPT-INFO-train_epoch:   dist 0.0527 | proto 2.7508 | de_rank 0.8335 | dir 1.2588
16:02:10-scGPT-INFO-train_epoch: | epoch  13 | 700/2375 batches | ms/batch 425.59 | loss 0.8633
16:02:10-scGPT-INFO-train_epoch:   dist 0.0466 | proto 2.7535 | de_rank 0.8342 | dir 1.2704
16:02:52-scGPT-INFO-train_epoch: | epoch  13 | 800/2375 batches | ms/batch 423.00 | loss 0.8652
16:02:52-scGPT-INFO-train_epoch:   dist 0.0485 | proto 2.7572 | de_rank 0.8363 | dir 1.2643
16:03:34-scGPT-INFO-train_epoch: | epoch  13 | 900/2375 batches | ms/batch 423.04 | loss 0.8631
16:03:34-scGPT-INFO-train_epoch:   dist 0.0515 | proto 2.7427 | de_rank 0.8386 | dir 1.2526
16:04:17-scGPT-INFO-train_epoch: | epoch  13 | 1000/2375 batches | ms/batch 424.77 | loss 0.8572
16:04:17-scGPT-INFO-train_epoch:   dist 0.0472 | proto 2.7282 | de_rank 0.8374 | dir 1.2612
16:05:00-scGPT-INFO-train_epoch: | epoch  13 | 1100/2375 batches | ms/batch 426.53 | loss 0.8566
16:05:00-scGPT-INFO-train_epoch:   dist 0.0462 | proto 2.7301 | de_rank 0.8388 | dir 1.2496
16:05:42-scGPT-INFO-train_epoch: | epoch  13 | 1200/2375 batches | ms/batch 426.20 | loss 0.8747
16:05:42-scGPT-INFO-train_epoch:   dist 0.0426 | proto 2.8077 | de_rank 0.8310 | dir 1.2834
16:06:25-scGPT-INFO-train_epoch: | epoch  13 | 1300/2375 batches | ms/batch 425.52 | loss 0.8590
16:06:25-scGPT-INFO-train_epoch:   dist 0.0431 | proto 2.7437 | de_rank 0.8406 | dir 1.2622
16:07:07-scGPT-INFO-train_epoch: | epoch  13 | 1400/2375 batches | ms/batch 426.16 | loss 0.8644
16:07:07-scGPT-INFO-train_epoch:   dist 0.0473 | proto 2.7539 | de_rank 0.8319 | dir 1.2884
16:07:50-scGPT-INFO-train_epoch: | epoch  13 | 1500/2375 batches | ms/batch 425.97 | loss 0.8592
16:07:50-scGPT-INFO-train_epoch:   dist 0.0464 | proto 2.7400 | de_rank 0.8329 | dir 1.2620
16:08:33-scGPT-INFO-train_epoch: | epoch  13 | 1600/2375 batches | ms/batch 429.19 | loss 0.8542
16:08:33-scGPT-INFO-train_epoch:   dist 0.0447 | proto 2.7272 | de_rank 0.8255 | dir 1.2609
16:09:16-scGPT-INFO-train_epoch: | epoch  13 | 1700/2375 batches | ms/batch 429.70 | loss 0.8574
16:09:16-scGPT-INFO-train_epoch:   dist 0.0436 | proto 2.7420 | de_rank 0.8291 | dir 1.2577
16:09:59-scGPT-INFO-train_epoch: | epoch  13 | 1800/2375 batches | ms/batch 429.00 | loss 0.8596
16:09:59-scGPT-INFO-train_epoch:   dist 0.0477 | proto 2.7379 | de_rank 0.8318 | dir 1.2672
16:10:41-scGPT-INFO-train_epoch: | epoch  13 | 1900/2375 batches | ms/batch 427.71 | loss 0.8641
16:10:41-scGPT-INFO-train_epoch:   dist 0.0531 | proto 2.7412 | de_rank 0.8302 | dir 1.2775
16:11:24-scGPT-INFO-train_epoch: | epoch  13 | 2000/2375 batches | ms/batch 426.52 | loss 0.8508
16:11:24-scGPT-INFO-train_epoch:   dist 0.0498 | proto 2.6977 | de_rank 0.8297 | dir 1.2700
16:12:07-scGPT-INFO-train_epoch: | epoch  13 | 2100/2375 batches | ms/batch 425.35 | loss 0.8624
16:12:07-scGPT-INFO-train_epoch:   dist 0.0562 | proto 2.7320 | de_rank 0.8289 | dir 1.2563
16:12:49-scGPT-INFO-train_epoch: | epoch  13 | 2200/2375 batches | ms/batch 424.71 | loss 0.8602
16:12:49-scGPT-INFO-train_epoch:   dist 0.0483 | proto 2.7383 | de_rank 0.8290 | dir 1.2738
16:13:31-scGPT-INFO-train_epoch: | epoch  13 | 2300/2375 batches | ms/batch 423.00 | loss 0.8600
16:13:31-scGPT-INFO-train_epoch:   dist 0.0431 | proto 2.7488 | de_rank 0.8346 | dir 1.2692
17:23:15-scGPT-INFO-main: | epoch  13 | time: 5160.97s | loss: 18.4790 | PDS: 0.5294 | DES: 0.2730 | MAE: 1.2886
17:23:15-scGPT-INFO-main:   overall_score: 6.18
17:23:58-scGPT-INFO-train_epoch: | epoch  14 | 100/2375 batches | ms/batch 426.03 | loss 0.8723
17:23:58-scGPT-INFO-train_epoch:   dist 0.0517 | proto 2.7750 | de_rank 0.8392 | dir 1.2729
17:24:40-scGPT-INFO-train_epoch: | epoch  14 | 200/2375 batches | ms/batch 418.79 | loss 0.8466
17:24:40-scGPT-INFO-train_epoch:   dist 0.0462 | proto 2.6901 | de_rank 0.8301 | dir 1.2668
17:25:22-scGPT-INFO-train_epoch: | epoch  14 | 300/2375 batches | ms/batch 420.24 | loss 0.8651
17:25:22-scGPT-INFO-train_epoch:   dist 0.0450 | proto 2.7660 | de_rank 0.8348 | dir 1.2627
17:26:04-scGPT-INFO-train_epoch: | epoch  14 | 400/2375 batches | ms/batch 417.80 | loss 0.8623
17:26:04-scGPT-INFO-train_epoch:   dist 0.0543 | proto 2.7327 | de_rank 0.8318 | dir 1.2672
17:26:46-scGPT-INFO-train_epoch: | epoch  14 | 500/2375 batches | ms/batch 420.14 | loss 0.8634
17:26:46-scGPT-INFO-train_epoch:   dist 0.0537 | proto 2.7382 | de_rank 0.8359 | dir 1.2601
17:27:28-scGPT-INFO-train_epoch: | epoch  14 | 600/2375 batches | ms/batch 420.91 | loss 0.8599
17:27:28-scGPT-INFO-train_epoch:   dist 0.0451 | proto 2.7481 | de_rank 0.8276 | dir 1.2616
17:28:10-scGPT-INFO-train_epoch: | epoch  14 | 700/2375 batches | ms/batch 420.55 | loss 0.8707
17:28:10-scGPT-INFO-train_epoch:   dist 0.0475 | proto 2.7827 | de_rank 0.8299 | dir 1.2709
17:28:53-scGPT-INFO-train_epoch: | epoch  14 | 800/2375 batches | ms/batch 425.95 | loss 0.8545
17:28:53-scGPT-INFO-train_epoch:   dist 0.0451 | proto 2.7248 | de_rank 0.8346 | dir 1.2555
17:29:35-scGPT-INFO-train_epoch: | epoch  14 | 900/2375 batches | ms/batch 425.40 | loss 0.8614
17:29:35-scGPT-INFO-train_epoch:   dist 0.0495 | proto 2.7441 | de_rank 0.8292 | dir 1.2547
17:30:18-scGPT-INFO-train_epoch: | epoch  14 | 1000/2375 batches | ms/batch 424.86 | loss 0.8564
17:30:18-scGPT-INFO-train_epoch:   dist 0.0482 | proto 2.7278 | de_rank 0.8315 | dir 1.2484
17:31:00-scGPT-INFO-train_epoch: | epoch  14 | 1100/2375 batches | ms/batch 424.66 | loss 0.8615
17:31:00-scGPT-INFO-train_epoch:   dist 0.0518 | proto 2.7381 | de_rank 0.8317 | dir 1.2537
17:31:42-scGPT-INFO-train_epoch: | epoch  14 | 1200/2375 batches | ms/batch 424.69 | loss 0.8573
17:31:42-scGPT-INFO-train_epoch:   dist 0.0462 | proto 2.7320 | de_rank 0.8327 | dir 1.2665
17:32:25-scGPT-INFO-train_epoch: | epoch  14 | 1300/2375 batches | ms/batch 424.42 | loss 0.8537
17:32:25-scGPT-INFO-train_epoch:   dist 0.0463 | proto 2.7218 | de_rank 0.8296 | dir 1.2514
17:33:07-scGPT-INFO-train_epoch: | epoch  14 | 1400/2375 batches | ms/batch 424.86 | loss 0.8616
17:33:07-scGPT-INFO-train_epoch:   dist 0.0525 | proto 2.7368 | de_rank 0.8279 | dir 1.2622
17:33:50-scGPT-INFO-train_epoch: | epoch  14 | 1500/2375 batches | ms/batch 424.52 | loss 0.8552
17:33:50-scGPT-INFO-train_epoch:   dist 0.0457 | proto 2.7265 | de_rank 0.8294 | dir 1.2638
17:34:32-scGPT-INFO-train_epoch: | epoch  14 | 1600/2375 batches | ms/batch 425.36 | loss 0.8603
17:34:32-scGPT-INFO-train_epoch:   dist 0.0492 | proto 2.7396 | de_rank 0.8228 | dir 1.2722
17:35:15-scGPT-INFO-train_epoch: | epoch  14 | 1700/2375 batches | ms/batch 428.23 | loss 0.8550
17:35:15-scGPT-INFO-train_epoch:   dist 0.0506 | proto 2.7141 | de_rank 0.8270 | dir 1.2678
17:35:58-scGPT-INFO-train_epoch: | epoch  14 | 1800/2375 batches | ms/batch 427.89 | loss 0.8712
17:35:58-scGPT-INFO-train_epoch:   dist 0.0509 | proto 2.7788 | de_rank 0.8288 | dir 1.2609
17:36:41-scGPT-INFO-train_epoch: | epoch  14 | 1900/2375 batches | ms/batch 426.36 | loss 0.8676
17:36:41-scGPT-INFO-train_epoch:   dist 0.0557 | proto 2.7558 | de_rank 0.8227 | dir 1.2588
17:37:24-scGPT-INFO-train_epoch: | epoch  14 | 2000/2375 batches | ms/batch 428.34 | loss 0.8589
17:37:24-scGPT-INFO-train_epoch:   dist 0.0476 | proto 2.7389 | de_rank 0.8257 | dir 1.2623
17:38:06-scGPT-INFO-train_epoch: | epoch  14 | 2100/2375 batches | ms/batch 428.01 | loss 0.8606
17:38:06-scGPT-INFO-train_epoch:   dist 0.0504 | proto 2.7372 | de_rank 0.8303 | dir 1.2606
17:38:49-scGPT-INFO-train_epoch: | epoch  14 | 2200/2375 batches | ms/batch 423.91 | loss 0.8640
17:38:49-scGPT-INFO-train_epoch:   dist 0.0472 | proto 2.7589 | de_rank 0.8318 | dir 1.2566
17:39:31-scGPT-INFO-train_epoch: | epoch  14 | 2300/2375 batches | ms/batch 424.49 | loss 0.8637
17:39:31-scGPT-INFO-train_epoch:   dist 0.0488 | proto 2.7568 | de_rank 0.8221 | dir 1.2610
