# scGPT Finetuning Configuration for VCC Perturbation Prediction
# Based on scGPT Tutorial_Perturbation.ipynb

# ========== Paths ==========
paths:
  # GEARS data directory (created by scripts/convert_to_gears.py)
  gears_data_dir: "data/processed/gears"
  dataset_name: "vcc"
  
  # Pretrained model directory
  pretrained_model_dir: "model/scGPT"
  
  # Output directory for finetuned model
  output_dir: "model/scGPT_finetuned"

# ========== Data Processing ==========
data:
  pad_token: "<pad>"
  special_tokens: ["<pad>", "<cls>", "<eoc>"]
  pad_value: 0
  pert_pad_id: 0
  include_zero_gene: "batch-wise"  # "batch-wise" uses less memory than "all"
  max_seq_len: 1536

# ========== Training Objectives ==========
training:
  # Primary objective (always on for perturbation)
  MLM: true   # Masked Language Modeling
  
  # Optional objectives (disabled for perturbation finetuning)
  CLS: false  # Cell type classification
  CCE: false  # Contrastive cell embedding
  MVC: false  # Masked value prediction for cell embedding
  ECS: false  # Elastic cell similarity
  
  # Mixed precision training
  amp: true

# ========== Optimizer ==========
optimizer:
  lr: 1.0e-4
  batch_size: 64
  eval_batch_size: 64
  epochs: 15
  schedule_interval: 1
  schedule_gamma: 0.9
  early_stop: 10  # patience for early stopping
  grad_clip: 1.0

# ========== Model Architecture ==========
# These are overridden by pretrained model config (args.json)
model:
  embsize: 512
  d_hid: 512
  nlayers: 12
  nhead: 8
  n_layers_cls: 3
  dropout: 0.0
  use_fast_transformer: false  # Disabled: flash-attn not available, uses PyTorch transformer

# ========== Partial Weight Loading ==========
# Only load these prefixes from pretrained model
# The rest (e.g., decoder) will be randomly initialized
load_param_prefixes:
  - "encoder"
  - "value_encoder"
  - "transformer_encoder"

# ========== Data Split ==========
# Train/Val split on 120 genes (excluding 30 test genes)
split:
  test_genes_file: "data/raw/test_set.csv"  # 30 test genes
  train_ratio: 0.833  # 100/120 genes for training
  val_ratio: 0.167    # 20/120 genes for validation
  seed: 42

# ========== Metrics ==========
metrics:
  mae_top_k: 2000  # Top K genes by |log2FC| for MAE calculation

# ========== Early Stopping ==========
early_stopping:
  metric: "combined"  # mean of nPDS, 1-DES, MAE_top2k (all lower is better)

# ========== Logging ==========
logging:
  log_interval: 100
  save_interval: 1  # Save every epoch

# ========== Hardware ==========
hardware:
  # Detected automatically, but can override
  device: "cuda"  # "cuda" or "cpu"
