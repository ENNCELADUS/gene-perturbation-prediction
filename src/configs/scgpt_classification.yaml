# scGPT Fine-tuning Configuration (Top-K Classification Objective)
#
# Aligns training with logreg's probability-ranked top-K retrieval.

# Data
data:
  h5ad_path: data/norman/perturb_processed.h5ad

# Norman GEARS-style Condition-level split (optimized)
condition_split:
  unseen_gene_fraction: 0.15      # 15% of single-genes → test-only (~12-15 genes, increased train coverage)
  seen_single_train_ratio: 0.9    # 90% train / 10% val for seen singles
  combo_seen2_train_ratio: 0.7    # 70% train for 2/2-seen combos
  combo_seen2_val_ratio: 0.15     # 15% val for 2/2-seen combos (remaining → test)
  min_cells_per_condition: 50     # Filter threshold for single-gene conditions
  min_cells_per_double: 30        # Lower threshold for double-gene (increase coverage, handle noise)
  seed: 42
  output_path: data/norman/splits/norman_condition_split_optimized_seed42.json

# Model
model:
  encoder: scgpt
  # Pretrained scGPT checkpoint (frozen backbone)
  pretrained_dir: model/scGPT
  finetune_apply_head: true
  finetune_apply_classifier: true
  freeze_encoder: true
  use_lora: false
  gene_alias_map_path: null
  # Preprocessing: Plan A - Use log-normalized + HVG filtered data from adata.X
  # Only binning is needed for scGPT value encoder
  preprocess: true
  preprocess_binning: 51
  preprocess_result_binned_key: X_binned

# Training configuration
training:
  mode: head_only  # frozen | head_only | lora_head
  loss_fn: classification  # infonce | classification
  epochs: 200
  batch_size: 32
  learning_rate: 1.0e-4
  head_learning_rate: 1.0e-4
  backbone_learning_rate: 1.0e-5
  unfreeze_last_n_layers: 2
  weight_decay: 0.01
  warmup_ratio: 0.1
  mask_perturbed: true
  max_grad_norm: 1.0
  early_stopping_patience: 10
  early_stopping_metric: val_loss
  balanced_sampling: true
  balanced_sampling_n_conditions: 8
  balanced_sampling_n_cells: 4
  checkpoint_dir: model/scgpt_finetune_cls

# LoRA configuration (for lora_head mode)
lora:
  rank: 8
  alpha: 16.0
  dropout: 0.1
  last_n_layers: 2
  target_modules:
    - out_proj
    - linear1
    - linear2

# Retrieval head configuration
head:
  hidden_dim: 256
  output_dim: 128
  dropout: 0.2

# InfoNCE loss configuration
infonce:
  temperature: 0.07

# Classification loss configuration (for classification mode)
classification:
  label_smoothing: 0.0

# Retrieval evaluation settings
retrieval:
  metric: prob
  top_k: [1, 5, 8, 10]

# Query setup
query:
  mode: cell
  query_split: test
  candidate_source: all

# Evaluation settings
evaluate:
  mode: classifier
  mask_perturbed: true
  mask_ablation: true

# Confidence estimation
confidence:
  enable: true
  method: margin
  coverage_points: 20
  top_k_agreement: 1

# Logging
logging:
  output_dir: results/
  experiment_name: scgpt_finetune_cls
