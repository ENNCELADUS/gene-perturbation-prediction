# Route A: Forward Model + Retrieval Configuration
# scGPT-based reverse perturbation prediction using generative forward modeling

# Data
data:
  h5ad_path: data/norman/perturb_processed.h5ad

# Norman GEARS-style Condition-level split (optimized)
condition_split:
  unseen_gene_fraction: 0.15      # 15% of single-genes → test-only (~12-15 genes)
  seen_single_train_ratio: 0.9    # 90% train / 10% val for seen singles
  combo_seen2_train_ratio: 0.7    # 70% train for 2/2-seen combos  
  combo_seen2_val_ratio: 0.15     # 15% val for 2/2-seen combos (remaining → test)
  min_cells_per_condition: 50     # Filter threshold for single-gene conditions
  min_cells_per_double: 30        # Lower threshold for double-gene conditions
  seed: 42
  output_path: data/norman/splits/norman_condition_split_optimized_seed42.json

# Model Configuration
model:
  encoder: scgpt
  # Pretrained scGPT checkpoint settings
  pretrained_checkpoint: model/scGPT/best_model.pt
  vocab_path: model/scGPT/vocab.json
  args_path: model/scGPT/args.json
  
  # Freeze strategy for finetuning
  freeze_encoder: true
  freeze_layers_up_to: 10  # Freeze transformer layers 0-10, train layer 11
  
  # Preprocessing (scGPT uses binned continuous values)
  preprocess: true
  preprocess_binning: 51
  preprocess_result_binned_key: X_binned

# Forward Model Training Configuration
training:
  # Optimizer settings
  lr_decoder: 1.0e-4          # Learning rate for decoder/mvc_decoder
  lr_last_layer: 1.0e-5       # Learning rate for last transformer layer + enc_norm
  weight_decay: 0.01
  
  # Training hyperparameters
  epochs: 30
  batch_size: 16              # Training batch size
  val_batch_size: 32          # Validation batch size
  warmup_ratio: 0.05          # LR warmup (5% of total steps)
  max_grad_norm: 1.0          # Gradient clipping
  
  # Data collator settings
  max_control_per_condition: 100  # Max control cells to sample per condition
  max_seq_len: 1200               # Maximum sequence length for tokenization
  
  # Checkpointing
  output_dir: results/forward
  save_every_n_epochs: 5
  early_stopping_patience: 10

# Reference Database Construction
reference_db:
  n_samples_per_condition: 200  # Number of predicted profiles per condition
  output_path: results/forward/reference_db.pkl

# Retrieval Configuration
retrieval:
  metric: cosine              # cosine | euclidean
  top_k: [1, 5, 10, 20]      # Top-K values for Hit@K metrics
  voting_weight: linear       # Voting aggregation: linear (rank-based decay)

# Evaluation Settings
evaluate:
  # Metrics to compute (from src/evaluate/metrics.py)
  metrics:
    - top_k_accuracy          # Exact match Hit@K
    - one_gene_overlap_hit    # Relevant retrieval Hit@K (one-gene overlap)
    - mrr                     # Mean Reciprocal Rank
    - ndcg                    # Normalized Discounted Cumulative Gain
  
  # Output
  results_path: results/forward/eval_results.json

# Logging
logging:
  output_dir: results/
  experiment_name: scgpt_route_a
